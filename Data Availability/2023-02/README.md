## Learning Translation Quality Evaluation on Low Resource Languages from
  Large Language Models

**Published Date:** 2023-02-07T14:35:35Z

**Link:** http://arxiv.org/pdf/2302.03491v1

**Abstract:**

  Learned metrics such as BLEURT have in recent years become widely employed to
evaluate the quality of machine translation systems. Training such metrics
requires data which can be expensive and difficult to acquire, particularly for
lower-resource languages. We show how knowledge can be distilled from Large
Language Models (LLMs) to improve upon such learned metrics without requiring
human annotators, by creating synthetic datasets which can be mixed into
existing datasets, requiring only a corpus of text in the target language. We
show that the performance of a BLEURT-like model on lower resource languages
can be improved in this way.


---

## Mitigating Data Scarcity for Large Language Models

**Published Date:** 2023-02-03T15:17:53Z

**Link:** http://arxiv.org/pdf/2302.01806v1

**Abstract:**

  In recent years, pretrained neural language models (PNLMs) have taken the
field of natural language processing by storm, achieving new benchmarks and
state-of-the-art performances. These models often rely heavily on annotated
data, which may not always be available. Data scarcity are commonly found in
specialized domains, such as medical, or in low-resource languages that are
underexplored by AI research. In this dissertation, we focus on mitigating data
scarcity using data augmentation and neural ensemble learning techniques for
neural language models. In both research directions, we implement neural
network algorithms and evaluate their impact on assisting neural language
models in downstream NLP tasks. Specifically, for data augmentation, we explore
two techniques: 1) creating positive training data by moving an answer span
around its original context and 2) using text simplification techniques to
introduce a variety of writing styles to the original training data. Our
results indicate that these simple and effective solutions improve the
performance of neural language models considerably in low-resource NLP domains
and tasks. For neural ensemble learning, we use a multilabel neural classifier
to select the best prediction outcome from a variety of individual pretrained
neural language models trained for a low-resource medical text simplification
task.


---

