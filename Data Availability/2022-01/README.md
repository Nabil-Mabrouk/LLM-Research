## Cross-Domain Deep Code Search with Few-Shot Meta Learning

**Published Date:** 2022-01-01T09:00:48Z

**Link:** http://arxiv.org/pdf/2201.00150v5

**Abstract:**

  Recently, pre-trained programming language models such as CodeBERT have
demonstrated substantial gains in code search. Despite showing great
performance, they rely on the availability of large amounts of parallel data to
fine-tune the semantic mappings between queries and code. This restricts their
practicality in domain-specific languages with relatively scarce and expensive
data. In this paper, we propose CDCS, a novel approach for domain-specific code
search. CDCS employs a transfer learning framework where an initial program
representation model is pre-trained on a large corpus of common programming
languages (such as Java and Python), and is further adapted to domain-specific
languages such as SQL and Solidity. Unlike cross-language CodeBERT, which is
directly fine-tuned in the target language, CDCS adapts a few-shot
meta-learning algorithm called MAML to learn the good initialization of model
parameters, which can be best reused in a domain-specific language. We evaluate
the proposed approach on two domain-specific languages, namely, SQL and
Solidity, with model transferred from two widely used languages (Python and
Java). Experimental results show that CDCS significantly outperforms
conventional pre-trained code models that are directly fine-tuned in
domain-specific languages, and it is particularly effective for scarce data.


---

## Cross-Domain Deep Code Search with Few-Shot Meta Learning

**first_author:** Yitian Chai et al.

**Published Date:** 2022-01-01T09:00:48Z

**Link:** http://arxiv.org/pdf/2201.00150v5

**Abstract:**

  Recently, pre-trained programming language models such as CodeBERT have
demonstrated substantial gains in code search. Despite showing great
performance, they rely on the availability of large amounts of parallel data to
fine-tune the semantic mappings between queries and code. This restricts their
practicality in domain-specific languages with relatively scarce and expensive
data. In this paper, we propose CDCS, a novel approach for domain-specific code
search. CDCS employs a transfer learning framework where an initial program
representation model is pre-trained on a large corpus of common programming
languages (such as Java and Python), and is further adapted to domain-specific
languages such as SQL and Solidity. Unlike cross-language CodeBERT, which is
directly fine-tuned in the target language, CDCS adapts a few-shot
meta-learning algorithm called MAML to learn the good initialization of model
parameters, which can be best reused in a domain-specific language. We evaluate
the proposed approach on two domain-specific languages, namely, SQL and
Solidity, with model transferred from two widely used languages (Python and
Java). Experimental results show that CDCS significantly outperforms
conventional pre-trained code models that are directly fine-tuned in
domain-specific languages, and it is particularly effective for scarce data.


---

