## UIO at SemEval-2023 Task 12: Multilingual fine-tuning for sentiment
  classification in low-resource languages

**Published Date:** 2023-04-27T13:51:18Z

**Link:** http://arxiv.org/pdf/2304.14189v1

**Abstract:**

  Our contribution to the 2023 AfriSenti-SemEval shared task 12: Sentiment
Analysis for African Languages, provides insight into how a multilingual large
language model can be a resource for sentiment analysis in languages not seen
during pretraining. The shared task provides datasets of a variety of African
languages from different language families. The languages are to various
degrees related to languages used during pretraining, and the language data
contain various degrees of code-switching. We experiment with both monolingual
and multilingual datasets for the final fine-tuning, and find that with the
provided datasets that contain samples in the thousands, monolingual
fine-tuning yields the best results.


---

## Learnings from Data Integration for Augmented Language Models

**Published Date:** 2023-04-10T13:28:35Z

**Link:** http://arxiv.org/pdf/2304.04576v1

**Abstract:**

  One of the limitations of large language models is that they do not have
access to up-to-date, proprietary or personal data. As a result, there are
multiple efforts to extend language models with techniques for accessing
external data. In that sense, LLMs share the vision of data integration systems
whose goal is to provide seamless access to a large collection of heterogeneous
data sources. While the details and the techniques of LLMs differ greatly from
those of data integration, this paper shows that some of the lessons learned
from research on data integration can elucidate the research path we are
conducting today on language models.


---

## VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and
  Dataset

**Published Date:** 2023-04-17T15:08:15Z

**Link:** http://arxiv.org/pdf/2304.08345v1

**Abstract:**

  In this paper, we propose a Vision-Audio-Language Omni-peRception pretraining
model (VALOR) for multi-modal understanding and generation. Different from
widely-studied vision-language pretraining models, VALOR jointly models
relationships of vision, audio and language in an end-to-end manner. It
contains three separate encoders for single modality representations, and a
decoder for multimodal conditional text generation. We design two pretext tasks
to pretrain VALOR model, including Multimodal Grouping Alignment (MGA) and
Multimodal Grouping Captioning (MGC). MGA projects vision, language and audio
to the same common space, building vision-language, audio-language and
audiovisual-language alignment simultaneously. MGC learns how to generate text
tokens in conditions of vision, audio or their both. To promote
vision-audio-language pretraining research, we construct a large-scale
high-quality tri-modality dataset named VALOR-1M, which contains 1M audiable
videos with human annotated audiovisual captions. Extensive experiments show
that VALOR can learn strong multimodal correlations and be generalized to
various downstream tasks (e.g., retrieval, captioning and question answering),
with different input modalities (e.g., vision-language, audio-language and
audiovisual-language). VALOR achieves new state-of-the-art performances on
series of public cross-modality benchmarks. Code and data are available at
project page https://casia-iva-group.github.io/projects/VALOR.


---

## GreekBART: The First Pretrained Greek Sequence-to-Sequence Model

**Published Date:** 2023-04-03T10:48:51Z

**Link:** http://arxiv.org/pdf/2304.00869v1

**Abstract:**

  The era of transfer learning has revolutionized the fields of Computer Vision
and Natural Language Processing, bringing powerful pretrained models with
exceptional performance across a variety of tasks. Specifically, Natural
Language Processing tasks have been dominated by transformer-based language
models. In Natural Language Inference and Natural Language Generation tasks,
the BERT model and its variants, as well as the GPT model and its successors,
demonstrated exemplary performance. However, the majority of these models are
pretrained and assessed primarily for the English language or on a multilingual
corpus. In this paper, we introduce GreekBART, the first Seq2Seq model based on
BART-base architecture and pretrained on a large-scale Greek corpus. We
evaluate and compare GreekBART against BART-random, Greek-BERT, and XLM-R on a
variety of discriminative tasks. In addition, we examine its performance on two
NLG tasks from GreekSUM, a newly introduced summarization dataset for the Greek
language. The model, the code, and the new summarization dataset will be
publicly available.


---

## VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and
  Dataset

**Published Date:** 2023-04-17T15:08:15Z

**Link:** http://arxiv.org/pdf/2304.08345v1

**Abstract:**

  In this paper, we propose a Vision-Audio-Language Omni-peRception pretraining
model (VALOR) for multi-modal understanding and generation. Different from
widely-studied vision-language pretraining models, VALOR jointly models
relationships of vision, audio and language in an end-to-end manner. It
contains three separate encoders for single modality representations, and a
decoder for multimodal conditional text generation. We design two pretext tasks
to pretrain VALOR model, including Multimodal Grouping Alignment (MGA) and
Multimodal Grouping Captioning (MGC). MGA projects vision, language and audio
to the same common space, building vision-language, audio-language and
audiovisual-language alignment simultaneously. MGC learns how to generate text
tokens in conditions of vision, audio or their both. To promote
vision-audio-language pretraining research, we construct a large-scale
high-quality tri-modality dataset named VALOR-1M, which contains 1M audiable
videos with human annotated audiovisual captions. Extensive experiments show
that VALOR can learn strong multimodal correlations and be generalized to
various downstream tasks (e.g., retrieval, captioning and question answering),
with different input modalities (e.g., vision-language, audio-language and
audiovisual-language). VALOR achieves new state-of-the-art performances on
series of public cross-modality benchmarks. Code and data are available at
project page https://casia-iva-group.github.io/projects/VALOR.


---

