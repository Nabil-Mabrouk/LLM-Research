## Reimagining Retrieval Augmented Language Models for Answering Queries

**Published Date:** 2023-06-01T18:08:51Z

**Link:** http://arxiv.org/pdf/2306.01061v1

**Abstract:**

  We present a reality check on large language models and inspect the promise
of retrieval augmented language models in comparison. Such language models are
semi-parametric, where models integrate model parameters and knowledge from
external data sources to make their predictions, as opposed to the parametric
nature of vanilla large language models. We give initial experimental findings
that semi-parametric architectures can be enhanced with views, a query
analyzer/planner, and provenance to make a significantly more powerful system
for question answering in terms of accuracy and efficiency, and potentially for
other NLP tasks


---

## From BERT to GPT-3 Codex: Harnessing the Potential of Very Large
  Language Models for Data Management

**Published Date:** 2023-06-15T17:59:29Z

**Link:** http://arxiv.org/pdf/2306.09339v1

**Abstract:**

  Large language models have recently advanced the state of the art on many
natural language processing benchmarks. The newest generation of models can be
applied to a variety of tasks with little to no specialized training. This
technology creates various opportunities for applications in the context of
data management.
  The tutorial will introduce participants to basic background on language
models, discuss different methods to use language models, and give an overview
and short demonstration of available libraries and APIs. Models for generating
natural language will be considered as well as models, such as GPT-3 Codex,
which complete program code or generate code from natural language
instructions. Finally, the tutorial will discuss recent research in the
database community that exploits language models in the context of traditional
database systems or proposes novel system architectures that are based on them.
  The tutorial is targeted at database researchers. No prior background on
language models is required. The goal of the tutorial is to introduce database
researchers to the latest generation of language models, and to their use cases
in the domain of data management.


---

## Soft Language Clustering for Multilingual Model Pre-training

**Published Date:** 2023-06-13T08:08:08Z

**Link:** http://arxiv.org/pdf/2306.07610v1

**Abstract:**

  Multilingual pre-trained language models have demonstrated impressive
(zero-shot) cross-lingual transfer abilities, however, their performance is
hindered when the target language has distant typology from source languages or
when pre-training data is limited in size. In this paper, we propose XLM-P,
which contextually retrieves prompts as flexible guidance for encoding
instances conditionally. Our XLM-P enables (1) lightweight modeling of
language-invariant and language-specific knowledge across languages, and (2)
easy integration with other multilingual pre-training methods. On the tasks of
XTREME including text classification, sequence labeling, question answering,
and sentence retrieval, both base- and large-size language models pre-trained
with our proposed method exhibit consistent performance improvement.
Furthermore, it provides substantial advantages for low-resource languages in
unsupervised sentence retrieval and for target languages that differ greatly
from the source language in cross-lingual transfer.


---

## MetaVL: Transferring In-Context Learning Ability From Language Models to
  Vision-Language Models

**Published Date:** 2023-06-02T07:21:03Z

**Link:** http://arxiv.org/pdf/2306.01311v1

**Abstract:**

  Large-scale language models have shown the ability to adapt to a new task via
conditioning on a few demonstrations (i.e., in-context learning). However, in
the vision-language domain, most large-scale pre-trained vision-language (VL)
models do not possess the ability to conduct in-context learning. How can we
enable in-context learning for VL models? In this paper, we study an
interesting hypothesis: can we transfer the in-context learning ability from
the language domain to VL domain? Specifically, we first meta-trains a language
model to perform in-context learning on NLP tasks (as in MetaICL); then we
transfer this model to perform VL tasks by attaching a visual encoder. Our
experiments suggest that indeed in-context learning ability can be transferred
cross modalities: our model considerably improves the in-context learning
capability on VL tasks and can even compensate for the size of the model
significantly. On VQA, OK-VQA, and GQA, our method could outperform the
baseline model while having 20 times fewer parameters.


---

## A two-way translation system of Chinese sign language based on computer
  vision

**Published Date:** 2023-06-03T16:00:57Z

**Link:** http://arxiv.org/pdf/2306.02144v2

**Abstract:**

  As the main means of communication for deaf people, sign language has a
special grammatical order, so it is meaningful and valuable to develop a
real-time translation system for sign language. In the research process, we
added a TSM module to the lightweight neural network model for the large
Chinese continuous sign language dataset . It effectively improves the network
performance with high accuracy and fast recognition speed. At the same time, we
improve the Bert-Base-Chinese model to divide Chinese sentences into words and
mapping the natural word order to the statute sign language order, and finally
use the corresponding word videos in the isolated sign language dataset to
generate the sentence video, so as to achieve the function of text-to-sign
language translation. In the last of our research we built a system with sign
language recognition and translation functions, and conducted performance tests
on the complete dataset. The sign language video recognition accuracy reached
about 99.3% with a time of about 0.05 seconds, and the sign language generation
video time was about 1.3 seconds. The sign language system has good performance
performance and is feasible.


---

## Turning large language models into cognitive models

**Published Date:** 2023-06-06T18:00:01Z

**Link:** http://arxiv.org/pdf/2306.03917v1

**Abstract:**

  Large language models are powerful systems that excel at many tasks, ranging
from translation to mathematical reasoning. Yet, at the same time, these models
often show unhuman-like characteristics. In the present paper, we address this
gap and ask whether large language models can be turned into cognitive models.
We find that -- after finetuning them on data from psychological experiments --
these models offer accurate representations of human behavior, even
outperforming traditional cognitive models in two decision-making domains. In
addition, we show that their representations contain the information necessary
to model behavior on the level of individual subjects. Finally, we demonstrate
that finetuning on multiple tasks enables large language models to predict
human behavior in a previously unseen task. Taken together, these results
suggest that large, pre-trained models can be adapted to become generalist
cognitive models, thereby opening up new research directions that could
transform cognitive psychology and the behavioral sciences as a whole.


---

## MotionGPT: Human Motion as a Foreign Language

**Published Date:** 2023-06-26T15:53:02Z

**Link:** http://arxiv.org/pdf/2306.14795v2

**Abstract:**

  Though the advancement of pre-trained large language models unfolds, the
exploration of building a unified model for language and other multi-modal
data, such as motion, remains challenging and untouched so far. Fortunately,
human motion displays a semantic coupling akin to human language, often
perceived as a form of body language. By fusing language data with large-scale
motion models, motion-language pre-training that can enhance the performance of
motion-related tasks becomes feasible. Driven by this insight, we propose
MotionGPT, a unified, versatile, and user-friendly motion-language model to
handle multiple motion-relevant tasks. Specifically, we employ the discrete
vector quantization for human motion and transfer 3D motion into motion tokens,
similar to the generation process of word tokens. Building upon this "motion
vocabulary", we perform language modeling on both motion and text in a unified
manner, treating human motion as a specific language. Moreover, inspired by
prompt learning, we pre-train MotionGPT with a mixture of motion-language data
and fine-tune it on prompt-based question-and-answer tasks. Extensive
experiments demonstrate that MotionGPT achieves state-of-the-art performances
on multiple motion tasks including text-driven motion generation, motion
captioning, motion prediction, and motion in-between.


---

## Benchmarking Large Language Model Capabilities for Conditional
  Generation

**Published Date:** 2023-06-29T08:59:40Z

**Link:** http://arxiv.org/pdf/2306.16793v1

**Abstract:**

  Pre-trained large language models (PLMs) underlie most new developments in
natural language processing. They have shifted the field from
application-specific model pipelines to a single model that is adapted to a
wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside
techniques like few-shot learning, have additionally shifted the output
modality to generation instead of classification or regression. Despite their
ubiquitous use, the generation quality of language models is rarely evaluated
when these models are introduced. Additionally, it is unclear how existing
generation tasks--while they can be used to compare systems at a high
level--relate to the real world use cases for which people have been adopting
them. In this work, we discuss how to adapt existing application-specific
generation benchmarks to PLMs and provide an in-depth, empirical study of the
limitations and capabilities of PLMs in natural language generation tasks along
dimensions such as scale, architecture, input and output language. Our results
show that PLMs differ in their applicability to different data regimes and
their generalization to multiple languages and inform which PLMs to use for a
given generation task setup. We share best practices to be taken into
consideration when benchmarking generation capabilities during the development
of upcoming PLMs.


---

## T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text
  Classification

**Published Date:** 2023-06-08T07:33:22Z

**Link:** http://arxiv.org/pdf/2306.04996v1

**Abstract:**

  Cross-lingual text classification leverages text classifiers trained in a
high-resource language to perform text classification in other languages with
no or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays,
cross-lingual text classifiers are typically built on large-scale, multilingual
language models (LMs) pretrained on a variety of languages of interest.
However, the performance of these models vary significantly across languages
and classification tasks, suggesting that the superposition of the language
modelling and classification tasks is not always effective. For this reason, in
this paper we propose revisiting the classic "translate-and-test" pipeline to
neatly separate the translation and classification stages. The proposed
approach couples 1) a neural machine translator translating from the targeted
language to a high-resource language, with 2) a text classifier trained in the
high-resource language, but the neural machine translator generates "soft"
translations to permit end-to-end backpropagation during fine-tuning of the
pipeline. Extensive experiments have been carried out over three cross-lingual
text classification datasets (XNLI, MLDoc and MultiEURLEX), with the results
showing that the proposed approach has significantly improved performance over
a competitive baseline.


---

## Large language models and (non-)linguistic recursion

**Published Date:** 2023-06-12T15:50:38Z

**Link:** http://arxiv.org/pdf/2306.07195v1

**Abstract:**

  Recursion is one of the hallmarks of human language. While many design
features of language have been shown to exist in animal communication systems,
recursion has not. Previous research shows that GPT-4 is the first large
language model (LLM) to exhibit metalinguistic abilities (Begu\v{s},
D\k{a}bkowski, and Rhodes 2023). Here, we propose several prompt designs aimed
at eliciting and analyzing recursive behavior in LLMs, both linguistic and
non-linguistic. We demonstrate that when explicitly prompted, GPT-4 can both
produce and analyze recursive structures. Thus, we present one of the first
studies investigating whether meta-linguistic awareness of recursion -- a
uniquely human cognitive property -- can emerge in transformers with a high
number of parameters such as GPT-4.


---

## ToolAlpaca: Generalized Tool Learning for Language Models with 3000
  Simulated Cases

**Published Date:** 2023-06-08T15:46:32Z

**Link:** http://arxiv.org/pdf/2306.05301v1

**Abstract:**

  Enabling large language models to effectively utilize real-world tools is
crucial for achieving embodied intelligence. Existing approaches to tool
learning have primarily relied on either extremely large language models, such
as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or
have utilized supervised learning to train limited types of tools on compact
models. However, it remains uncertain whether smaller language models can
achieve generalized tool-use abilities without specific tool-specific training.
To address this question, this paper introduces ToolAlpaca, a novel framework
designed to automatically generate a tool-use corpus and learn generalized
tool-use abilities on compact language models with minimal human intervention.
Specifically, ToolAlpaca first collects a comprehensive dataset by building a
multi-agent simulation environment, which contains 3938 tool-use instances from
more than 400 real-world tool APIs spanning 50 distinct categories.
Subsequently, the constructed corpus is employed to fine-tune compact language
models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B,
respectively. Finally, we evaluate the ability of these models to utilize
previously unseen tools without specific training. Experimental results
demonstrate that ToolAlpaca achieves effective generalized tool-use
capabilities comparable to those of extremely large language models like
GPT-3.5. This validation supports the notion that learning generalized tool-use
abilities is feasible for compact language models.


---

## A Survey on Multimodal Large Language Models

**Published Date:** 2023-06-23T15:21:52Z

**Link:** http://arxiv.org/pdf/2306.13549v1

**Abstract:**

  Multimodal Large Language Model (MLLM) recently has been a new rising
research hotspot, which uses powerful Large Language Models (LLMs) as a brain
to perform multimodal tasks. The surprising emergent capabilities of MLLM, such
as writing stories based on images and OCR-free math reasoning, are rare in
traditional methods, suggesting a potential path to artificial general
intelligence. In this paper, we aim to trace and summarize the recent progress
of MLLM. First of all, we present the formulation of MLLM and delineate its
related concepts. Then, we discuss the key techniques and applications,
including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning
(M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning
(LAVR). Finally, we discuss existing challenges and point out promising
research directions. In light of the fact that the era of MLLM has only just
begun, we will keep updating this survey and hope it can inspire more research.
An associated GitHub link collecting the latest papers is available at
https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.


---

## ChessGPT: Bridging Policy Learning and Language Modeling

**Published Date:** 2023-06-15T15:35:31Z

**Link:** http://arxiv.org/pdf/2306.09200v1

**Abstract:**

  When solving decision-making tasks, humans typically depend on information
from two key sources: (1) Historical policy data, which provides interaction
replay from the environment, and (2) Analytical insights in natural language
form, exposing the invaluable thought process or strategic considerations.
Despite this, the majority of preceding research focuses on only one source:
they either use historical replay exclusively to directly learn policy or value
functions, or engaged in language model training utilizing mere language
corpus. In this paper, we argue that a powerful autonomous agent should cover
both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning
and language modeling by integrating data from these two sources in Chess
games. Specifically, we build a large-scale game and language dataset related
to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and
ChessGPT, integrating policy learning and language modeling. Finally, we
propose a full evaluation framework for evaluating language model's chess
ability. Experimental results validate our model and dataset's effectiveness.
We open source our code, model, and dataset at
https://github.com/waterhorse1/ChessGPT.


---

## Radiology-GPT: A Large Language Model for Radiology

**Published Date:** 2023-06-14T17:57:24Z

**Link:** http://arxiv.org/pdf/2306.08666v1

**Abstract:**

  We introduce Radiology-GPT, a large language model for radiology. Using an
instruction tuning approach on an extensive dataset of radiology domain
knowledge, Radiology-GPT demonstrates superior performance compared to general
language models such as StableLM, Dolly and LLaMA. It exhibits significant
versatility in radiological diagnosis, research, and communication. This work
serves as a catalyst for future developments in clinical NLP. The successful
implementation of Radiology-GPT is indicative of the potential of localizing
generative large language models, specifically tailored for distinctive medical
specialties, while ensuring adherence to privacy standards such as HIPAA. The
prospect of developing individualized, large-scale language models that cater
to specific needs of various hospitals presents a promising direction. The
fusion of conversational competence and domain-specific knowledge in these
models is set to foster future development in healthcare AI. A demo of
Radiology-GPT is available at
https://huggingface.co/spaces/allen-eric/radiology-gpt.


---

## Efficient Spoken Language Recognition via Multilabel Classification

**Published Date:** 2023-06-02T23:04:19Z

**Link:** http://arxiv.org/pdf/2306.01945v1

**Abstract:**

  Spoken language recognition (SLR) is the task of automatically identifying
the language present in a speech signal. Existing SLR models are either too
computationally expensive or too large to run effectively on devices with
limited resources. For real-world deployment, a model should also gracefully
handle unseen languages outside of the target language set, yet prior work has
focused on closed-set classification where all input languages are known
a-priori. In this paper we address these two limitations: we explore efficient
model architectures for SLR based on convolutional networks, and propose a
multilabel training strategy to handle non-target languages at inference time.
Using the VoxLingua107 dataset, we show that our models obtain competitive
results while being orders of magnitude smaller and faster than current
state-of-the-art methods, and that our multilabel strategy is more robust to
unseen non-target languages compared to multiclass classification.


---

## Knowledge Distillation of Large Language Models

**Published Date:** 2023-06-14T14:44:03Z

**Link:** http://arxiv.org/pdf/2306.08543v1

**Abstract:**

  Knowledge Distillation (KD) is a promising technique for reducing the high
computational demand of large language models (LLMs). However, previous KD
methods are primarily applied to white-box classification models or training
small models to imitate black-box model APIs like ChatGPT. How to effectively
distill the knowledge from white-box generative LLMs is still under-explored,
which becomes more and more important with the prosperity of LLMs. In this
work, we propose MiniLLM that distills smaller language models from generative
larger language models. We first replace the forward Kullback-Leibler
divergence (KLD) objective in the standard KD approaches with reverse KLD,
which is more suitable for KD on generative language models, to prevent the
student model from overestimating the low-probability regions of the teacher
distribution. Then, we derive an effective optimization approach to learn this
objective. Extensive experiments in the instruction-following setting show that
the MiniLLM models generate more precise responses with the higher overall
quality, lower exposure bias, better calibration, and higher long-text
generation performance. Our method is also scalable for different model
families with 120M to 13B parameters. We will release our code and model
checkpoints at https://aka.ms/MiniLLM.


---

## Language acquisition: do children and language models follow similar
  learning stages?

**Published Date:** 2023-06-06T11:08:20Z

**Link:** http://arxiv.org/pdf/2306.03586v1

**Abstract:**

  During language acquisition, children follow a typical sequence of learning
stages, whereby they first learn to categorize phonemes before they develop
their lexicon and eventually master increasingly complex syntactic structures.
However, the computational principles that lead to this learning trajectory
remain largely unknown. To investigate this, we here compare the learning
trajectories of deep language models to those of children. Specifically, we
test whether, during its training, GPT-2 exhibits stages of language
acquisition comparable to those observed in children aged between 18 months and
6 years. For this, we train 48 GPT-2 models from scratch and evaluate their
syntactic and semantic abilities at each training step, using 96 probes curated
from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these
evaluations with the behavior of 54 children during language production. Our
analyses reveal three main findings. First, similarly to children, the language
models tend to learn linguistic skills in a systematic order. Second, this
learning scheme is parallel: the language tasks that are learned last improve
from the very first training steps. Third, some - but not all - learning stages
are shared between children and these language models. Overall, these results
shed new light on the principles of language acquisition, and highlight
important divergences in how humans and modern algorithms learn to process
natural language.


---

## Large Language Models Converge on Brain-Like Word Representations

**Published Date:** 2023-06-02T22:09:46Z

**Link:** http://arxiv.org/pdf/2306.01930v1

**Abstract:**

  One of the greatest puzzles of all time is how understanding arises from
neural mechanics. Our brains are networks of billions of biological neurons
transmitting chemical and electrical signals along their connections. Large
language models are networks of millions or billions of digital neurons,
implementing functions that read the output of other functions in complex
networks. The failure to see how meaning would arise from such mechanics has
led many cognitive scientists and philosophers to various forms of dualism --
and many artificial intelligence researchers to dismiss large language models
as stochastic parrots or jpeg-like compressions of text corpora. We show that
human-like representations arise in large language models. Specifically, the
larger neural language models get, the more their representations are
structurally similar to neural response measurements from brain imaging.


---

## Large Multimodal Models: Notes on CVPR 2023 Tutorial

**Published Date:** 2023-06-26T17:59:31Z

**Link:** http://arxiv.org/pdf/2306.14895v1

**Abstract:**

  This tutorial note summarizes the presentation on ``Large Multimodal Models:
Towards Building and Surpassing Multimodal GPT-4'', a part of CVPR 2023
tutorial on ``Recent Advances in Vision Foundation Models''. The tutorial
consists of three parts. We first introduce the background on recent GPT-like
large models for vision-and-language modeling to motivate the research in
instruction-tuned large multimodal models (LMMs). As a pre-requisite, we
describe the basics of instruction-tuning in large language models, which is
further extended to the multimodal space. Lastly, we illustrate how to build
the minimum prototype of multimodal GPT-4 like models with the open-source
resource, and review the recently emerged topics.


---

## Questioning the Survey Responses of Large Language Models

**Published Date:** 2023-06-13T17:48:27Z

**Link:** http://arxiv.org/pdf/2306.07951v1

**Abstract:**

  As large language models increase in capability, researchers have started to
conduct surveys of all kinds on these models with varying scientific
motivations. In this work, we examine what we can learn from a model's survey
responses on the basis of the well-established American Community Survey (ACS)
by the U.S. Census Bureau. Evaluating more than a dozen different models,
varying in size from a few hundred million to ten billion parameters, hundreds
of thousands of times each on questions from the ACS, we systematically
establish two dominant patterns. First, smaller models have a significant
position and labeling bias, for example, towards survey responses labeled with
the letter "A". This A-bias diminishes, albeit slowly, as model size increases.
Second, when adjusting for this labeling bias through randomized answer
ordering, models still do not trend toward US population statistics or those of
any cognizable population. Rather, models across the board trend toward
uniformly random aggregate statistics over survey responses. This pattern is
robust to various different ways of prompting the model, including what is the
de-facto standard. Our findings demonstrate that aggregate statistics of a
language model's survey responses lack the signals found in human populations.
This absence of statistical signal cautions about the use of survey responses
from large language models at present time.


---

## Iterative Translation Refinement with Large Language Models

**Published Date:** 2023-06-06T16:51:03Z

**Link:** http://arxiv.org/pdf/2306.03856v1

**Abstract:**

  Large language models have shown surprising performances in understanding
instructions and performing natural language tasks. In this paper, we propose
iterative translation refinement to leverage the power of large language models
for more natural translation and post-editing. We show that by simply involving
a large language model in an iterative process, the output quality improves
beyond mere translation. Extensive test scenarios with GPT-3.5 reveal that
although iterations reduce string-based metric scores, neural metrics indicate
comparable if not improved translation quality. Further, human evaluations
demonstrate that our method effectively reduces translationese compared to
initial GPT translations and even human references, especially for into-English
directions. Ablation studies underscore the importance of anchoring the
refinement process to the source input and a reasonable initial translation.


---

## MetaVL: Transferring In-Context Learning Ability From Language Models to
  Vision-Language Models

**Published Date:** 2023-06-02T07:21:03Z

**Link:** http://arxiv.org/pdf/2306.01311v1

**Abstract:**

  Large-scale language models have shown the ability to adapt to a new task via
conditioning on a few demonstrations (i.e., in-context learning). However, in
the vision-language domain, most large-scale pre-trained vision-language (VL)
models do not possess the ability to conduct in-context learning. How can we
enable in-context learning for VL models? In this paper, we study an
interesting hypothesis: can we transfer the in-context learning ability from
the language domain to VL domain? Specifically, we first meta-trains a language
model to perform in-context learning on NLP tasks (as in MetaICL); then we
transfer this model to perform VL tasks by attaching a visual encoder. Our
experiments suggest that indeed in-context learning ability can be transferred
cross modalities: our model considerably improves the in-context learning
capability on VL tasks and can even compensate for the size of the model
significantly. On VQA, OK-VQA, and GQA, our method could outperform the
baseline model while having 20 times fewer parameters.


---

## T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text
  Classification

**Published Date:** 2023-06-08T07:33:22Z

**Link:** http://arxiv.org/pdf/2306.04996v1

**Abstract:**

  Cross-lingual text classification leverages text classifiers trained in a
high-resource language to perform text classification in other languages with
no or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays,
cross-lingual text classifiers are typically built on large-scale, multilingual
language models (LMs) pretrained on a variety of languages of interest.
However, the performance of these models vary significantly across languages
and classification tasks, suggesting that the superposition of the language
modelling and classification tasks is not always effective. For this reason, in
this paper we propose revisiting the classic "translate-and-test" pipeline to
neatly separate the translation and classification stages. The proposed
approach couples 1) a neural machine translator translating from the targeted
language to a high-resource language, with 2) a text classifier trained in the
high-resource language, but the neural machine translator generates "soft"
translations to permit end-to-end backpropagation during fine-tuning of the
pipeline. Extensive experiments have been carried out over three cross-lingual
text classification datasets (XNLI, MLDoc and MultiEURLEX), with the results
showing that the proposed approach has significantly improved performance over
a competitive baseline.


---

## Knowledge Distillation of Large Language Models

**Published Date:** 2023-06-14T14:44:03Z

**Link:** http://arxiv.org/pdf/2306.08543v1

**Abstract:**

  Knowledge Distillation (KD) is a promising technique for reducing the high
computational demand of large language models (LLMs). However, previous KD
methods are primarily applied to white-box classification models or training
small models to imitate black-box model APIs like ChatGPT. How to effectively
distill the knowledge from white-box generative LLMs is still under-explored,
which becomes more and more important with the prosperity of LLMs. In this
work, we propose MiniLLM that distills smaller language models from generative
larger language models. We first replace the forward Kullback-Leibler
divergence (KLD) objective in the standard KD approaches with reverse KLD,
which is more suitable for KD on generative language models, to prevent the
student model from overestimating the low-probability regions of the teacher
distribution. Then, we derive an effective optimization approach to learn this
objective. Extensive experiments in the instruction-following setting show that
the MiniLLM models generate more precise responses with the higher overall
quality, lower exposure bias, better calibration, and higher long-text
generation performance. Our method is also scalable for different model
families with 120M to 13B parameters. We will release our code and model
checkpoints at https://aka.ms/MiniLLM.


---

## Evaluating the Effectiveness of Natural Language Inference for Hate
  Speech Detection in Languages with Limited Labeled Data

**Published Date:** 2023-06-06T14:40:41Z

**Link:** http://arxiv.org/pdf/2306.03722v2

**Abstract:**

  Most research on hate speech detection has focused on English where a
sizeable amount of labeled training data is available. However, to expand hate
speech detection into more languages, approaches that require minimal training
data are needed. In this paper, we test whether natural language inference
(NLI) models which perform well in zero- and few-shot settings can benefit hate
speech detection performance in scenarios where only a limited amount of
labeled data is available in the target language. Our evaluation on five
languages demonstrates large performance improvements of NLI fine-tuning over
direct fine-tuning in the target language. However, the effectiveness of
previous work that proposed intermediate fine-tuning on English data is hard to
match. Only in settings where the English training data does not match the test
domain, can our customised NLI-formulation outperform intermediate fine-tuning
on English. Based on our extensive experiments, we propose a set of
recommendations for hate speech detection in languages where minimal labeled
training data is available.


---

## Assessing Phrase Break of ESL Speech with Pre-trained Language Models
  and Large Language Models

**Published Date:** 2023-06-08T07:10:39Z

**Link:** http://arxiv.org/pdf/2306.04980v1

**Abstract:**

  This work introduces approaches to assessing phrase breaks in ESL learners'
speech using pre-trained language models (PLMs) and large language models
(LLMs). There are two tasks: overall assessment of phrase break for a speech
clip and fine-grained assessment of every possible phrase break position. To
leverage NLP models, speech input is first force-aligned with texts, and then
pre-processed into a token sequence, including words and phrase break
information. To utilize PLMs, we propose a pre-training and fine-tuning
pipeline with the processed tokens. This process includes pre-training with a
replaced break token detection module and fine-tuning with text classification
and sequence labeling. To employ LLMs, we design prompts for ChatGPT. The
experiments show that with the PLMs, the dependence on labeled training data
has been greatly reduced, and the performance has improved. Meanwhile, we
verify that ChatGPT, a renowned LLM, has potential for further advancement in
this area.


---

## LLaVA-Med: Training a Large Language-and-Vision Assistant for
  Biomedicine in One Day

**Published Date:** 2023-06-01T16:50:07Z

**Link:** http://arxiv.org/pdf/2306.00890v1

**Abstract:**

  Conversational generative AI has demonstrated remarkable promise for
empowering biomedical practitioners, but current investigations focus on
unimodal text. Multimodal conversational AI has seen rapid progress by
leveraging billions of image-text pairs from the public web, but such
general-domain vision-language models still lack sophistication in
understanding and conversing about biomedical images. In this paper, we propose
a cost-efficient approach for training a vision-language conversational
assistant that can answer open-ended research questions of biomedical images.
The key idea is to leverage a large-scale, broad-coverage biomedical
figure-caption dataset extracted from PubMed Central, use GPT-4 to
self-instruct open-ended instruction-following data from the captions, and then
fine-tune a large general-domain vision-language model using a novel curriculum
learning method. Specifically, the model first learns to align biomedical
vocabulary using the figure-caption pairs as is, then learns to master
open-ended conversational semantics using GPT-4 generated instruction-following
data, broadly mimicking how a layperson gradually acquires biomedical
knowledge. This enables us to train a Large Language and Vision Assistant for
BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med
exhibits excellent multimodal conversational capability and can follow
open-ended instruction to assist with inquiries about a biomedical image. On
three standard biomedical visual question answering datasets, LLaVA-Med
outperforms previous supervised state-of-the-art on certain metrics. To
facilitate biomedical multimodal research, we will release our
instruction-following data and the LLaVA-Med model.


---

## MetaVL: Transferring In-Context Learning Ability From Language Models to
  Vision-Language Models

**first_author:** Masoud Monajatipoor et al.

**Published Date:** 2023-06-02T07:21:03Z

**Link:** http://arxiv.org/pdf/2306.01311v1

**Abstract:**

  Large-scale language models have shown the ability to adapt to a new task via
conditioning on a few demonstrations (i.e., in-context learning). However, in
the vision-language domain, most large-scale pre-trained vision-language (VL)
models do not possess the ability to conduct in-context learning. How can we
enable in-context learning for VL models? In this paper, we study an
interesting hypothesis: can we transfer the in-context learning ability from
the language domain to VL domain? Specifically, we first meta-trains a language
model to perform in-context learning on NLP tasks (as in MetaICL); then we
transfer this model to perform VL tasks by attaching a visual encoder. Our
experiments suggest that indeed in-context learning ability can be transferred
cross modalities: our model considerably improves the in-context learning
capability on VL tasks and can even compensate for the size of the model
significantly. On VQA, OK-VQA, and GQA, our method could outperform the
baseline model while having 20 times fewer parameters.


---

## Knowledge Distillation of Large Language Models

**first_author:** Yuxian Gu et al.

**Published Date:** 2023-06-14T14:44:03Z

**Link:** http://arxiv.org/pdf/2306.08543v1

**Abstract:**

  Knowledge Distillation (KD) is a promising technique for reducing the high
computational demand of large language models (LLMs). However, previous KD
methods are primarily applied to white-box classification models or training
small models to imitate black-box model APIs like ChatGPT. How to effectively
distill the knowledge from white-box generative LLMs is still under-explored,
which becomes more and more important with the prosperity of LLMs. In this
work, we propose MiniLLM that distills smaller language models from generative
larger language models. We first replace the forward Kullback-Leibler
divergence (KLD) objective in the standard KD approaches with reverse KLD,
which is more suitable for KD on generative language models, to prevent the
student model from overestimating the low-probability regions of the teacher
distribution. Then, we derive an effective optimization approach to learn this
objective. Extensive experiments in the instruction-following setting show that
the MiniLLM models generate more precise responses with the higher overall
quality, lower exposure bias, better calibration, and higher long-text
generation performance. Our method is also scalable for different model
families with 120M to 13B parameters. We will release our code and model
checkpoints at https://aka.ms/MiniLLM.


---

## Large Multimodal Models: Notes on CVPR 2023 Tutorial

**first_author:** Chunyuan Li et al.

**Published Date:** 2023-06-26T17:59:31Z

**Link:** http://arxiv.org/pdf/2306.14895v1

**Abstract:**

  This tutorial note summarizes the presentation on ``Large Multimodal Models:
Towards Building and Surpassing Multimodal GPT-4'', a part of CVPR 2023
tutorial on ``Recent Advances in Vision Foundation Models''. The tutorial
consists of three parts. We first introduce the background on recent GPT-like
large models for vision-and-language modeling to motivate the research in
instruction-tuned large multimodal models (LMMs). As a pre-requisite, we
describe the basics of instruction-tuning in large language models, which is
further extended to the multimodal space. Lastly, we illustrate how to build
the minimum prototype of multimodal GPT-4 like models with the open-source
resource, and review the recently emerged topics.


---

