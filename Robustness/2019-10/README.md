## Neural Cross-Lingual Relation Extraction Based on Bilingual Word
  Embedding Mapping

**Published Date:** 2019-10-31T19:30:54Z

**Link:** http://arxiv.org/pdf/1911.00069v1

**Abstract:**

  Relation extraction (RE) seeks to detect and classify semantic relationships
between entities, which provides useful information for many NLP applications.
Since the state-of-the-art RE models require large amounts of manually
annotated data and language-specific resources to achieve high accuracy, it is
very challenging to transfer an RE model of a resource-rich language to a
resource-poor language. In this paper, we propose a new approach for
cross-lingual RE model transfer based on bilingual word embedding mapping. It
projects word embeddings from a target language to a source language, so that a
well-trained source-language neural network RE model can be directly applied to
the target language. Experiment results show that the proposed approach
achieves very good performance for a number of target languages on both
in-house and open datasets, using a small bilingual dictionary with only 1K
word pairs.


---

## Alternating Recurrent Dialog Model with Large-scale Pre-trained Language
  Models

**Published Date:** 2019-10-09T02:31:37Z

**Link:** http://arxiv.org/pdf/1910.03756v3

**Abstract:**

  Existing dialog system models require extensive human annotations and are
difficult to generalize to different tasks. The recent success of large
pre-trained language models such as BERT and GPT-2 (Devlin et al., 2019;
Radford et al., 2019) have suggested the effectiveness of incorporating
language priors in down-stream NLP tasks. However, how much pre-trained
language models can help dialog response generation is still under exploration.
In this paper, we propose a simple, general, and effective framework:
Alternating Roles Dialog Model (ARDM). ARDM models each speaker separately and
takes advantage of the large pre-trained language model. It requires no
supervision from human annotations such as belief states or dialog acts to
achieve effective conversations. ARDM outperforms or is on par with
state-of-the-art methods on two popular task-oriented dialog datasets:
CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging,
non-collaborative tasks such as persuasion. In persuasion tasks, ARDM is
capable of generating human-like responses to persuade people to donate to a
charity.


---

## Federated Learning of N-gram Language Models

**Published Date:** 2019-10-08T14:48:43Z

**Link:** http://arxiv.org/pdf/1910.03432v1

**Abstract:**

  We propose algorithms to train production-quality n-gram language models
using federated learning. Federated learning is a distributed computation
platform that can be used to train global models for portable devices such as
smart phones. Federated learning is especially relevant for applications
handling privacy-sensitive data, such as virtual keyboards, because training is
performed without the users' data ever leaving their devices. While the
principles of federated learning are fairly generic, its methodology assumes
that the underlying models are neural networks. However, virtual keyboards are
typically powered by n-gram language models for latency reasons.
  We propose to train a recurrent neural network language model using the
decentralized FederatedAveraging algorithm and to approximate this federated
model server-side with an n-gram model that can be deployed to devices for fast
inference. Our technical contributions include ways of handling large
vocabularies, algorithms to correct capitalization errors in user data, and
efficient finite state transducer algorithms to convert word language models to
word-piece language models and vice versa. The n-gram language models trained
with federated learning are compared to n-grams trained with traditional
server-based algorithms using A/B tests on tens of millions of users of virtual
keyboard. Results are presented for two languages, American English and
Brazilian Portuguese. This work demonstrates that high-quality n-gram language
models can be trained directly on client mobile devices without sensitive
training data ever leaving the devices.


---

## Neural Cross-Lingual Relation Extraction Based on Bilingual Word
  Embedding Mapping

**Published Date:** 2019-10-31T19:30:54Z

**Link:** http://arxiv.org/pdf/1911.00069v1

**Abstract:**

  Relation extraction (RE) seeks to detect and classify semantic relationships
between entities, which provides useful information for many NLP applications.
Since the state-of-the-art RE models require large amounts of manually
annotated data and language-specific resources to achieve high accuracy, it is
very challenging to transfer an RE model of a resource-rich language to a
resource-poor language. In this paper, we propose a new approach for
cross-lingual RE model transfer based on bilingual word embedding mapping. It
projects word embeddings from a target language to a source language, so that a
well-trained source-language neural network RE model can be directly applied to
the target language. Experiment results show that the proposed approach
achieves very good performance for a number of target languages on both
in-house and open datasets, using a small bilingual dictionary with only 1K
word pairs.


---

## Alternating Recurrent Dialog Model with Large-scale Pre-trained Language
  Models

**Published Date:** 2019-10-09T02:31:37Z

**Link:** http://arxiv.org/pdf/1910.03756v3

**Abstract:**

  Existing dialog system models require extensive human annotations and are
difficult to generalize to different tasks. The recent success of large
pre-trained language models such as BERT and GPT-2 (Devlin et al., 2019;
Radford et al., 2019) have suggested the effectiveness of incorporating
language priors in down-stream NLP tasks. However, how much pre-trained
language models can help dialog response generation is still under exploration.
In this paper, we propose a simple, general, and effective framework:
Alternating Roles Dialog Model (ARDM). ARDM models each speaker separately and
takes advantage of the large pre-trained language model. It requires no
supervision from human annotations such as belief states or dialog acts to
achieve effective conversations. ARDM outperforms or is on par with
state-of-the-art methods on two popular task-oriented dialog datasets:
CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging,
non-collaborative tasks such as persuasion. In persuasion tasks, ARDM is
capable of generating human-like responses to persuade people to donate to a
charity.


---

## Neural Cross-Lingual Relation Extraction Based on Bilingual Word
  Embedding Mapping

**first_author:** Jian Ni et al.

**Published Date:** 2019-10-31T19:30:54Z

**Link:** http://arxiv.org/pdf/1911.00069v1

**Abstract:**

  Relation extraction (RE) seeks to detect and classify semantic relationships
between entities, which provides useful information for many NLP applications.
Since the state-of-the-art RE models require large amounts of manually
annotated data and language-specific resources to achieve high accuracy, it is
very challenging to transfer an RE model of a resource-rich language to a
resource-poor language. In this paper, we propose a new approach for
cross-lingual RE model transfer based on bilingual word embedding mapping. It
projects word embeddings from a target language to a source language, so that a
well-trained source-language neural network RE model can be directly applied to
the target language. Experiment results show that the proposed approach
achieves very good performance for a number of target languages on both
in-house and open datasets, using a small bilingual dictionary with only 1K
word pairs.


---

## Alternating Recurrent Dialog Model with Large-scale Pre-trained Language
  Models

**first_author:** Qingyang Wu et al.

**Published Date:** 2019-10-09T02:31:37Z

**Link:** http://arxiv.org/pdf/1910.03756v3

**Abstract:**

  Existing dialog system models require extensive human annotations and are
difficult to generalize to different tasks. The recent success of large
pre-trained language models such as BERT and GPT-2 (Devlin et al., 2019;
Radford et al., 2019) have suggested the effectiveness of incorporating
language priors in down-stream NLP tasks. However, how much pre-trained
language models can help dialog response generation is still under exploration.
In this paper, we propose a simple, general, and effective framework:
Alternating Roles Dialog Model (ARDM). ARDM models each speaker separately and
takes advantage of the large pre-trained language model. It requires no
supervision from human annotations such as belief states or dialog acts to
achieve effective conversations. ARDM outperforms or is on par with
state-of-the-art methods on two popular task-oriented dialog datasets:
CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging,
non-collaborative tasks such as persuasion. In persuasion tasks, ARDM is
capable of generating human-like responses to persuade people to donate to a
charity.


---

