## Language Modeling with Sparse Product of Sememe Experts

**Published Date:** 2018-10-29T20:13:05Z

**Link:** http://arxiv.org/pdf/1810.12387v1

**Abstract:**

  Most language modeling methods rely on large-scale data to statistically
learn the sequential patterns of words. In this paper, we argue that words are
atomic language units but not necessarily atomic semantic units. Inspired by
HowNet, we use sememes, the minimum semantic units in human languages, to
represent the implicit semantics behind words for language modeling, named
Sememe-Driven Language Model (SDLM). More specifically, to predict the next
word, SDLM first estimates the sememe distribution gave textual context.
Afterward, it regards each sememe as a distinct semantic expert, and these
experts jointly identify the most probable senses and the corresponding word.
In this way, SDLM enables language models to work beyond word-level
manipulation to fine-grained sememe-level semantics and offers us more powerful
tools to fine-tune language models and improve the interpretability as well as
the robustness of language models. Experiments on language modeling and the
downstream application of headline gener- ation demonstrate the significant
effect of SDLM. Source code and data used in the experiments can be accessed at
https:// github.com/thunlp/SDLM-pytorch.


---

## Multi-Source Cross-Lingual Model Transfer: Learning What to Share

**Published Date:** 2018-10-08T16:11:01Z

**Link:** http://arxiv.org/pdf/1810.03552v3

**Abstract:**

  Modern NLP applications have enjoyed a great boost utilizing neural networks
models. Such deep neural models, however, are not applicable to most human
languages due to the lack of annotated training data for various NLP tasks.
Cross-lingual transfer learning (CLTL) is a viable method for building NLP
models for a low-resource target language by leveraging labeled data from other
(source) languages. In this work, we focus on the multilingual transfer setting
where training data in multiple source languages is leveraged to further boost
target language performance.
  Unlike most existing methods that rely only on language-invariant features
for CLTL, our approach coherently utilizes both language-invariant and
language-specific features at instance level. Our model leverages adversarial
networks to learn language-invariant features, and mixture-of-experts models to
dynamically exploit the similarity between the target language and each
individual source language. This enables our model to learn effectively what to
share between various languages in the multilingual setup. Moreover, when
coupled with unsupervised multilingual embeddings, our model can operate in a
zero-resource setting where neither target language training data nor
cross-lingual resources are available. Our model achieves significant
performance gains over prior art, as shown in an extensive set of experiments
over multiple text classification and sequence tagging tasks including a
large-scale industry dataset.


---

## Strategies for Language Identification in Code-Mixed Low Resource
  Languages

**Published Date:** 2018-10-16T17:35:31Z

**Link:** http://arxiv.org/pdf/1810.07156v2

**Abstract:**

  In recent years, substantial work has been done on language tagging of
code-mixed data, but most of them use large amounts of data to build their
models. In this article, we present three strategies to build a word level
language tagger for code-mixed data using very low resources. Each of them
secured an accuracy higher than our baseline model, and the best performing
system got an accuracy around 91%. Combining all, the ensemble system achieved
an accuracy of around 92.6%.


---

## Language Modeling with Sparse Product of Sememe Experts

**Published Date:** 2018-10-29T20:13:05Z

**Link:** http://arxiv.org/pdf/1810.12387v1

**Abstract:**

  Most language modeling methods rely on large-scale data to statistically
learn the sequential patterns of words. In this paper, we argue that words are
atomic language units but not necessarily atomic semantic units. Inspired by
HowNet, we use sememes, the minimum semantic units in human languages, to
represent the implicit semantics behind words for language modeling, named
Sememe-Driven Language Model (SDLM). More specifically, to predict the next
word, SDLM first estimates the sememe distribution gave textual context.
Afterward, it regards each sememe as a distinct semantic expert, and these
experts jointly identify the most probable senses and the corresponding word.
In this way, SDLM enables language models to work beyond word-level
manipulation to fine-grained sememe-level semantics and offers us more powerful
tools to fine-tune language models and improve the interpretability as well as
the robustness of language models. Experiments on language modeling and the
downstream application of headline gener- ation demonstrate the significant
effect of SDLM. Source code and data used in the experiments can be accessed at
https:// github.com/thunlp/SDLM-pytorch.


---

## Language Modeling with Sparse Product of Sememe Experts

**first_author:** Yihong Gu et al.

**Published Date:** 2018-10-29T20:13:05Z

**Link:** http://arxiv.org/pdf/1810.12387v1

**Abstract:**

  Most language modeling methods rely on large-scale data to statistically
learn the sequential patterns of words. In this paper, we argue that words are
atomic language units but not necessarily atomic semantic units. Inspired by
HowNet, we use sememes, the minimum semantic units in human languages, to
represent the implicit semantics behind words for language modeling, named
Sememe-Driven Language Model (SDLM). More specifically, to predict the next
word, SDLM first estimates the sememe distribution gave textual context.
Afterward, it regards each sememe as a distinct semantic expert, and these
experts jointly identify the most probable senses and the corresponding word.
In this way, SDLM enables language models to work beyond word-level
manipulation to fine-grained sememe-level semantics and offers us more powerful
tools to fine-tune language models and improve the interpretability as well as
the robustness of language models. Experiments on language modeling and the
downstream application of headline gener- ation demonstrate the significant
effect of SDLM. Source code and data used in the experiments can be accessed at
https:// github.com/thunlp/SDLM-pytorch.


---

