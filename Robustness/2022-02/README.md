## Cedille: A large autoregressive French language model

**Published Date:** 2022-02-07T17:40:43Z

**Link:** http://arxiv.org/pdf/2202.03371v1

**Abstract:**

  Scaling up the size and training of autoregressive language models has
enabled novel ways of solving Natural Language Processing tasks using zero-shot
and few-shot learning. While extreme-scale language models such as GPT-3 offer
multilingual capabilities, zero-shot learning for languages other than English
remain largely unexplored. Here, we introduce Cedille, a large open source
auto-regressive language model, specifically trained for the French language.
Our results show that Cedille outperforms existing French language models and
is competitive with GPT-3 on a range of French zero-shot benchmarks.
Furthermore, we provide an in-depth comparison of the toxicity exhibited by
these models, showing that Cedille marks an improvement in language model
safety thanks to dataset filtering.


---

## Reward Modeling for Mitigating Toxicity in Transformer-based Language
  Models

**Published Date:** 2022-02-19T19:26:22Z

**Link:** http://arxiv.org/pdf/2202.09662v6

**Abstract:**

  Transformer-based language models are able to generate fluent text and be
efficiently adapted across various natural language generation tasks. However,
language models that are pretrained on large unlabeled web text corpora have
been shown to suffer from degenerating toxic content and social bias behaviors,
consequently hindering their safe deployment. Various detoxification methods
were proposed to mitigate the language model's toxicity; however, these methods
struggled to detoxify language models when conditioned on prompts that contain
specific social identities related to gender, race, or religion. In this study,
we propose Reinforce-Detoxify; A reinforcement learning-based method for
mitigating toxicity in language models. We address the challenge of safety in
language models and propose a new reward model that is able to detect toxic
content and mitigate unintended bias towards social identities in toxicity
prediction. The experiments demonstrate that the Reinforce-Detoxify method for
language model detoxification outperforms existing detoxification approaches in
automatic evaluation metrics, indicating the ability of our approach in
language model detoxification and less prone to unintended bias toward social
identities in generated content.


---

## Cedille: A large autoregressive French language model

**Published Date:** 2022-02-07T17:40:43Z

**Link:** http://arxiv.org/pdf/2202.03371v1

**Abstract:**

  Scaling up the size and training of autoregressive language models has
enabled novel ways of solving Natural Language Processing tasks using zero-shot
and few-shot learning. While extreme-scale language models such as GPT-3 offer
multilingual capabilities, zero-shot learning for languages other than English
remain largely unexplored. Here, we introduce Cedille, a large open source
auto-regressive language model, specifically trained for the French language.
Our results show that Cedille outperforms existing French language models and
is competitive with GPT-3 on a range of French zero-shot benchmarks.
Furthermore, we provide an in-depth comparison of the toxicity exhibited by
these models, showing that Cedille marks an improvement in language model
safety thanks to dataset filtering.


---

## Deduplicating Training Data Mitigates Privacy Risks in Language Models

**Published Date:** 2022-02-14T08:20:15Z

**Link:** http://arxiv.org/pdf/2202.06539v3

**Abstract:**

  Past work has shown that large language models are susceptible to privacy
attacks, where adversaries generate sequences from a trained model and detect
which sequences are memorized from the training set. In this work, we show that
the success of these attacks is largely due to duplication in commonly used
web-scraped training sets. We first show that the rate at which language models
regenerate training sequences is superlinearly related to a sequence's count in
the training set. For instance, a sequence that is present 10 times in the
training data is on average generated ~1000 times more often than a sequence
that is present only once. We next show that existing methods for detecting
memorized sequences have near-chance accuracy on non-duplicated training
sequences. Finally, we find that after applying methods to deduplicate training
data, language models are considerably more secure against these types of
privacy attacks. Taken together, our results motivate an increased focus on
deduplication in privacy-sensitive applications and a reevaluation of the
practicality of existing privacy attacks.


---

## Cedille: A large autoregressive French language model

**first_author:** Martin Müller et al.

**Published Date:** 2022-02-07T17:40:43Z

**Link:** http://arxiv.org/pdf/2202.03371v1

**Abstract:**

  Scaling up the size and training of autoregressive language models has
enabled novel ways of solving Natural Language Processing tasks using zero-shot
and few-shot learning. While extreme-scale language models such as GPT-3 offer
multilingual capabilities, zero-shot learning for languages other than English
remain largely unexplored. Here, we introduce Cedille, a large open source
auto-regressive language model, specifically trained for the French language.
Our results show that Cedille outperforms existing French language models and
is competitive with GPT-3 on a range of French zero-shot benchmarks.
Furthermore, we provide an in-depth comparison of the toxicity exhibited by
these models, showing that Cedille marks an improvement in language model
safety thanks to dataset filtering.


---

