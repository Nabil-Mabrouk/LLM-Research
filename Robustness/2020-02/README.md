## From English To Foreign Languages: Transferring Pre-trained Language
  Models

**Published Date:** 2020-02-18T00:22:54Z

**Link:** http://arxiv.org/pdf/2002.07306v2

**Abstract:**

  Pre-trained models have demonstrated their effectiveness in many downstream
natural language processing (NLP) tasks. The availability of multilingual
pre-trained models enables zero-shot transfer of NLP tasks from high resource
languages to low resource ones. However, recent research in improving
pre-trained models focuses heavily on English. While it is possible to train
the latest neural architectures for other languages from scratch, it is
undesirable due to the required amount of compute. In this work, we tackle the
problem of transferring an existing pre-trained model from English to other
languages under a limited computational budget. With a single GPU, our approach
can obtain a foreign BERT base model within a day and a foreign BERT large
within two days. Furthermore, evaluating our models on six languages, we
demonstrate that our models are better than multilingual BERT on two zero-shot
tasks: natural language inference and dependency parsing.


---

## Universal Phone Recognition with a Multilingual Allophone System

**Published Date:** 2020-02-26T21:28:57Z

**Link:** http://arxiv.org/pdf/2002.11800v1

**Abstract:**

  Multilingual models can improve language processing, particularly for low
resource situations, by sharing parameters across languages. Multilingual
acoustic models, however, generally ignore the difference between phonemes
(sounds that can support lexical contrasts in a particular language) and their
corresponding phones (the sounds that are actually spoken, which are language
independent). This can lead to performance degradation when combining a variety
of training languages, as identically annotated phonemes can actually
correspond to several different underlying phonetic realizations. In this work,
we propose a joint model of both language-independent phone and
language-dependent phoneme distributions. In multilingual ASR experiments over
11 languages, we find that this model improves testing performance by 2%
phoneme error rate absolute in low-resource conditions. Additionally, because
we are explicitly modeling language-independent phones, we can build a
(nearly-)universal phone recognizer that, when combined with the PHOIBLE large,
manually curated database of phone inventories, can be customized into 2,000
language dependent recognizers. Experiments on two low-resourced indigenous
languages, Inuktitut and Tusom, show that our recognizer achieves phone
accuracy improvements of more than 17%, moving a step closer to speech
recognition for all languages in the world.


---

## Universal Phone Recognition with a Multilingual Allophone System

**Published Date:** 2020-02-26T21:28:57Z

**Link:** http://arxiv.org/pdf/2002.11800v1

**Abstract:**

  Multilingual models can improve language processing, particularly for low
resource situations, by sharing parameters across languages. Multilingual
acoustic models, however, generally ignore the difference between phonemes
(sounds that can support lexical contrasts in a particular language) and their
corresponding phones (the sounds that are actually spoken, which are language
independent). This can lead to performance degradation when combining a variety
of training languages, as identically annotated phonemes can actually
correspond to several different underlying phonetic realizations. In this work,
we propose a joint model of both language-independent phone and
language-dependent phoneme distributions. In multilingual ASR experiments over
11 languages, we find that this model improves testing performance by 2%
phoneme error rate absolute in low-resource conditions. Additionally, because
we are explicitly modeling language-independent phones, we can build a
(nearly-)universal phone recognizer that, when combined with the PHOIBLE large,
manually curated database of phone inventories, can be customized into 2,000
language dependent recognizers. Experiments on two low-resourced indigenous
languages, Inuktitut and Tusom, show that our recognizer achieves phone
accuracy improvements of more than 17%, moving a step closer to speech
recognition for all languages in the world.


---

## Compositional Languages Emerge in a Neural Iterated Learning Model

**Published Date:** 2020-02-04T15:19:09Z

**Link:** http://arxiv.org/pdf/2002.01365v2

**Abstract:**

  The principle of compositionality, which enables natural language to
represent complex concepts via a structured combination of simpler ones, allows
us to convey an open-ended set of messages using a limited vocabulary. If
compositionality is indeed a natural property of language, we may expect it to
appear in communication protocols that are created by neural agents in language
games. In this paper, we propose an effective neural iterated learning (NIL)
algorithm that, when applied to interacting neural agents, facilitates the
emergence of a more structured type of language. Indeed, these languages
provide learning speed advantages to neural agents during training, which can
be incrementally amplified via NIL. We provide a probabilistic model of NIL and
an explanation of why the advantage of compositional language exist. Our
experiments confirm our analysis, and also demonstrate that the emerged
languages largely improve the generalizing power of the neural agent
communication.


---

## Universal Phone Recognition with a Multilingual Allophone System

**first_author:** Xinjian Li et al.

**Published Date:** 2020-02-26T21:28:57Z

**Link:** http://arxiv.org/pdf/2002.11800v1

**Abstract:**

  Multilingual models can improve language processing, particularly for low
resource situations, by sharing parameters across languages. Multilingual
acoustic models, however, generally ignore the difference between phonemes
(sounds that can support lexical contrasts in a particular language) and their
corresponding phones (the sounds that are actually spoken, which are language
independent). This can lead to performance degradation when combining a variety
of training languages, as identically annotated phonemes can actually
correspond to several different underlying phonetic realizations. In this work,
we propose a joint model of both language-independent phone and
language-dependent phoneme distributions. In multilingual ASR experiments over
11 languages, we find that this model improves testing performance by 2%
phoneme error rate absolute in low-resource conditions. Additionally, because
we are explicitly modeling language-independent phones, we can build a
(nearly-)universal phone recognizer that, when combined with the PHOIBLE large,
manually curated database of phone inventories, can be customized into 2,000
language dependent recognizers. Experiments on two low-resourced indigenous
languages, Inuktitut and Tusom, show that our recognizer achieves phone
accuracy improvements of more than 17%, moving a step closer to speech
recognition for all languages in the world.


---

## Compositional Languages Emerge in a Neural Iterated Learning Model

**first_author:** Yi Ren et al.

**Published Date:** 2020-02-04T15:19:09Z

**Link:** http://arxiv.org/pdf/2002.01365v2

**Abstract:**

  The principle of compositionality, which enables natural language to
represent complex concepts via a structured combination of simpler ones, allows
us to convey an open-ended set of messages using a limited vocabulary. If
compositionality is indeed a natural property of language, we may expect it to
appear in communication protocols that are created by neural agents in language
games. In this paper, we propose an effective neural iterated learning (NIL)
algorithm that, when applied to interacting neural agents, facilitates the
emergence of a more structured type of language. Indeed, these languages
provide learning speed advantages to neural agents during training, which can
be incrementally amplified via NIL. We provide a probabilistic model of NIL and
an explanation of why the advantage of compositional language exist. Our
experiments confirm our analysis, and also demonstrate that the emerged
languages largely improve the generalizing power of the neural agent
communication.


---

