## A Precis of Language Models are not Models of Language

**Published Date:** 2022-05-16T12:50:58Z

**Link:** http://arxiv.org/pdf/2205.07634v1

**Abstract:**

  Natural Language Processing is one of the leading application areas in the
current resurgence of Artificial Intelligence, spearheaded by Artificial Neural
Networks. We show that despite their many successes at performing linguistic
tasks, Large Neural Language Models are ill-suited as comprehensive models of
natural language. The wider implication is that, in spite of the often
overbearing optimism about AI, modern neural models do not represent a
revolution in our understanding of cognition.


---

## Generalizing Multimodal Pre-training into Multilingual via Language
  Acquisition

**Published Date:** 2022-05-29T08:53:22Z

**Link:** http://arxiv.org/pdf/2206.11091v1

**Abstract:**

  English-based Vision-Language Pre-training (VLP) has achieved great success
in various downstream tasks. Some efforts have been taken to generalize this
success to non-English languages through Multilingual Vision-Language
Pre-training (M-VLP). However, due to the large number of languages, M-VLP
models often require huge computing resources and cannot be flexibly extended
to new languages. In this work, we propose a \textbf{M}ulti\textbf{L}ingual
\textbf{A}cquisition (MLA) framework that can easily generalize a monolingual
Vision-Language Pre-training model into multilingual. Specifically, we design a
lightweight language acquisition encoder based on state-of-the-art monolingual
VLP models. We further propose a two-stage training strategy to optimize the
language acquisition encoder, namely the Native Language Transfer stage and the
Language Exposure stage. With much less multilingual training data and
computing resources, our model achieves state-of-the-art performance on
multilingual image-text and video-text retrieval benchmarks.


---

## Overcoming Language Disparity in Online Content Classification with
  Multimodal Learning

**Published Date:** 2022-05-19T17:56:02Z

**Link:** http://arxiv.org/pdf/2205.09744v1

**Abstract:**

  Advances in Natural Language Processing (NLP) have revolutionized the way
researchers and practitioners address crucial societal problems. Large language
models are now the standard to develop state-of-the-art solutions for text
detection and classification tasks. However, the development of advanced
computational techniques and resources is disproportionately focused on the
English language, sidelining a majority of the languages spoken globally. While
existing research has developed better multilingual and monolingual language
models to bridge this language disparity between English and non-English
languages, we explore the promise of incorporating the information contained in
images via multimodal machine learning. Our comparative analyses on three
detection tasks focusing on crisis information, fake news, and emotion
recognition, as well as five high-resource non-English languages, demonstrate
that: (a) detection frameworks based on pre-trained large language models like
BERT and multilingual-BERT systematically perform better on the English
language compared against non-English languages, and (b) including images via
multimodal learning bridges this performance gap. We situate our findings with
respect to existing work on the pitfalls of large language models, and discuss
their theoretical and practical implications. Resources for this paper are
available at https://multimodality-language-disparity.github.io/.


---

## Phylogeny-Inspired Adaptation of Multilingual Models to New Languages

**Published Date:** 2022-05-19T15:49:19Z

**Link:** http://arxiv.org/pdf/2205.09634v2

**Abstract:**

  Large pretrained multilingual models, trained on dozens of languages, have
delivered promising results due to cross-lingual learning capabilities on
variety of language tasks. Further adapting these models to specific languages,
especially ones unseen during pre-training, is an important goal towards
expanding the coverage of language technologies. In this study, we show how we
can use language phylogenetic information to improve cross-lingual transfer
leveraging closely related languages in a structured, linguistically-informed
manner. We perform adapter-based training on languages from diverse language
families (Germanic, Uralic, Tupian, Uto-Aztecan) and evaluate on both syntactic
and semantic tasks, obtaining more than 20% relative performance improvements
over strong commonly used baselines, especially on languages unseen during
pre-training.


---

## TiBERT: Tibetan Pre-trained Language Model

**Published Date:** 2022-05-15T14:45:08Z

**Link:** http://arxiv.org/pdf/2205.07303v1

**Abstract:**

  The pre-trained language model is trained on large-scale unlabeled text and
can achieve state-of-the-art results in many different downstream tasks.
However, the current pre-trained language model is mainly concentrated in the
Chinese and English fields. For low resource language such as Tibetan, there is
lack of a monolingual pre-trained model. To promote the development of Tibetan
natural language processing tasks, this paper collects the large-scale training
data from Tibetan websites and constructs a vocabulary that can cover 99.95$\%$
of the words in the corpus by using Sentencepiece. Then, we train the Tibetan
monolingual pre-trained language model named TiBERT on the data and vocabulary.
Finally, we apply TiBERT to the downstream tasks of text classification and
question generation, and compare it with classic models and multilingual
pre-trained models, the experimental results show that TiBERT can achieve the
best performance. Our model is published in http://tibert.cmli-nlp.com/


---

## Few-shot Subgoal Planning with Language Models

**Published Date:** 2022-05-28T01:03:30Z

**Link:** http://arxiv.org/pdf/2205.14288v1

**Abstract:**

  Pre-trained large language models have shown successful progress in many
language understanding benchmarks. This work explores the capability of these
models to predict actionable plans in real-world environments. Given a text
instruction, we show that language priors encoded in pre-trained language
models allow us to infer fine-grained subgoal sequences. In contrast to recent
methods which make strong assumptions about subgoal supervision, our
experiments show that language models can infer detailed subgoal sequences from
few training sequences without any fine-tuning. We further propose a simple
strategy to re-rank language model predictions based on interaction and
feedback from the environment. Combined with pre-trained navigation and visual
reasoning components, our approach demonstrates competitive performance on
subgoal prediction and task completion in the ALFRED benchmark compared to
prior methods that assume more subgoal supervision.


---

## Structured, flexible, and robust: benchmarking and improving large
  language models towards more human-like behavior in out-of-distribution
  reasoning tasks

**Published Date:** 2022-05-11T18:14:33Z

**Link:** http://arxiv.org/pdf/2205.05718v1

**Abstract:**

  Human language offers a powerful window into our thoughts -- we tell stories,
give explanations, and express our beliefs and goals through words. Abundant
evidence also suggests that language plays a developmental role in structuring
our learning. Here, we ask: how much of human-like thinking can be captured by
learning statistical patterns in language alone? We first contribute a new
challenge benchmark for comparing humans and distributional large language
models (LLMs). Our benchmark contains two problem-solving domains (planning and
explanation generation) and is designed to require generalization to new,
out-of-distribution problems expressed in language. We find that humans are far
more robust than LLMs on this benchmark. Next, we propose a hybrid
Parse-and-Solve model, which augments distributional LLMs with a structured
symbolic reasoning module. We find that this model shows more robust adaptation
to out-of-distribution planning problems, demonstrating the promise of hybrid
AI models for more human-like reasoning.


---

## A Precis of Language Models are not Models of Language

**Published Date:** 2022-05-16T12:50:58Z

**Link:** http://arxiv.org/pdf/2205.07634v1

**Abstract:**

  Natural Language Processing is one of the leading application areas in the
current resurgence of Artificial Intelligence, spearheaded by Artificial Neural
Networks. We show that despite their many successes at performing linguistic
tasks, Large Neural Language Models are ill-suited as comprehensive models of
natural language. The wider implication is that, in spite of the often
overbearing optimism about AI, modern neural models do not represent a
revolution in our understanding of cognition.


---

## Few-shot Subgoal Planning with Language Models

**Published Date:** 2022-05-28T01:03:30Z

**Link:** http://arxiv.org/pdf/2205.14288v1

**Abstract:**

  Pre-trained large language models have shown successful progress in many
language understanding benchmarks. This work explores the capability of these
models to predict actionable plans in real-world environments. Given a text
instruction, we show that language priors encoded in pre-trained language
models allow us to infer fine-grained subgoal sequences. In contrast to recent
methods which make strong assumptions about subgoal supervision, our
experiments show that language models can infer detailed subgoal sequences from
few training sequences without any fine-tuning. We further propose a simple
strategy to re-rank language model predictions based on interaction and
feedback from the environment. Combined with pre-trained navigation and visual
reasoning components, our approach demonstrates competitive performance on
subgoal prediction and task completion in the ALFRED benchmark compared to
prior methods that assume more subgoal supervision.


---

## Visually-Augmented Language Modeling

**Published Date:** 2022-05-20T13:41:12Z

**Link:** http://arxiv.org/pdf/2205.10178v2

**Abstract:**

  Human language is grounded on multimodal knowledge including visual knowledge
like colors, sizes, and shapes. However, current large-scale pre-trained
language models rely on text-only self-supervised training with massive text
data, which precludes them from utilizing relevant visual information when
necessary. To address this, we propose a novel pre-training framework, named
VaLM, to Visually-augment text tokens with retrieved relevant images for
Language Modeling. Specifically, VaLM builds on a novel latent text-image
alignment method via an image retrieval module to fetch corresponding images
given a textual context. With the visually-augmented context, VaLM uses a
visual knowledge fusion layer to enable multimodal grounded language modeling
by attending to both text context and visual knowledge in images. We evaluate
VaLM on various visual knowledge-intensive commonsense reasoning tasks, which
require visual information to excel. The experimental results illustrate that
VaLM outperforms all strong language-only and vision-language baselines with
substantial gains in reasoning object commonsense including color, size, and
shape. Our code is available at https://github.com/Victorwz/VaLM.


---

## Adaptive Activation Network For Low Resource Multilingual Speech
  Recognition

**Published Date:** 2022-05-28T04:02:59Z

**Link:** http://arxiv.org/pdf/2205.14326v1

**Abstract:**

  Low resource automatic speech recognition (ASR) is a useful but thorny task,
since deep learning ASR models usually need huge amounts of training data. The
existing models mostly established a bottleneck (BN) layer by pre-training on a
large source language, and transferring to the low resource target language. In
this work, we introduced an adaptive activation network to the upper layers of
ASR model, and applied different activation functions to different languages.
We also proposed two approaches to train the model: (1) cross-lingual learning,
replacing the activation function from source language to target language, (2)
multilingual learning, jointly training the Connectionist Temporal
Classification (CTC) loss of each language and the relevance of different
languages. Our experiments on IARPA Babel datasets demonstrated that our
approaches outperform the from-scratch training and traditional bottleneck
feature based methods. In addition, combining the cross-lingual learning and
multilingual learning together could further improve the performance of
multilingual speech recognition.


---

## A Precis of Language Models are not Models of Language

**first_author:** Csaba Veres et al.

**Published Date:** 2022-05-16T12:50:58Z

**Link:** http://arxiv.org/pdf/2205.07634v1

**Abstract:**

  Natural Language Processing is one of the leading application areas in the
current resurgence of Artificial Intelligence, spearheaded by Artificial Neural
Networks. We show that despite their many successes at performing linguistic
tasks, Large Neural Language Models are ill-suited as comprehensive models of
natural language. The wider implication is that, in spite of the often
overbearing optimism about AI, modern neural models do not represent a
revolution in our understanding of cognition.


---

## Few-shot Subgoal Planning with Language Models

**first_author:** Lajanugen Logeswaran et al.

**Published Date:** 2022-05-28T01:03:30Z

**Link:** http://arxiv.org/pdf/2205.14288v1

**Abstract:**

  Pre-trained large language models have shown successful progress in many
language understanding benchmarks. This work explores the capability of these
models to predict actionable plans in real-world environments. Given a text
instruction, we show that language priors encoded in pre-trained language
models allow us to infer fine-grained subgoal sequences. In contrast to recent
methods which make strong assumptions about subgoal supervision, our
experiments show that language models can infer detailed subgoal sequences from
few training sequences without any fine-tuning. We further propose a simple
strategy to re-rank language model predictions based on interaction and
feedback from the environment. Combined with pre-trained navigation and visual
reasoning components, our approach demonstrates competitive performance on
subgoal prediction and task completion in the ALFRED benchmark compared to
prior methods that assume more subgoal supervision.


---

## Visually-Augmented Language Modeling

**first_author:** Weizhi Wang et al.

**Published Date:** 2022-05-20T13:41:12Z

**Link:** http://arxiv.org/pdf/2205.10178v2

**Abstract:**

  Human language is grounded on multimodal knowledge including visual knowledge
like colors, sizes, and shapes. However, current large-scale pre-trained
language models rely on text-only self-supervised training with massive text
data, which precludes them from utilizing relevant visual information when
necessary. To address this, we propose a novel pre-training framework, named
VaLM, to Visually-augment text tokens with retrieved relevant images for
Language Modeling. Specifically, VaLM builds on a novel latent text-image
alignment method via an image retrieval module to fetch corresponding images
given a textual context. With the visually-augmented context, VaLM uses a
visual knowledge fusion layer to enable multimodal grounded language modeling
by attending to both text context and visual knowledge in images. We evaluate
VaLM on various visual knowledge-intensive commonsense reasoning tasks, which
require visual information to excel. The experimental results illustrate that
VaLM outperforms all strong language-only and vision-language baselines with
substantial gains in reasoning object commonsense including color, size, and
shape. Our code is available at https://github.com/Victorwz/VaLM.


---

## Adaptive Activation Network For Low Resource Multilingual Speech
  Recognition

**first_author:** Jian Luo et al.

**Published Date:** 2022-05-28T04:02:59Z

**Link:** http://arxiv.org/pdf/2205.14326v1

**Abstract:**

  Low resource automatic speech recognition (ASR) is a useful but thorny task,
since deep learning ASR models usually need huge amounts of training data. The
existing models mostly established a bottleneck (BN) layer by pre-training on a
large source language, and transferring to the low resource target language. In
this work, we introduced an adaptive activation network to the upper layers of
ASR model, and applied different activation functions to different languages.
We also proposed two approaches to train the model: (1) cross-lingual learning,
replacing the activation function from source language to target language, (2)
multilingual learning, jointly training the Connectionist Temporal
Classification (CTC) loss of each language and the relevance of different
languages. Our experiments on IARPA Babel datasets demonstrated that our
approaches outperform the from-scratch training and traditional bottleneck
feature based methods. In addition, combining the cross-lingual learning and
multilingual learning together could further improve the performance of
multilingual speech recognition.


---

## Same Neurons, Different Languages: Probing Morphosyntax in Multilingual
  Pre-trained Models

