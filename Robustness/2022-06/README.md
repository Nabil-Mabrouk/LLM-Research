## Is the Language Familiarity Effect gradual? A computational modelling
  approach

**Published Date:** 2022-06-27T16:08:42Z

**Link:** http://arxiv.org/pdf/2206.13415v1

**Abstract:**

  According to the Language Familiarity Effect (LFE), people are better at
discriminating between speakers of their native language. Although this
cognitive effect was largely studied in the literature, experiments have only
been conducted on a limited number of language pairs and their results only
show the presence of the effect without yielding a gradual measure that may
vary across language pairs. In this work, we show that the computational model
of LFE introduced by Thorburn, Feldmand and Schatz (2019) can address these two
limitations. In a first experiment, we attest to this model's capacity to
obtain a gradual measure of the LFE by replicating behavioural findings on
native and accented speech. In a second experiment, we evaluate LFE on a large
number of language pairs, including many which have never been tested on
humans. We show that the effect is replicated across a wide array of languages,
providing further evidence of its universality. Building on the gradual measure
of LFE, we also show that languages belonging to the same family yield smaller
scores, supporting the idea of an effect of language distance on LFE.


---

## cViL: Cross-Lingual Training of Vision-Language Models using Knowledge
  Distillation

**Published Date:** 2022-06-07T14:46:30Z

**Link:** http://arxiv.org/pdf/2206.03354v2

**Abstract:**

  Vision-and-language tasks are gaining popularity in the research community,
but the focus is still mainly on English. We propose a pipeline that utilizes
English-only vision-language models to train a monolingual model for a target
language. We propose to extend OSCAR+, a model which leverages object tags as
anchor points for learning image-text alignments, to train on visual question
answering datasets in different languages. We propose a novel approach to
knowledge distillation to train the model in other languages using parallel
sentences. Compared to other models that use the target language in the
pretraining corpora, we can leverage an existing English model to transfer the
knowledge to the target language using significantly lesser resources. We also
release a large-scale visual question answering dataset in Japanese and Hindi
language. Though we restrict our work to visual question answering, our model
can be extended to any sequence-level classification task, and it can be
extended to other languages as well. This paper focuses on two languages for
the visual question answering task - Japanese and Hindi. Our pipeline
outperforms the current state-of-the-art models by a relative increase of 4.4%
and 13.4% respectively in accuracy.


---

## Norm Participation Grounds Language

**Published Date:** 2022-06-06T20:21:59Z

**Link:** http://arxiv.org/pdf/2206.02885v2

**Abstract:**

  The striking recent advances in eliciting seemingly meaningful language
behaviour from language-only machine learning models have only made more
apparent, through the surfacing of clear limitations, the need to go beyond the
language-only mode and to ground these models "in the world". Proposals for
doing so vary in the details, but what unites them is that the solution is
sought in the addition of non-linguistic data types such as images or video
streams, while largely keeping the mode of learning constant. I propose a
different, and more wide-ranging conception of how grounding should be
understood: What grounds language is its normative nature. There are standards
for doing things right, these standards are public and authoritative, while at
the same time acceptance of authority can and must be disputed and negotiated,
in interactions in which only bearers of normative status can rightfully
participate. What grounds language, then, is the determined use that language
users make of it, and what it is grounded in is the community of language
users. I sketch this idea, and draw some conclusions for work on computational
modelling of meaningful language use.


---

## Why is constrained neural language generation particularly challenging?

**Published Date:** 2022-06-11T02:07:33Z

**Link:** http://arxiv.org/pdf/2206.05395v1

**Abstract:**

  Recent advances in deep neural language models combined with the capacity of
large scale datasets have accelerated the development of natural language
generation systems that produce fluent and coherent texts (to various degrees
of success) in a multitude of tasks and application contexts. However,
controlling the output of these models for desired user and task needs is still
an open challenge. This is crucial not only to customizing the content and
style of the generated language, but also to their safe and reliable deployment
in the real world. We present an extensive survey on the emerging topic of
constrained neural language generation in which we formally define and
categorize the problems of natural language generation by distinguishing
between conditions and constraints (the latter being testable conditions on the
output text instead of the input), present constrained text generation tasks,
and review existing methods and evaluation metrics for constrained text
generation. Our aim is to highlight recent progress and trends in this emerging
field, informing on the most promising directions and limitations towards
advancing the state-of-the-art of constrained neural language generation
research.


---

## Is the Language Familiarity Effect gradual? A computational modelling
  approach

**Published Date:** 2022-06-27T16:08:42Z

**Link:** http://arxiv.org/pdf/2206.13415v1

**Abstract:**

  According to the Language Familiarity Effect (LFE), people are better at
discriminating between speakers of their native language. Although this
cognitive effect was largely studied in the literature, experiments have only
been conducted on a limited number of language pairs and their results only
show the presence of the effect without yielding a gradual measure that may
vary across language pairs. In this work, we show that the computational model
of LFE introduced by Thorburn, Feldmand and Schatz (2019) can address these two
limitations. In a first experiment, we attest to this model's capacity to
obtain a gradual measure of the LFE by replicating behavioural findings on
native and accented speech. In a second experiment, we evaluate LFE on a large
number of language pairs, including many which have never been tested on
humans. We show that the effect is replicated across a wide array of languages,
providing further evidence of its universality. Building on the gradual measure
of LFE, we also show that languages belonging to the same family yield smaller
scores, supporting the idea of an effect of language distance on LFE.


---

## Bottleneck Low-rank Transformers for Low-resource Spoken Language
  Understanding

**Published Date:** 2022-06-28T23:08:32Z

**Link:** http://arxiv.org/pdf/2206.14318v1

**Abstract:**

  End-to-end spoken language understanding (SLU) systems benefit from
pretraining on large corpora, followed by fine-tuning on application-specific
data. The resulting models are too large for on-edge applications. For
instance, BERT-based systems contain over 110M parameters. Observing the model
is overparameterized, we propose lean transformer structure where the dimension
of the attention mechanism is automatically reduced using group sparsity. We
propose a variant where the learned attention subspace is transferred to an
attention bottleneck layer. In a low-resource setting and without pre-training,
the resulting compact SLU model achieves accuracies competitive with
pre-trained large models.


---

## Revisiting the "Video" in Video-Language Understanding

**Published Date:** 2022-06-03T17:57:33Z

**Link:** http://arxiv.org/pdf/2206.01720v1

**Abstract:**

  What makes a video task uniquely suited for videos, beyond what can be
understood from a single image? Building on recent progress in self-supervised
image-language models, we revisit this question in the context of video and
language tasks. We propose the atemporal probe (ATP), a new model for
video-language analysis which provides a stronger bound on the baseline
accuracy of multimodal models constrained by image-level understanding. By
applying this model to standard discriminative video and language tasks, such
as video question answering and text-to-video retrieval, we characterize the
limitations and potential of current video-language benchmarks. We find that
understanding of event temporality is often not necessary to achieve strong or
state-of-the-art performance, even compared with recent large-scale
video-language models and in contexts intended to benchmark deeper video-level
understanding. We also demonstrate how ATP can improve both video-language
dataset and model design. We describe a technique for leveraging ATP to better
disentangle dataset subsets with a higher concentration of temporally
challenging data, improving benchmarking efficacy for causal and temporal
understanding. Further, we show that effectively integrating ATP into full
video-level temporal models can improve efficiency and state-of-the-art
accuracy.


---

## Solving Quantitative Reasoning Problems with Language Models

**Published Date:** 2022-06-29T18:54:49Z

**Link:** http://arxiv.org/pdf/2206.14858v2

**Abstract:**

  Language models have achieved remarkable performance on a wide range of tasks
that require natural language understanding. Nevertheless, state-of-the-art
models have generally struggled with tasks that require quantitative reasoning,
such as solving mathematics, science, and engineering problems at the college
level. To help close this gap, we introduce Minerva, a large language model
pretrained on general natural language data and further trained on technical
content. The model achieves state-of-the-art performance on technical
benchmarks without the use of external tools. We also evaluate our model on
over two hundred undergraduate-level problems in physics, biology, chemistry,
economics, and other sciences that require quantitative reasoning, and find
that the model can correctly answer nearly a third of them.


---

## Is the Language Familiarity Effect gradual? A computational modelling
  approach

**first_author:** Maureen de Seyssel et al.

**Published Date:** 2022-06-27T16:08:42Z

**Link:** http://arxiv.org/pdf/2206.13415v1

**Abstract:**

  According to the Language Familiarity Effect (LFE), people are better at
discriminating between speakers of their native language. Although this
cognitive effect was largely studied in the literature, experiments have only
been conducted on a limited number of language pairs and their results only
show the presence of the effect without yielding a gradual measure that may
vary across language pairs. In this work, we show that the computational model
of LFE introduced by Thorburn, Feldmand and Schatz (2019) can address these two
limitations. In a first experiment, we attest to this model's capacity to
obtain a gradual measure of the LFE by replicating behavioural findings on
native and accented speech. In a second experiment, we evaluate LFE on a large
number of language pairs, including many which have never been tested on
humans. We show that the effect is replicated across a wide array of languages,
providing further evidence of its universality. Building on the gradual measure
of LFE, we also show that languages belonging to the same family yield smaller
scores, supporting the idea of an effect of language distance on LFE.


---

## Norm Participation Grounds Language

**first_author:** David Schlangen et al.

**Published Date:** 2022-06-06T20:21:59Z

**Link:** http://arxiv.org/pdf/2206.02885v2

**Abstract:**

  The striking recent advances in eliciting seemingly meaningful language
behaviour from language-only machine learning models have only made more
apparent, through the surfacing of clear limitations, the need to go beyond the
language-only mode and to ground these models "in the world". Proposals for
doing so vary in the details, but what unites them is that the solution is
sought in the addition of non-linguistic data types such as images or video
streams, while largely keeping the mode of learning constant. I propose a
different, and more wide-ranging conception of how grounding should be
understood: What grounds language is its normative nature. There are standards
for doing things right, these standards are public and authoritative, while at
the same time acceptance of authority can and must be disputed and negotiated,
in interactions in which only bearers of normative status can rightfully
participate. What grounds language, then, is the determined use that language
users make of it, and what it is grounded in is the community of language
users. I sketch this idea, and draw some conclusions for work on computational
modelling of meaningful language use.


---

