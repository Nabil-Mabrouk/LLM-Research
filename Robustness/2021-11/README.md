## Utilizing Resource-Rich Language Datasets for End-to-End Scene Text
  Recognition in Resource-Poor Languages

**Published Date:** 2021-11-24T05:28:46Z

**Link:** http://arxiv.org/pdf/2111.12276v1

**Abstract:**

  This paper presents a novel training method for end-to-end scene text
recognition. End-to-end scene text recognition offers high recognition
accuracy, especially when using the encoder-decoder model based on Transformer.
To train a highly accurate end-to-end model, we need to prepare a large
image-to-text paired dataset for the target language. However, it is difficult
to collect this data, especially for resource-poor languages. To overcome this
difficulty, our proposed method utilizes well-prepared large datasets in
resource-rich languages such as English, to train the resource-poor
encoder-decoder model. Our key idea is to build a model in which the encoder
reflects knowledge of multiple languages while the decoder specializes in
knowledge of just the resource-poor language. To this end, the proposed method
pre-trains the encoder by using a multilingual dataset that combines the
resource-poor language's dataset and the resource-rich language's dataset to
learn language-invariant knowledge for scene text recognition. The proposed
method also pre-trains the decoder by using the resource-poor language's
dataset to make the decoder better suited to the resource-poor language.
Experiments on Japanese scene text recognition using a small, publicly
available dataset demonstrate the effectiveness of the proposed method.


---

## Recent Advances in Natural Language Processing via Large Pre-Trained
  Language Models: A Survey

**Published Date:** 2021-11-01T20:08:05Z

**Link:** http://arxiv.org/pdf/2111.01243v1

**Abstract:**

  Large, pre-trained transformer-based language models such as BERT have
drastically changed the Natural Language Processing (NLP) field. We present a
survey of recent work that uses these large language models to solve NLP tasks
via pre-training then fine-tuning, prompting, or text generation approaches. We
also present approaches that use pre-trained language models to generate data
for training augmentation or other purposes. We conclude with discussions on
limitations and suggested directions for future research.


---

## Exploring Language Patterns in a Medical Licensure Exam Item Bank

**Published Date:** 2021-11-20T02:45:35Z

**Link:** http://arxiv.org/pdf/2111.10501v1

**Abstract:**

  This study examines the use of natural language processing (NLP) models to
evaluate whether language patterns used by item writers in a medical licensure
exam might contain evidence of biased or stereotypical language. This type of
bias in item language choices can be particularly impactful for items in a
medical licensure assessment, as it could pose a threat to content validity and
defensibility of test score validity evidence. To the best of our knowledge,
this is the first attempt using machine learning (ML) and NLP to explore
language bias on a large item bank. Using a prediction algorithm trained on
clusters of similar item stems, we demonstrate that our approach can be used to
review large item banks for potential biased language or stereotypical patient
characteristics in clinical science vignettes. The findings may guide the
development of methods to address stereotypical language patterns found in test
items and enable an efficient updating of those items, if needed, to reflect
contemporary norms, thereby improving the evidence to support the validity of
the test scores.


---

## Interpreting Language Models Through Knowledge Graph Extraction

**Published Date:** 2021-11-16T15:18:01Z

**Link:** http://arxiv.org/pdf/2111.08546v1

**Abstract:**

  Transformer-based language models trained on large text corpora have enjoyed
immense popularity in the natural language processing community and are
commonly used as a starting point for downstream tasks. While these models are
undeniably useful, it is a challenge to quantify their performance beyond
traditional accuracy metrics. In this paper, we compare BERT-based language
models through snapshots of acquired knowledge at sequential stages of the
training process. Structured relationships from training corpora may be
uncovered through querying a masked language model with probing tasks. We
present a methodology to unveil a knowledge acquisition timeline by generating
knowledge graph extracts from cloze "fill-in-the-blank" statements at various
stages of RoBERTa's early training. We extend this analysis to a comparison of
pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This
work proposes a quantitative framework to compare language models through
knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech
analysis (POSOR) to identify the linguistic strengths of each model variant.
Using these metrics, machine learning practitioners can compare models,
diagnose their models' behavioral strengths and weaknesses, and identify new
targeted datasets to improve model performance.


---

## Recent Advances in Natural Language Processing via Large Pre-Trained
  Language Models: A Survey

**Published Date:** 2021-11-01T20:08:05Z

**Link:** http://arxiv.org/pdf/2111.01243v1

**Abstract:**

  Large, pre-trained transformer-based language models such as BERT have
drastically changed the Natural Language Processing (NLP) field. We present a
survey of recent work that uses these large language models to solve NLP tasks
via pre-training then fine-tuning, prompting, or text generation approaches. We
also present approaches that use pre-trained language models to generate data
for training augmentation or other purposes. We conclude with discussions on
limitations and suggested directions for future research.


---

## Recent Advances in Natural Language Processing via Large Pre-Trained
  Language Models: A Survey

**first_author:** Bonan Min et al.

**Published Date:** 2021-11-01T20:08:05Z

**Link:** http://arxiv.org/pdf/2111.01243v1

**Abstract:**

  Large, pre-trained transformer-based language models such as BERT have
drastically changed the Natural Language Processing (NLP) field. We present a
survey of recent work that uses these large language models to solve NLP tasks
via pre-training then fine-tuning, prompting, or text generation approaches. We
also present approaches that use pre-trained language models to generate data
for training augmentation or other purposes. We conclude with discussions on
limitations and suggested directions for future research.


---

