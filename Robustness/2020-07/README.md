## A Study on Effects of Implicit and Explicit Language Model Information
  for DBLSTM-CTC Based Handwriting Recognition

**Published Date:** 2020-07-31T08:23:37Z

**Link:** http://arxiv.org/pdf/2008.01532v1

**Abstract:**

  Deep Bidirectional Long Short-Term Memory (D-BLSTM) with a Connectionist
Temporal Classification (CTC) output layer has been established as one of the
state-of-the-art solutions for handwriting recognition. It is well known that
the DBLSTM trained by using a CTC objective function will learn both local
character image dependency for character modeling and long-range contextual
dependency for implicit language modeling. In this paper, we study the effects
of implicit and explicit language model information for DBLSTM-CTC based
handwriting recognition by comparing the performance of using or without using
an explicit language model in decoding. It is observed that even using one
million lines of training sentences to train the DBLSTM, using an explicit
language model is still helpful. To deal with such a large-scale training
problem, a GPU-based training tool has been developed for CTC training of
DBLSTM by using a mini-batch based epochwise Back Propagation Through Time
(BPTT) algorithm.


---

## Towards Multi-Language Recipe Personalisation and Recommendation

**Published Date:** 2020-07-27T11:26:49Z

**Link:** http://arxiv.org/pdf/2007.13440v2

**Abstract:**

  Multi-language recipe personalisation and recommendation is an under-explored
field of information retrieval in academic and production systems. The existing
gaps in our current understanding are numerous, even on fundamental questions
such as whether consistent and high-quality recipe recommendation can be
delivered across languages. In this paper, we introduce the multi-language
recipe recommendation setting and present grounding results that will help to
establish the potential and absolute value of future work in this area. Our
work draws on several billion events from millions of recipes and users from
Arabic, English, Indonesian, Russian, and Spanish. We represent recipes using a
combination of normalised ingredients, standardised skills and image embeddings
obtained without human intervention. In modelling, we take a classical approach
based on optimising an embedded bi-linear user-item metric space towards the
interactions that most strongly elicit cooking intent. For users without
interaction histories, a bespoke content-based cold-start model that predicts
context and recipe affinity is introduced. We show that our approach to
personalisation is stable and easily scales to new languages. A robust
cross-validation campaign is employed and consistently rejects baseline models
and representations, strongly favouring those we propose. Our results are
presented in a language-oriented (as opposed to model-oriented) fashion to
emphasise the language-based goals of this work. We believe that this is the
first large-scale work that comprehensively considers the value and potential
of multi-language recipe recommendation and personalisation as well as
delivering scalable and reliable models.


---

