## Learning to Scale Multilingual Representations for Vision-Language Tasks

**Published Date:** 2020-04-09T01:03:44Z

**Link:** http://arxiv.org/pdf/2004.04312v2

**Abstract:**

  Current multilingual vision-language models either require a large number of
additional parameters for each supported language, or suffer performance
degradation as languages are added. In this paper, we propose a Scalable
Multilingual Aligned Language Representation (SMALR) that supports many
languages with few model parameters without sacrificing downstream task
performance. SMALR learns a fixed size language-agnostic representation for
most words in a multilingual vocabulary, keeping language-specific features for
just a few. We use a masked cross-language modeling loss to align features with
context from other languages. Additionally, we propose a cross-lingual
consistency module that ensures predictions made for a query and its machine
translation are comparable. The effectiveness of SMALR is demonstrated with ten
diverse languages, over twice the number supported in vision-language tasks to
date. We evaluate on multilingual image-sentence retrieval and outperform prior
work by 3-4% with less than 1/5th the training parameters compared to other
word embedding methods.


---

## Data Augmentation for Spoken Language Understanding via Pretrained
  Language Models

**Published Date:** 2020-04-29T04:07:12Z

**Link:** http://arxiv.org/pdf/2004.13952v2

**Abstract:**

  The training of spoken language understanding (SLU) models often faces the
problem of data scarcity. In this paper, we put forward a data augmentation
method using pretrained language models to boost the variability and accuracy
of generated utterances. Furthermore, we investigate and propose solutions to
two previously overlooked semi-supervised learning scenarios of data scarcity
in SLU: i) Rich-in-Ontology: ontology information with numerous valid dialogue
acts is given; ii) Rich-in-Utterance: a large number of unlabelled utterances
are available. Empirical results show that our method can produce synthetic
training data that boosts the performance of language understanding models in
various scenarios.


---

## lamBERT: Language and Action Learning Using Multimodal BERT

**Published Date:** 2020-04-15T13:54:55Z

**Link:** http://arxiv.org/pdf/2004.07093v1

**Abstract:**

  Recently, the bidirectional encoder representations from transformers (BERT)
model has attracted much attention in the field of natural language processing,
owing to its high performance in language understanding-related tasks. The BERT
model learns language representation that can be adapted to various tasks via
pre-training using a large corpus in an unsupervised manner. This study
proposes the language and action learning using multimodal BERT (lamBERT) model
that enables the learning of language and actions by 1) extending the BERT
model to multimodal representation and 2) integrating it with reinforcement
learning. To verify the proposed model, an experiment is conducted in a grid
environment that requires language understanding for the agent to act properly.
As a result, the lamBERT model obtained higher rewards in multitask settings
and transfer settings when compared to other models, such as the convolutional
neural network-based model and the lamBERT model without pre-training.


---

## The Cost of Training NLP Models: A Concise Overview

**Published Date:** 2020-04-19T16:28:35Z

**Link:** http://arxiv.org/pdf/2004.08900v1

**Abstract:**

  We review the cost of training large-scale language models, and the drivers
of these costs. The intended audience includes engineers and scientists
budgeting their model-training experiments, as well as non-practitioners trying
to make sense of the economics of modern-day Natural Language Processing (NLP).


---

## Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space

**Published Date:** 2020-04-05T06:20:18Z

**Link:** http://arxiv.org/pdf/2004.04092v4

**Abstract:**

  When trained effectively, the Variational Autoencoder (VAE) can be both a
powerful generative model and an effective representation learning framework
for natural language. In this paper, we propose the first large-scale language
VAE model, Optimus. A universal latent embedding space for sentences is first
pre-trained on large text corpus, and then fine-tuned for various language
generation and understanding tasks. Compared with GPT-2, Optimus enables guided
language generation from an abstract level using the latent vectors. Compared
with BERT, Optimus can generalize better on low-resource language understanding
tasks due to the smooth latent space structure. Extensive experimental results
on a wide range of language tasks demonstrate the effectiveness of Optimus. It
achieves new state-of-the-art on VAE language modeling benchmarks. We hope that
our first pre-trained big VAE language model itself and results can help the
NLP community renew the interests of deep generative models in the era of
large-scale pre-training, and make these principled methods more practical.


---

## Unnatural Language Processing: Bridging the Gap Between Synthetic and
  Natural Language Data

**Published Date:** 2020-04-28T16:41:00Z

**Link:** http://arxiv.org/pdf/2004.13645v1

**Abstract:**

  Large, human-annotated datasets are central to the development of natural
language processing models. Collecting these datasets can be the most
challenging part of the development process. We address this problem by
introducing a general purpose technique for ``simulation-to-real'' transfer in
language understanding problems with a delimited set of target behaviors,
making it possible to develop models that can interpret natural utterances
without natural training data. We begin with a synthetic data generation
procedure, and train a model that can accurately interpret utterances produced
by the data generator. To generalize to natural utterances, we automatically
find projections of natural language utterances onto the support of the
synthetic language, using learned sentence embeddings to define a distance
metric. With only synthetic training data, our approach matches or outperforms
state-of-the-art models trained on natural language data in several domains.
These results suggest that simulation-to-real transfer is a practical framework
for developing NLP applications, and that improved models for transfer might
provide wide-ranging improvements in downstream tasks.


---

## Learning to Scale Multilingual Representations for Vision-Language Tasks

**Published Date:** 2020-04-09T01:03:44Z

**Link:** http://arxiv.org/pdf/2004.04312v2

**Abstract:**

  Current multilingual vision-language models either require a large number of
additional parameters for each supported language, or suffer performance
degradation as languages are added. In this paper, we propose a Scalable
Multilingual Aligned Language Representation (SMALR) that supports many
languages with few model parameters without sacrificing downstream task
performance. SMALR learns a fixed size language-agnostic representation for
most words in a multilingual vocabulary, keeping language-specific features for
just a few. We use a masked cross-language modeling loss to align features with
context from other languages. Additionally, we propose a cross-lingual
consistency module that ensures predictions made for a query and its machine
translation are comparable. The effectiveness of SMALR is demonstrated with ten
diverse languages, over twice the number supported in vision-language tasks to
date. We evaluate on multilingual image-sentence retrieval and outperform prior
work by 3-4% with less than 1/5th the training parameters compared to other
word embedding methods.


---

## Data Augmentation for Spoken Language Understanding via Pretrained
  Language Models

**Published Date:** 2020-04-29T04:07:12Z

**Link:** http://arxiv.org/pdf/2004.13952v2

**Abstract:**

  The training of spoken language understanding (SLU) models often faces the
problem of data scarcity. In this paper, we put forward a data augmentation
method using pretrained language models to boost the variability and accuracy
of generated utterances. Furthermore, we investigate and propose solutions to
two previously overlooked semi-supervised learning scenarios of data scarcity
in SLU: i) Rich-in-Ontology: ontology information with numerous valid dialogue
acts is given; ii) Rich-in-Utterance: a large number of unlabelled utterances
are available. Empirical results show that our method can produce synthetic
training data that boosts the performance of language understanding models in
various scenarios.


---

## DIET: Lightweight Language Understanding for Dialogue Systems

**Published Date:** 2020-04-21T12:10:48Z

**Link:** http://arxiv.org/pdf/2004.09936v3

**Abstract:**

  Large-scale pre-trained language models have shown impressive results on
language understanding benchmarks like GLUE and SuperGLUE, improving
considerably over other pre-training methods like distributed representations
(GloVe) and purely supervised approaches. We introduce the Dual Intent and
Entity Transformer (DIET) architecture, and study the effectiveness of
different pre-trained representations on intent and entity prediction, two
common dialogue language understanding tasks. DIET advances the state of the
art on a complex multi-domain NLU dataset and achieves similarly high
performance on other simpler datasets. Surprisingly, we show that there is no
clear benefit to using large pre-trained models for this task, and in fact DIET
improves upon the current state of the art even in a purely supervised setup
without any pre-trained embeddings. Our best performing model outperforms
fine-tuning BERT and is about six times faster to train.


---

## Learning to Scale Multilingual Representations for Vision-Language Tasks

**first_author:** Andrea Burns et al.

**Published Date:** 2020-04-09T01:03:44Z

**Link:** http://arxiv.org/pdf/2004.04312v2

**Abstract:**

  Current multilingual vision-language models either require a large number of
additional parameters for each supported language, or suffer performance
degradation as languages are added. In this paper, we propose a Scalable
Multilingual Aligned Language Representation (SMALR) that supports many
languages with few model parameters without sacrificing downstream task
performance. SMALR learns a fixed size language-agnostic representation for
most words in a multilingual vocabulary, keeping language-specific features for
just a few. We use a masked cross-language modeling loss to align features with
context from other languages. Additionally, we propose a cross-lingual
consistency module that ensures predictions made for a query and its machine
translation are comparable. The effectiveness of SMALR is demonstrated with ten
diverse languages, over twice the number supported in vision-language tasks to
date. We evaluate on multilingual image-sentence retrieval and outperform prior
work by 3-4% with less than 1/5th the training parameters compared to other
word embedding methods.


---

## Data Augmentation for Spoken Language Understanding via Pretrained
  Language Models

**first_author:** Baolin Peng et al.

**Published Date:** 2020-04-29T04:07:12Z

**Link:** http://arxiv.org/pdf/2004.13952v2

**Abstract:**

  The training of spoken language understanding (SLU) models often faces the
problem of data scarcity. In this paper, we put forward a data augmentation
method using pretrained language models to boost the variability and accuracy
of generated utterances. Furthermore, we investigate and propose solutions to
two previously overlooked semi-supervised learning scenarios of data scarcity
in SLU: i) Rich-in-Ontology: ontology information with numerous valid dialogue
acts is given; ii) Rich-in-Utterance: a large number of unlabelled utterances
are available. Empirical results show that our method can produce synthetic
training data that boosts the performance of language understanding models in
various scenarios.


---

