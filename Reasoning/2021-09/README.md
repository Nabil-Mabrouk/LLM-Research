## Evaluating Transferability of BERT Models on Uralic Languages

**Published Date:** 2021-09-13T21:10:29Z

**Link:** http://arxiv.org/pdf/2109.06327v2

**Abstract:**

  Transformer-based language models such as BERT have outperformed previous
models on a large number of English benchmarks, but their evaluation is often
limited to English or a small number of well-resourced languages. In this work,
we evaluate monolingual, multilingual, and randomly initialized language models
from the BERT family on a variety of Uralic languages including Estonian,
Finnish, Hungarian, Erzya, Moksha, Karelian, Livvi, Komi Permyak, Komi Zyrian,
Northern S\'ami, and Skolt S\'ami. When monolingual models are available
(currently only et, fi, hu), these perform better on their native language, but
in general they transfer worse than multilingual models or models of
genetically unrelated languages that share the same character set. Remarkably,
straightforward transfer of high-resource models, even without special efforts
toward hyperparameter optimization, yields what appear to be state of the art
POS and NER tools for the minority Uralic languages where there is sufficient
data for finetuning.


---

## On Language Models for Creoles

**Published Date:** 2021-09-13T15:51:15Z

**Link:** http://arxiv.org/pdf/2109.06074v1

**Abstract:**

  Creole languages such as Nigerian Pidgin English and Haitian Creole are
under-resourced and largely ignored in the NLP literature. Creoles typically
result from the fusion of a foreign language with multiple local languages, and
what grammatical and lexical features are transferred to the creole is a
complex process. While creoles are generally stable, the prominence of some
features may be much stronger with certain demographics or in some linguistic
situations. This paper makes several contributions: We collect existing corpora
and release models for Haitian Creole, Nigerian Pidgin English, and Singaporean
Colloquial English. We evaluate these models on intrinsic and extrinsic tasks.
Motivated by the above literature, we compare standard language models with
distributionally robust ones and find that, somewhat surprisingly, the standard
language models are superior to the distributionally robust ones. We
investigate whether this is an effect of over-parameterization or relative
distributional stability, and find that the difference persists in the absence
of over-parameterization, and that drift is limited, confirming the relative
stability of creole languages.


---

