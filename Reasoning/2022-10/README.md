## What do Large Language Models Learn beyond Language?

**Published Date:** 2022-10-21T23:43:13Z

**Link:** http://arxiv.org/pdf/2210.12302v1

**Abstract:**

  Large language models (LMs) have rapidly become a mainstay in Natural
Language Processing. These models are known to acquire rich linguistic
knowledge from training on large amounts of text. In this paper, we investigate
if pre-training on text also confers these models with helpful `inductive
biases' for non-linguistic reasoning. On a set of 19 diverse non-linguistic
tasks involving quantitative computations, recognizing regular expressions and
reasoning over strings. We find that pretrained models significantly outperform
comparable non-pretrained neural models. This remains true also in experiments
with training non-pretrained models with fewer parameters to account for model
regularization effects. We further explore the effect of text domain on LMs by
pretraining models from text from different domains and provenances. Our
experiments surprisingly reveal that the positive effects of pre-training
persist even when pretraining on multi-lingual text or computer code, and even
for text generated from synthetic languages. Our findings suggest a hitherto
unexplored deep connection between pre-training and inductive learning
abilities of language models.


---

## Language Models are Multilingual Chain-of-Thought Reasoners

**Published Date:** 2022-10-06T17:03:34Z

**Link:** http://arxiv.org/pdf/2210.03057v1

**Abstract:**

  We evaluate the reasoning abilities of large language models in multilingual
settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by
manually translating 250 grade-school math problems from the GSM8K dataset
(Cobbe et al., 2021) into ten typologically diverse languages. We find that the
ability to solve MGSM problems via chain-of-thought prompting emerges with
increasing model scale, and that models have strikingly strong multilingual
reasoning abilities, even in underrepresented languages such as Bengali and
Swahili. Finally, we show that the multilingual reasoning abilities of language
models extend to other tasks such as commonsense reasoning and word-in-context
semantic judgment. The MGSM benchmark is publicly available at
https://github.com/google-research/url-nlp.


---

