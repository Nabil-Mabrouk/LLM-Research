[{"title": "Lost in Translation: Large Language Models in Non-English Content\n  Analysis", "published_date": "2023-06-12T19:10:47Z", "link": "http://arxiv.org/pdf/2306.07377v1", "abstract": "  In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa,\nGoogle's PaLM) have become the dominant approach for building AI systems to\nanalyze and generate language online. However, the automated systems that\nincreasingly mediate our interactions online -- such as chatbots, content\nmoderation systems, and search engines -- are primarily designed for and work\nfar more effectively in English than in the world's other 7,000 languages.\nRecently, researchers and technology companies have attempted to extend the\ncapabilities of large language models into languages other than English by\nbuilding what are called multilingual language models.\n  In this paper, we explain how these multilingual language models work and\nexplore their capabilities and limits. Part I provides a simple technical\nexplanation of how large language models work, why there is a gap in available\ndata between English and other languages, and how multilingual language models\nattempt to bridge that gap. Part II accounts for the challenges of doing\ncontent analysis with large language models in general and multilingual\nlanguage models in particular. Part III offers recommendations for companies,\nresearchers, and policymakers to keep in mind when considering researching,\ndeveloping and deploying large and multilingual language models.\n", "first_author": "Gabriel Nicholas", "year_month": "2023-06"}, {"title": "Cedille: A large autoregressive French language model", "published_date": "2022-02-07T17:40:43Z", "link": "http://arxiv.org/pdf/2202.03371v1", "abstract": "  Scaling up the size and training of autoregressive language models has\nenabled novel ways of solving Natural Language Processing tasks using zero-shot\nand few-shot learning. While extreme-scale language models such as GPT-3 offer\nmultilingual capabilities, zero-shot learning for languages other than English\nremain largely unexplored. Here, we introduce Cedille, a large open source\nauto-regressive language model, specifically trained for the French language.\nOur results show that Cedille outperforms existing French language models and\nis competitive with GPT-3 on a range of French zero-shot benchmarks.\nFurthermore, we provide an in-depth comparison of the toxicity exhibited by\nthese models, showing that Cedille marks an improvement in language model\nsafety thanks to dataset filtering.\n", "first_author": "Martin M\u00fcller", "year_month": "2022-02"}, {"title": "How Good are Commercial Large Language Models on African Languages?", "published_date": "2023-05-11T02:29:53Z", "link": "http://arxiv.org/pdf/2305.06530v1", "abstract": "  Recent advancements in Natural Language Processing (NLP) has led to the\nproliferation of large pretrained language models. These models have been shown\nto yield good performance, using in-context learning, even on unseen tasks and\nlanguages. They have also been exposed as commercial APIs as a form of\nlanguage-model-as-a-service, with great adoption. However, their performance on\nAfrican languages is largely unknown. We present a preliminary analysis of\ncommercial large language models on two tasks (machine translation and text\nclassification) across eight African languages, spanning different language\nfamilies and geographical areas. Our results suggest that commercial language\nmodels produce below-par performance on African languages. We also find that\nthey perform better on text classification than machine translation. In\ngeneral, our findings present a call-to-action to ensure African languages are\nwell represented in commercial large language models, given their growing\npopularity.\n", "first_author": "Jessica Ojo", "year_month": "2023-05"}, {"title": "A Precis of Language Models are not Models of Language", "published_date": "2022-05-16T12:50:58Z", "link": "http://arxiv.org/pdf/2205.07634v1", "abstract": "  Natural Language Processing is one of the leading application areas in the\ncurrent resurgence of Artificial Intelligence, spearheaded by Artificial Neural\nNetworks. We show that despite their many successes at performing linguistic\ntasks, Large Neural Language Models are ill-suited as comprehensive models of\nnatural language. The wider implication is that, in spite of the often\noverbearing optimism about AI, modern neural models do not represent a\nrevolution in our understanding of cognition.\n", "first_author": "Csaba Veres", "year_month": "2022-05"}, {"title": "Using large language models for (de-)formalization and natural\n  argumentation exercises for beginner's students", "published_date": "2023-04-12T23:05:02Z", "link": "http://arxiv.org/pdf/2304.06186v1", "abstract": "  We describe two systems that use text-davinci-003, a large language model,\nfor the automatized correction of (i) exercises in translating back and forth\nbetween natural language and the languages of propositional logic and\nfirst-order predicate logic and (ii) exercises in writing simple arguments in\nnatural language in non-mathematical scenarios.\n", "first_author": "Merlin Carl", "year_month": "2023-04"}, {"title": "Should we Stop Training More Monolingual Models, and Simply Use Machine\n  Translation Instead?", "published_date": "2021-04-21T10:21:24Z", "link": "http://arxiv.org/pdf/2104.10441v1", "abstract": "  Most work in NLP makes the assumption that it is desirable to develop\nsolutions in the native language in question. There is consequently a strong\ntrend towards building native language models even for low-resource languages.\nThis paper questions this development, and explores the idea of simply\ntranslating the data into English, thereby enabling the use of pretrained, and\nlarge-scale, English language models. We demonstrate empirically that a large\nEnglish language model coupled with modern machine translation outperforms\nnative language models in most Scandinavian languages. The exception to this is\nFinnish, which we assume is due to inferior translation quality. Our results\nsuggest that machine translation is a mature technology, which raises a serious\ncounter-argument for training native language models for low-resource\nlanguages. This paper therefore strives to make a provocative but important\npoint. As English language models are improving at an unprecedented pace, which\nin turn improves machine translation, it is from an empirical and environmental\nstand-point more effective to translate data from low-resource languages into\nEnglish, than to build language models for such languages.\n", "first_author": "Tim Isbister", "year_month": "2021-04"}, {"title": "Beyond the limitations of any imaginable mechanism: large language\n  models and psycholinguistics", "published_date": "2023-02-28T20:49:38Z", "link": "http://arxiv.org/pdf/2303.00077v1", "abstract": "  Large language models are not detailed models of human linguistic processing.\nThey are, however, extremely successful at their primary task: providing a\nmodel for language. For this reason and because there are no animal models for\nlanguage, large language models are important in psycholinguistics: they are\nuseful as a practical tool, as an illustrative comparative, and\nphilosophically, as a basis for recasting the relationship between language and\nthought.\n", "first_author": "Conor Houghton", "year_month": "2023-02"}, {"title": "Enhance Reasoning Ability of Visual-Language Models via Large Language\n  Models", "published_date": "2023-05-22T17:33:44Z", "link": "http://arxiv.org/pdf/2305.13267v1", "abstract": "  Pre-trained visual language models (VLM) have shown excellent performance in\nimage caption tasks. However, it sometimes shows insufficient reasoning\nability. In contrast, large language models (LLMs) emerge with powerful\nreasoning capabilities. Therefore, we propose a method called TReE, which\ntransfers the reasoning ability of a large language model to a visual language\nmodel in zero-shot scenarios. TReE contains three stages: observation,\nthinking, and re-thinking. Observation stage indicates that VLM obtains the\noverall information of the relative image. Thinking stage combines the image\ninformation and task description as the prompt of the LLM, inference with the\nrationals. Re-Thinking stage learns from rationale and then inference the final\nresult through VLM.\n", "first_author": "Yueting Yang", "year_month": "2023-05"}, {"title": "Images in Language Space: Exploring the Suitability of Large Language\n  Models for Vision & Language Tasks", "published_date": "2023-05-23T07:50:36Z", "link": "http://arxiv.org/pdf/2305.13782v1", "abstract": "  Large language models have demonstrated robust performance on various\nlanguage tasks using zero-shot or few-shot learning paradigms. While being\nactively researched, multimodal models that can additionally handle images as\ninput have yet to catch up in size and generality with language-only models. In\nthis work, we ask whether language-only models can be utilised for tasks that\nrequire visual input -- but also, as we argue, often require a strong reasoning\ncomponent. Similar to some recent related work, we make visual information\naccessible to the language model using separate verbalisation models.\nSpecifically, we investigate the performance of open-source, open-access\nlanguage models against GPT-3 on five vision-language tasks when given\ntextually-encoded visual information. Our results suggest that language models\nare effective for solving vision-language tasks even with limited samples. This\napproach also enhances the interpretability of a model's output by providing a\nmeans of tracing the output back through the verbalised image content.\n", "first_author": "Sherzod Hakimov", "year_month": "2023-05"}, {"title": "When Being Unseen from mBERT is just the Beginning: Handling New\n  Languages With Multilingual Language Models", "published_date": "2020-10-24T10:15:03Z", "link": "http://arxiv.org/pdf/2010.12858v2", "abstract": "  Transfer learning based on pretraining language models on a large amount of\nraw data has become a new norm to reach state-of-the-art performance in NLP.\nStill, it remains unclear how this approach should be applied for unseen\nlanguages that are not covered by any available large-scale multilingual\nlanguage model and for which only a small amount of raw data is generally\navailable. In this work, by comparing multilingual and monolingual models, we\nshow that such models behave in multiple ways on unseen languages. Some\nlanguages greatly benefit from transfer learning and behave similarly to\nclosely related high resource languages whereas others apparently do not.\nFocusing on the latter, we show that this failure to transfer is largely\nrelated to the impact of the script used to write such languages.\nTransliterating those languages improves very significantly the ability of\nlarge-scale multilingual language models on downstream tasks.\n", "first_author": "Benjamin Muller", "year_month": "2020-10"}, {"title": "Language Modelling for Source Code with Transformer-XL", "published_date": "2020-07-31T02:42:18Z", "link": "http://arxiv.org/pdf/2007.15813v1", "abstract": "  It has been found that software, like natural language texts, exhibits\n\"naturalness\", which can be captured by statistical language models. In recent\nyears, neural language models have been proposed to represent the naturalness\nof software through deep learning. In this paper, we conduct an experimental\nevaluation of state-of-the-art neural language models for source code,\nincluding RNN-based models and Transformer-XL based models. Through experiments\non a large-scale Python code corpus, we find that the Transformer-XL model\noutperforms RNN-based models (including LSTM and GRU models) in capturing the\nnaturalness of software, with far less computational cost.\n", "first_author": "Thomas Dowdell", "year_month": "2020-07"}, {"title": "MiLMo:Minority Multilingual Pre-trained Language Model", "published_date": "2022-12-04T09:28:17Z", "link": "http://arxiv.org/pdf/2212.01779v2", "abstract": "  Pre-trained language models are trained on large-scale unsupervised data, and\nthey can fine-turn the model only on small-scale labeled datasets, and achieve\ngood results. Multilingual pre-trained language models can be trained on\nmultiple languages, and the model can understand multiple languages at the same\ntime. At present, the search on pre-trained models mainly focuses on rich\nresources, while there is relatively little research on low-resource languages\nsuch as minority languages, and the public multilingual pre-trained language\nmodel can not work well for minority languages. Therefore, this paper\nconstructs a multilingual pre-trained model named MiLMo that performs better on\nminority language tasks, including Mongolian, Tibetan, Uyghur, Kazakh and\nKorean. To solve the problem of scarcity of datasets on minority languages and\nverify the effectiveness of the MiLMo model, this paper constructs a minority\nmultilingual text classification dataset named MiTC, and trains a word2vec\nmodel for each language. By comparing the word2vec model and the pre-trained\nmodel in the text classification task, this paper provides an optimal scheme\nfor the downstream task research of minority languages. The final experimental\nresults show that the performance of the pre-trained model is better than that\nof the word2vec model, and it has achieved the best results in minority\nmultilingual text classification. The multilingual pre-trained model MiLMo,\nmultilingual word2vec model and multilingual text classification dataset MiTC\nare published on http://milmo.cmli-nlp.com/.\n", "first_author": "Junjie Deng", "year_month": "2022-12"}, {"title": "Simulation of language competition by physicists", "published_date": "2005-11-05T18:01:10Z", "link": "http://arxiv.org/pdf/physics/0511049v1", "abstract": "  Following Abrams and Strogatz 2003 and Patriarca and Leppanen 2004, five\nother physics groups independently started to simulate the competition of\nlanguages, as opposed to the evolution of a human language out of ape sounds,\nor the learning of a language by a child. This talk concentrates on the models\nof Christian Schulze et al and of Viviane de Oliveira et al which allow the\nsimulation of a large number of languages, similar to today's 8,000 human\nlanguages. The first model deals with a continuous process of random mutations\nof a language, transfer from and to other languages, and flight away from\nlanguages spoken by only a few people. The second model combines these flight\nand mutation aspects, ignores transfer and describes the colonization of a\nlarge geographical region by people starting at one lattice point. The size of\na language is defined by the number of people speaking it. The first model\ngives a realistic log-normal shape for the histogram of language sizes but the\nnumbers are bad. For the second model our Monte Carlo simulations give sizes up\nto thousand million, but not the nearly log-normal shape. A meeting is planned\nfor mid-September 2006 in Poland.\n", "first_author": "Christian Schulze", "year_month": "2005-11"}, {"title": "Learning to Scale Multilingual Representations for Vision-Language Tasks", "published_date": "2020-04-09T01:03:44Z", "link": "http://arxiv.org/pdf/2004.04312v2", "abstract": "  Current multilingual vision-language models either require a large number of\nadditional parameters for each supported language, or suffer performance\ndegradation as languages are added. In this paper, we propose a Scalable\nMultilingual Aligned Language Representation (SMALR) that supports many\nlanguages with few model parameters without sacrificing downstream task\nperformance. SMALR learns a fixed size language-agnostic representation for\nmost words in a multilingual vocabulary, keeping language-specific features for\njust a few. We use a masked cross-language modeling loss to align features with\ncontext from other languages. Additionally, we propose a cross-lingual\nconsistency module that ensures predictions made for a query and its machine\ntranslation are comparable. The effectiveness of SMALR is demonstrated with ten\ndiverse languages, over twice the number supported in vision-language tasks to\ndate. We evaluate on multilingual image-sentence retrieval and outperform prior\nwork by 3-4% with less than 1/5th the training parameters compared to other\nword embedding methods.\n", "first_author": "Andrea Burns", "year_month": "2020-04"}, {"title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages", "published_date": "2023-06-29T08:20:57Z", "link": "http://arxiv.org/pdf/2306.16774v1", "abstract": "  Vision-Language Pre-training (VLP) has advanced the performance of many\nvision-language tasks, such as image-text retrieval, visual entailment, and\nvisual reasoning. The pre-training mostly utilizes lexical databases and image\nqueries in English. Previous work has demonstrated that the pre-training in\nEnglish does not transfer well to other languages in a zero-shot setting.\nHowever, multilingual pre-trained language models (MPLM) have excelled at a\nvariety of single-modal language tasks. In this paper, we propose a simple yet\nefficient approach to adapt VLP to unseen languages using MPLM. We utilize a\ncross-lingual contextualized token embeddings alignment approach to train text\nencoders for non-English languages. Our approach does not require image input\nand primarily uses machine translation, eliminating the need for target\nlanguage data. Our evaluation across three distinct tasks (image-text\nretrieval, visual entailment, and natural language visual reasoning)\ndemonstrates that this approach outperforms the state-of-the-art multilingual\nvision-language models without requiring large parallel corpora. Our code is\navailable at https://github.com/Yasminekaroui/CliCoTea.\n", "first_author": "Yasmine Karoui", "year_month": "2023-06"}, {"title": "ChatLaw: Open-Source Legal Large Language Model with Integrated External\n  Knowledge Bases", "published_date": "2023-06-28T10:48:34Z", "link": "http://arxiv.org/pdf/2306.16092v1", "abstract": "  Large Language Models (LLMs) have shown the potential to revolutionize\nnatural language processing tasks in various domains, sparking great interest\nin vertical-specific large models. However, unlike proprietary models such as\nBloombergGPT and FinGPT, which have leveraged their unique data accumulations\nto make strides in the finance domain, there hasn't not many similar large\nlanguage models in the Chinese legal domain to facilitate its digital\ntransformation.\n  In this paper, we propose an open-source legal large language model named\nChatLaw. Due to the importance of data quality, we carefully designed a legal\ndomain fine-tuning dataset. Additionally, to overcome the problem of model\nhallucinations in legal data screening during reference data retrieval, we\nintroduce a method that combines vector database retrieval with keyword\nretrieval to effectively reduce the inaccuracy of relying solely on vector\ndatabase retrieval. Furthermore, we propose a self-attention method to enhance\nthe ability of large models to overcome errors present in reference data,\nfurther optimizing the issue of model hallucinations at the model level and\nimproving the problem-solving capabilities of large models. We also\nopen-sourced our model and part of the data at\nhttps://github.com/PKU-YuanGroup/ChatLaw.\n", "first_author": "Jiaxi Cui", "year_month": "2023-06"}, {"title": "Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination", "published_date": "2022-10-21T21:33:10Z", "link": "http://arxiv.org/pdf/2210.12261v1", "abstract": "  Large-scale pretrained language models have made significant advances in\nsolving downstream language understanding tasks. However, they generally suffer\nfrom reporting bias, the phenomenon describing the lack of explicit commonsense\nknowledge in written text, e.g., ''an orange is orange''. To overcome this\nlimitation, we develop a novel approach, Z-LaVI, to endow language models with\nvisual imagination capabilities. Specifically, we leverage two complementary\ntypes of ''imaginations'': (i) recalling existing images through retrieval and\n(ii) synthesizing nonexistent images via text-to-image generation. Jointly\nexploiting the language inputs and the imagination, a pretrained\nvision-language model (e.g., CLIP) eventually composes a zero-shot solution to\nthe original language tasks. Notably, fueling language models with imagination\ncan effectively leverage visual knowledge to solve plain language tasks. In\nconsequence, Z-LaVI consistently improves the zero-shot performance of existing\nlanguage models across a diverse set of language tasks.\n", "first_author": "Yue Yang", "year_month": "2022-10"}, {"title": "Regularization Advantages of Multilingual Neural Language Models for Low\n  Resource Domains", "published_date": "2019-05-29T13:27:11Z", "link": "http://arxiv.org/pdf/1906.01496v1", "abstract": "  Neural language modeling (LM) has led to significant improvements in several\napplications, including Automatic Speech Recognition. However, they typically\nrequire large amounts of training data, which is not available for many domains\nand languages. In this study, we propose a multilingual neural language model\narchitecture, trained jointly on the domain-specific data of several\nlow-resource languages. The proposed multilingual LM consists of language\nspecific word embeddings in the encoder and decoder, and one language specific\nLSTM layer, plus two LSTM layers with shared parameters across the languages.\nThis multilingual LM model facilitates transfer learning across the languages,\nacting as an extra regularizer in very low-resource scenarios. We integrate our\nproposed multilingual approach with a state-of-the-art highly-regularized\nneural LM, and evaluate on the conversational data domain for four languages\nover a range of training data sizes. Compared to monolingual LMs, the results\nshow significant improvements of our proposed multilingual LM when the amount\nof available training data is limited, indicating the advantages of\ncross-lingual parameter sharing in very low-resource language modeling.\n", "first_author": "Navid Rekabsaz", "year_month": "2019-05"}, {"title": "Language Modeling with Sparse Product of Sememe Experts", "published_date": "2018-10-29T20:13:05Z", "link": "http://arxiv.org/pdf/1810.12387v1", "abstract": "  Most language modeling methods rely on large-scale data to statistically\nlearn the sequential patterns of words. In this paper, we argue that words are\natomic language units but not necessarily atomic semantic units. Inspired by\nHowNet, we use sememes, the minimum semantic units in human languages, to\nrepresent the implicit semantics behind words for language modeling, named\nSememe-Driven Language Model (SDLM). More specifically, to predict the next\nword, SDLM first estimates the sememe distribution gave textual context.\nAfterward, it regards each sememe as a distinct semantic expert, and these\nexperts jointly identify the most probable senses and the corresponding word.\nIn this way, SDLM enables language models to work beyond word-level\nmanipulation to fine-grained sememe-level semantics and offers us more powerful\ntools to fine-tune language models and improve the interpretability as well as\nthe robustness of language models. Experiments on language modeling and the\ndownstream application of headline gener- ation demonstrate the significant\neffect of SDLM. Source code and data used in the experiments can be accessed at\nhttps:// github.com/thunlp/SDLM-pytorch.\n", "first_author": "Yihong Gu", "year_month": "2018-10"}, {"title": "Language Transfer of Audio Word2Vec: Learning Audio Segment\n  Representations without Target Language Data", "published_date": "2017-07-19T10:54:00Z", "link": "http://arxiv.org/pdf/1707.06519v1", "abstract": "  Audio Word2Vec offers vector representations of fixed dimensionality for\nvariable-length audio segments using Sequence-to-sequence Autoencoder (SA).\nThese vector representations are shown to describe the sequential phonetic\nstructures of the audio segments to a good degree, with real world applications\nsuch as query-by-example Spoken Term Detection (STD). This paper examines the\ncapability of language transfer of Audio Word2Vec. We train SA from one\nlanguage (source language) and use it to extract the vector representation of\nthe audio segments of another language (target language). We found that SA can\nstill catch phonetic structure from the audio segments of the target language\nif the source and target languages are similar. In query-by-example STD, we\nobtain the vector representations from the SA learned from a large amount of\nsource language data, and found them surpass the representations from naive\nencoder and SA directly learned from a small amount of target language data.\nThe result shows that it is possible to learn Audio Word2Vec model from\nhigh-resource languages and use it on low-resource languages. This further\nexpands the usability of Audio Word2Vec.\n", "first_author": "Chia-Hao Shen", "year_month": "2017-07"}, {"title": "Multilingual Constituency Parsing with Self-Attention and Pre-Training", "published_date": "2018-12-31T11:01:02Z", "link": "http://arxiv.org/pdf/1812.11760v2", "abstract": "  We show that constituency parsing benefits from unsupervised pre-training\nacross a variety of languages and a range of pre-training conditions. We first\ncompare the benefits of no pre-training, fastText, ELMo, and BERT for English\nand find that BERT outperforms ELMo, in large part due to increased model\ncapacity, whereas ELMo in turn outperforms the non-contextual fastText\nembeddings. We also find that pre-training is beneficial across all 11\nlanguages tested; however, large model sizes (more than 100 million parameters)\nmake it computationally expensive to train separate models for each language.\nTo address this shortcoming, we show that joint multilingual pre-training and\nfine-tuning allows sharing all but a small number of parameters between ten\nlanguages in the final model. The 10x reduction in model size compared to\nfine-tuning one model per language causes only a 3.2% relative error increase\nin aggregate. We further explore the idea of joint fine-tuning and show that it\ngives low-resource languages a way to benefit from the larger datasets of other\nlanguages. Finally, we demonstrate new state-of-the-art results for 11\nlanguages, including English (95.8 F1) and Chinese (91.8 F1).\n", "first_author": "Nikita Kitaev", "year_month": "2018-12"}, {"title": "Data Augmentation for Spoken Language Understanding via Pretrained\n  Language Models", "published_date": "2020-04-29T04:07:12Z", "link": "http://arxiv.org/pdf/2004.13952v2", "abstract": "  The training of spoken language understanding (SLU) models often faces the\nproblem of data scarcity. In this paper, we put forward a data augmentation\nmethod using pretrained language models to boost the variability and accuracy\nof generated utterances. Furthermore, we investigate and propose solutions to\ntwo previously overlooked semi-supervised learning scenarios of data scarcity\nin SLU: i) Rich-in-Ontology: ontology information with numerous valid dialogue\nacts is given; ii) Rich-in-Utterance: a large number of unlabelled utterances\nare available. Empirical results show that our method can produce synthetic\ntraining data that boosts the performance of language understanding models in\nvarious scenarios.\n", "first_author": "Baolin Peng", "year_month": "2020-04"}, {"title": "Autocorrelations Decay in Texts and Applicability Limits of Language\n  Models", "published_date": "2023-05-11T07:23:01Z", "link": "http://arxiv.org/pdf/2305.06615v1", "abstract": "  We show that the laws of autocorrelations decay in texts are closely related\nto applicability limits of language models. Using distributional semantics we\nempirically demonstrate that autocorrelations of words in texts decay according\nto a power law. We show that distributional semantics provides coherent\nautocorrelations decay exponents for texts translated to multiple languages.\nThe autocorrelations decay in generated texts is quantitatively and often\nqualitatively different from the literary texts. We conclude that language\nmodels exhibiting Markov behavior, including large autoregressive language\nmodels, may have limitations when applied to long texts, whether analysis or\ngeneration.\n", "first_author": "Nikolay Mikhaylovskiy", "year_month": "2023-05"}, {"title": "InforMask: Unsupervised Informative Masking for Language Model\n  Pretraining", "published_date": "2022-10-21T07:10:56Z", "link": "http://arxiv.org/pdf/2210.11771v1", "abstract": "  Masked language modeling is widely used for pretraining large language models\nfor natural language understanding (NLU). However, random masking is\nsuboptimal, allocating an equal masking rate for all tokens. In this paper, we\npropose InforMask, a new unsupervised masking strategy for training masked\nlanguage models. InforMask exploits Pointwise Mutual Information (PMI) to\nselect the most informative tokens to mask. We further propose two\noptimizations for InforMask to improve its efficiency. With a one-off\npreprocessing step, InforMask outperforms random masking and previously\nproposed masking strategies on the factual recall benchmark LAMA and the\nquestion answering benchmark SQuAD v1 and v2.\n", "first_author": "Nafis Sadeq", "year_month": "2022-10"}, {"title": "Structured Pruning of Large Language Models", "published_date": "2019-10-10T17:44:18Z", "link": "http://arxiv.org/pdf/1910.04732v2", "abstract": "  Large language models have recently achieved state of the art performance\nacross a wide variety of natural language tasks. Meanwhile, the size of these\nmodels and their latency have significantly increased, which makes their usage\ncostly, and raises an interesting question: do language models need to be\nlarge? We study this question through the lens of model compression. We present\na generic, structured pruning approach by parameterizing each weight matrix\nusing its low-rank factorization, and adaptively removing rank-1 components\nduring training. On language modeling tasks, our structured approach\noutperforms other unstructured and block-structured pruning baselines at\nvarious compression levels, while achieving significant speedups during both\ntraining and inference. We also demonstrate that our method can be applied to\npruning adaptive word embeddings in large language models, and to pruning the\nBERT model on several downstream fine-tuning classification benchmarks.\n", "first_author": "Ziheng Wang", "year_month": "2019-10"}, {"title": "Is the Language Familiarity Effect gradual? A computational modelling\n  approach", "published_date": "2022-06-27T16:08:42Z", "link": "http://arxiv.org/pdf/2206.13415v1", "abstract": "  According to the Language Familiarity Effect (LFE), people are better at\ndiscriminating between speakers of their native language. Although this\ncognitive effect was largely studied in the literature, experiments have only\nbeen conducted on a limited number of language pairs and their results only\nshow the presence of the effect without yielding a gradual measure that may\nvary across language pairs. In this work, we show that the computational model\nof LFE introduced by Thorburn, Feldmand and Schatz (2019) can address these two\nlimitations. In a first experiment, we attest to this model's capacity to\nobtain a gradual measure of the LFE by replicating behavioural findings on\nnative and accented speech. In a second experiment, we evaluate LFE on a large\nnumber of language pairs, including many which have never been tested on\nhumans. We show that the effect is replicated across a wide array of languages,\nproviding further evidence of its universality. Building on the gradual measure\nof LFE, we also show that languages belonging to the same family yield smaller\nscores, supporting the idea of an effect of language distance on LFE.\n", "first_author": "Maureen de Seyssel", "year_month": "2022-06"}, {"title": "MetaVL: Transferring In-Context Learning Ability From Language Models to\n  Vision-Language Models", "published_date": "2023-06-02T07:21:03Z", "link": "http://arxiv.org/pdf/2306.01311v1", "abstract": "  Large-scale language models have shown the ability to adapt to a new task via\nconditioning on a few demonstrations (i.e., in-context learning). However, in\nthe vision-language domain, most large-scale pre-trained vision-language (VL)\nmodels do not possess the ability to conduct in-context learning. How can we\nenable in-context learning for VL models? In this paper, we study an\ninteresting hypothesis: can we transfer the in-context learning ability from\nthe language domain to VL domain? Specifically, we first meta-trains a language\nmodel to perform in-context learning on NLP tasks (as in MetaICL); then we\ntransfer this model to perform VL tasks by attaching a visual encoder. Our\nexperiments suggest that indeed in-context learning ability can be transferred\ncross modalities: our model considerably improves the in-context learning\ncapability on VL tasks and can even compensate for the size of the model\nsignificantly. On VQA, OK-VQA, and GQA, our method could outperform the\nbaseline model while having 20 times fewer parameters.\n", "first_author": "Masoud Monajatipoor", "year_month": "2023-06"}, {"title": "Multilevel Large Language Models for Everyone", "published_date": "2023-07-25T03:18:04Z", "link": "http://arxiv.org/pdf/2307.13221v1", "abstract": "  Large language models have made significant progress in the past few years.\nHowever, they are either generic {\\it or} field specific, splitting the\ncommunity into different groups. In this paper, we unify these large language\nmodels into a larger map, where the generic {\\it and} specific models are\nlinked together and can improve each other, based on the user personal input\nand information from the internet. The idea of linking several large language\nmodels together is inspired by the functionality of human brain. The specific\nregions on the brain cortex are specific for certain low level functionality.\nAnd these regions can jointly work together to achieve more complex high level\nfunctionality. Such behavior on human brain cortex sheds the light to design\nthe multilevel large language models that contain global level, field level and\nuser level models. The user level models run on local machines to achieve\nefficient response and protect the user's privacy. Such multilevel models\nreduce some redundancy and perform better than the single level models. The\nproposed multilevel idea can be applied in various applications, such as\nnatural language processing, computer vision tasks, professional assistant,\nbusiness and healthcare.\n", "first_author": "Yuanhao Gong", "year_month": "2023-07"}, {"title": "Improving Cross-lingual Information Retrieval on Low-Resource Languages\n  via Optimal Transport Distillation", "published_date": "2023-01-29T22:30:36Z", "link": "http://arxiv.org/pdf/2301.12566v1", "abstract": "  Benefiting from transformer-based pre-trained language models, neural ranking\nmodels have made significant progress. More recently, the advent of\nmultilingual pre-trained language models provides great support for designing\nneural cross-lingual retrieval models. However, due to unbalanced pre-training\ndata in different languages, multilingual language models have already shown a\nperformance gap between high and low-resource languages in many downstream\ntasks. And cross-lingual retrieval models built on such pre-trained models can\ninherit language bias, leading to suboptimal result for low-resource languages.\nMoreover, unlike the English-to-English retrieval task, where large-scale\ntraining collections for document ranking such as MS MARCO are available, the\nlack of cross-lingual retrieval data for low-resource language makes it more\nchallenging for training cross-lingual retrieval models. In this work, we\npropose OPTICAL: Optimal Transport distillation for low-resource Cross-lingual\ninformation retrieval. To transfer a model from high to low resource languages,\nOPTICAL forms the cross-lingual token alignment task as an optimal transport\nproblem to learn from a well-trained monolingual retrieval model. By separating\nthe cross-lingual knowledge from knowledge of query document matching, OPTICAL\nonly needs bitext data for distillation training, which is more feasible for\nlow-resource languages. Experimental results show that, with minimal training\ndata, OPTICAL significantly outperforms strong baselines on low-resource\nlanguages, including neural machine translation.\n", "first_author": "Zhiqi Huang", "year_month": "2023-01"}, {"title": "Spontaneous Emerging Preference in Two-tower Language Model", "published_date": "2022-10-13T13:55:19Z", "link": "http://arxiv.org/pdf/2210.07041v1", "abstract": "  The ever-growing size of the foundation language model has brought\nsignificant performance gains in various types of downstream tasks. With the\nexistence of side-effects brought about by the large size of the foundation\nlanguage model such as deployment cost, availability issues, and environmental\ncost, there is some interest in exploring other possible directions, such as a\ndivide-and-conquer scheme. In this paper, we are asking a basic question: are\nlanguage processes naturally dividable? We study this problem with a simple\ntwo-tower language model setting, where two language models with identical\nconfigurations are trained side-by-side cooperatively. With this setting, we\ndiscover the spontaneous emerging preference phenomenon, where some of the\ntokens are consistently better predicted by one tower while others by another\ntower. This phenomenon is qualitatively stable, regardless of model\nconfiguration and type, suggesting this as an intrinsic property of natural\nlanguage. This study suggests that interesting properties of natural language\nare still waiting to be discovered, which may aid the future development of\nnatural language processing techniques.\n", "first_author": "Zhengqi He", "year_month": "2022-10"}, {"title": "Can Large Language Models Infer and Disagree Like Humans?", "published_date": "2023-05-23T07:55:34Z", "link": "http://arxiv.org/pdf/2305.13788v1", "abstract": "  Large Language Models (LLMs) have shown stellar achievements in solving a\nbroad range of tasks. When generating text, it is common to sample tokens from\nthese models: whether LLMs closely align with the human disagreement\ndistribution has not been well-studied, especially within the scope of Natural\nLanguage Inference (NLI). In this paper, we evaluate the performance and\nalignment of LLM distribution with humans using two different techniques: Monte\nCarlo Reconstruction (MCR) and Log Probability Reconstruction (LPR). As a\nresult, we show LLMs exhibit limited ability in solving NLI tasks and\nsimultaneously fail to capture human disagreement distribution, raising\nconcerns about their natural language understanding (NLU) ability and their\nrepresentativeness of human users.\n", "first_author": "Noah Lee", "year_month": "2023-05"}, {"title": "Instruction Tuning with Lexicons for Zero-Shot Style Classification", "published_date": "2023-05-24T00:17:36Z", "link": "http://arxiv.org/pdf/2305.14592v1", "abstract": "  Style is used to convey authors' intentions and attitudes. Despite the\nsuccess of large pre-trained language models on style classification, prior\nwork relies on fine-tuning with labeled examples. Prompting large language\nmodels to classify style without fine-tuning is challenging because language\nstyles can be difficult to define. In this study, we investigate the\neffectiveness of style lexicons as a means for instructing language models how\nto identify new styles that are unseen during training. Our experiments show\nthat lexicon-based instructions improve transfer zero-shot performance\nsignificantly. We will release our code and data.\n", "first_author": "Ruohao Guo", "year_month": "2023-05"}, {"title": "A Systematic Evaluation of Large Language Models of Code", "published_date": "2022-02-26T15:53:55Z", "link": "http://arxiv.org/pdf/2202.13169v3", "abstract": "  Large language models (LMs) of code have recently shown tremendous promise in\ncompleting code and synthesizing code from natural language descriptions.\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\n2021)) are not publicly available, leaving many questions about their model and\ndata design decisions. We aim to fill in some of these blanks through a\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\nCodex itself is not open-source, we find that existing open-source models do\nachieve close results in some programming languages, although targeted mainly\nfor natural language modeling. We further identify an important missing piece\nin the form of a large open-source model trained exclusively on a multi-lingual\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\non the GPT-2 architecture, which was trained on 249GB of code across 12\nprogramming languages on a single machine. In the C programming language,\nPolyCoder outperforms all models including Codex. Our trained models are\nopen-source and publicly available at https://github.com/VHellendoorn/Code-LMs,\nwhich enables future research and application in this area.\n", "first_author": "Frank F. Xu", "year_month": "2022-02"}, {"title": "Exploring Large Language Models for Classical Philology", "published_date": "2023-05-23T05:21:02Z", "link": "http://arxiv.org/pdf/2305.13698v1", "abstract": "  Recent advances in NLP have led to the creation of powerful language models\nfor many languages including Ancient Greek and Latin. While prior work on\nClassical languages unanimously uses BERT, in this work we create four language\nmodels for Ancient Greek that vary along two dimensions to study their\nversatility for tasks of interest for Classical languages: we explore (i)\nencoder-only and encoder-decoder architectures using RoBERTa and T5 as strong\nmodel types, and create for each of them (ii) a monolingual Ancient Greek and a\nmultilingual instance that includes Latin and English. We evaluate all models\non morphological and syntactic tasks, including lemmatization, which\ndemonstrates the added value of T5's decoding abilities. We further define two\nprobing tasks to investigate the knowledge acquired by models pre-trained on\nClassical texts. Our experiments provide the first benchmarking analysis of\nexisting models of Ancient Greek. Results show that our models provide\nsignificant improvements over the SoTA. The systematic analysis of model types\ncan inform future research in designing language models for Classical\nlanguages, including the development of novel generative tasks. We make all our\nmodels available as community resources, along with a large curated\npre-training corpus for Ancient Greek, to support the creation of a larger,\ncomparable model zoo for Classical Philology. Our models and resources are\navailable at https://github.com/Heidelberg-NLP/ancient-language-models.\n", "first_author": "Frederick Riemenschneider", "year_month": "2023-05"}, {"title": "Knowledge of cultural moral norms in large language models", "published_date": "2023-06-02T18:23:35Z", "link": "http://arxiv.org/pdf/2306.01857v1", "abstract": "  Moral norms vary across cultures. A recent line of work suggests that English\nlarge language models contain human-like moral biases, but these studies\ntypically do not examine moral variation in a diverse cultural setting. We\ninvestigate the extent to which monolingual English language models contain\nknowledge about moral norms in different countries. We consider two levels of\nanalysis: 1) whether language models capture fine-grained moral variation\nacross countries over a variety of topics such as ``homosexuality'' and\n``divorce''; 2) whether language models capture cultural diversity and shared\ntendencies in which topics people around the globe tend to diverge or agree on\nin their moral judgment. We perform our analyses with two public datasets from\nthe World Values Survey (across 55 countries) and PEW global surveys (across 40\ncountries) on morality. We find that pre-trained English language models\npredict empirical moral norms across countries worse than the English moral\nnorms reported previously. However, fine-tuning language models on the survey\ndata improves inference across countries at the expense of a less accurate\nestimate of the English moral norms. We discuss the relevance and challenges of\nincorporating cultural knowledge into the automated inference of moral norms.\n", "first_author": "Aida Ramezani", "year_month": "2023-06"}, {"title": "Neural Cross-Lingual Relation Extraction Based on Bilingual Word\n  Embedding Mapping", "published_date": "2019-10-31T19:30:54Z", "link": "http://arxiv.org/pdf/1911.00069v1", "abstract": "  Relation extraction (RE) seeks to detect and classify semantic relationships\nbetween entities, which provides useful information for many NLP applications.\nSince the state-of-the-art RE models require large amounts of manually\nannotated data and language-specific resources to achieve high accuracy, it is\nvery challenging to transfer an RE model of a resource-rich language to a\nresource-poor language. In this paper, we propose a new approach for\ncross-lingual RE model transfer based on bilingual word embedding mapping. It\nprojects word embeddings from a target language to a source language, so that a\nwell-trained source-language neural network RE model can be directly applied to\nthe target language. Experiment results show that the proposed approach\nachieves very good performance for a number of target languages on both\nin-house and open datasets, using a small bilingual dictionary with only 1K\nword pairs.\n", "first_author": "Jian Ni", "year_month": "2019-10"}, {"title": "ClimateBert: A Pretrained Language Model for Climate-Related Text", "published_date": "2021-10-22T18:47:34Z", "link": "http://arxiv.org/pdf/2110.12010v3", "abstract": "  Over the recent years, large pretrained language models (LM) have\nrevolutionized the field of natural language processing (NLP). However, while\npretraining on general language has been shown to work very well for common\nlanguage, it has been observed that niche language poses problems. In\nparticular, climate-related texts include specific language that common LMs can\nnot represent accurately. We argue that this shortcoming of today's LMs limits\nthe applicability of modern NLP to the broad field of text processing of\nclimate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based\nlanguage model that is further pretrained on over 2 million paragraphs of\nclimate-related texts, crawled from various sources such as common news,\nresearch articles, and climate reporting of companies. We find that CLIMATEBERT\nleads to a 48% improvement on a masked language model objective which, in turn,\nleads to lowering error rates by 3.57% to 35.71% for various climate-related\ndownstream tasks like text classification, sentiment analysis, and\nfact-checking.\n", "first_author": "Nicolas Webersinke", "year_month": "2021-10"}, {"title": "Emerging Language Spaces Learned From Massively Multilingual Corpora", "published_date": "2018-02-01T12:58:16Z", "link": "http://arxiv.org/pdf/1802.00273v1", "abstract": "  Translations capture important information about languages that can be used\nas implicit supervision in learning linguistic properties and semantic\nrepresentations. In an information-centric view, translated texts may be\nconsidered as semantic mirrors of the original text and the significant\nvariations that we can observe across various languages can be used to\ndisambiguate a given expression using the linguistic signal that is grounded in\ntranslation. Parallel corpora consisting of massive amounts of human\ntranslations with a large linguistic variation can be applied to increase\nabstractions and we propose the use of highly multilingual machine translation\nmodels to find language-independent meaning representations. Our initial\nexperiments show that neural machine translation models can indeed learn in\nsuch a setup and we can show that the learning algorithm picks up information\nabout the relation between languages in order to optimize transfer leaning with\nshared parameters. The model creates a continuous language space that\nrepresents relationships in terms of geometric distances, which we can\nvisualize to illustrate how languages cluster according to language families\nand groups. Does this open the door for new ideas of data-driven language\ntypology with promising models and techniques in empirical cross-linguistic\nresearch?\n", "first_author": "J\u00f6rg Tiedemann", "year_month": "2018-02"}, {"title": "Cross-modal Language Generation using Pivot Stabilization for Web-scale\n  Language Coverage", "published_date": "2020-05-01T06:58:18Z", "link": "http://arxiv.org/pdf/2005.00246v1", "abstract": "  Cross-modal language generation tasks such as image captioning are directly\nhurt in their ability to support non-English languages by the trend of\ndata-hungry models combined with the lack of non-English annotations. We\ninvestigate potential solutions for combining existing language-generation\nannotations in English with translation capabilities in order to create\nsolutions at web-scale in both domain and language coverage. We describe an\napproach called Pivot-Language Generation Stabilization (PLuGS), which\nleverages directly at training time both existing English annotations (gold\ndata) as well as their machine-translated versions (silver data); at run-time,\nit generates first an English caption and then a corresponding target-language\ncaption. We show that PLuGS models outperform other candidate solutions in\nevaluations performed over 5 different target languages, under a large-domain\ntestset using images from the Open Images dataset. Furthermore, we find an\ninteresting effect where the English captions generated by the PLuGS models are\nbetter than the captions generated by the original, monolingual English model.\n", "first_author": "Ashish V. Thapliyal", "year_month": "2020-05"}, {"title": "Large Language Models for Business Process Management: Opportunities and\n  Challenges", "published_date": "2023-04-09T20:32:09Z", "link": "http://arxiv.org/pdf/2304.04309v1", "abstract": "  Large language models are deep learning models with a large number of\nparameters. The models made noticeable progress on a large number of tasks, and\nas a consequence allowing them to serve as valuable and versatile tools for a\ndiverse range of applications. Their capabilities also offer opportunities for\nbusiness process management, however, these opportunities have not yet been\nsystematically investigated. In this paper, we address this research problem by\nforegrounding various management tasks of the BPM lifecycle. We investigate six\nresearch directions highlighting problems that need to be addressed when using\nlarge language models, including usage guidelines for practitioners.\n", "first_author": "Maxim Vidgof", "year_month": "2023-04"}, {"title": "Norm Participation Grounds Language", "published_date": "2022-06-06T20:21:59Z", "link": "http://arxiv.org/pdf/2206.02885v2", "abstract": "  The striking recent advances in eliciting seemingly meaningful language\nbehaviour from language-only machine learning models have only made more\napparent, through the surfacing of clear limitations, the need to go beyond the\nlanguage-only mode and to ground these models \"in the world\". Proposals for\ndoing so vary in the details, but what unites them is that the solution is\nsought in the addition of non-linguistic data types such as images or video\nstreams, while largely keeping the mode of learning constant. I propose a\ndifferent, and more wide-ranging conception of how grounding should be\nunderstood: What grounds language is its normative nature. There are standards\nfor doing things right, these standards are public and authoritative, while at\nthe same time acceptance of authority can and must be disputed and negotiated,\nin interactions in which only bearers of normative status can rightfully\nparticipate. What grounds language, then, is the determined use that language\nusers make of it, and what it is grounded in is the community of language\nusers. I sketch this idea, and draw some conclusions for work on computational\nmodelling of meaningful language use.\n", "first_author": "David Schlangen", "year_month": "2022-06"}, {"title": "Gender Lost In Translation: How Bridging The Gap Between Languages\n  Affects Gender Bias in Zero-Shot Multilingual Translation", "published_date": "2023-05-26T13:51:50Z", "link": "http://arxiv.org/pdf/2305.16935v1", "abstract": "  Neural machine translation (NMT) models often suffer from gender biases that\nharm users and society at large. In this work, we explore how bridging the gap\nbetween languages for which parallel data is not available affects gender bias\nin multilingual NMT, specifically for zero-shot directions. We evaluate\ntranslation between grammatical gender languages which requires preserving the\ninherent gender information from the source in the target language. We study\nthe effect of encouraging language-agnostic hidden representations on models'\nability to preserve gender and compare pivot-based and zero-shot translation\nregarding the influence of the bridge language (participating in all language\npairs during training) on gender preservation. We find that language-agnostic\nrepresentations mitigate zero-shot models' masculine bias, and with increased\nlevels of gender inflection in the bridge language, pivoting surpasses\nzero-shot translation regarding fairer gender preservation for speaker-related\ngender agreement.\n", "first_author": "Lena Cabrera", "year_month": "2023-05"}, {"title": "Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in\n  Natural Language Understanding", "published_date": "2022-04-13T10:32:03Z", "link": "http://arxiv.org/pdf/2204.06283v2", "abstract": "  In the age of large transformer language models, linguistic evaluation play\nan important role in diagnosing models' abilities and limitations on natural\nlanguage understanding. However, current evaluation methods show some\nsignificant shortcomings. In particular, they do not provide insight into how\nwell a language model captures distinct linguistic skills essential for\nlanguage understanding and reasoning. Thus they fail to effectively map out the\naspects of language understanding that remain challenging to existing models,\nwhich makes it hard to discover potential limitations in models and datasets.\nIn this paper, we introduce Curriculum as a new format of NLI benchmark for\nevaluation of broad-coverage linguistic phenomena. Curriculum contains a\ncollection of datasets that covers 36 types of major linguistic phenomena and\nan evaluation procedure for diagnosing how well a language model captures\nreasoning skills for distinct types of linguistic phenomena. We show that this\nlinguistic-phenomena-driven benchmark can serve as an effective tool for\ndiagnosing model behavior and verifying model learning quality. In addition,\nour experiments provide insight into the limitation of existing benchmark\ndatasets and state-of-the-art models that may encourage future research on\nre-designing datasets, model architectures, and learning objectives.\n", "first_author": "Zeming Chen", "year_month": "2022-04"}, {"title": "Leveraging Large Language Models to Generate Answer Set Programs", "published_date": "2023-07-15T03:40:55Z", "link": "http://arxiv.org/pdf/2307.07699v1", "abstract": "  Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated\nexceptional performance in various natural language processing tasks and have\nshown the ability to solve certain reasoning problems. However, their reasoning\ncapabilities are limited and relatively shallow, despite the application of\nvarious prompting techniques. In contrast, formal logic is adept at handling\ncomplex reasoning, but translating natural language descriptions into formal\nlogic is a challenging task that non-experts struggle with. This paper proposes\na neuro-symbolic method that combines the strengths of large language models\nand answer set programming. Specifically, we employ an LLM to transform natural\nlanguage descriptions of logic puzzles into answer set programs. We carefully\ndesign prompts for an LLM to convert natural language descriptions into answer\nset programs in a step by step manner. Surprisingly, with just a few in-context\nlearning examples, LLMs can generate reasonably complex answer set programs.\nThe majority of errors made are relatively simple and can be easily corrected\nby humans, thus enabling LLMs to effectively assist in the creation of answer\nset programs.\n", "first_author": "Adam Ishay", "year_month": "2023-07"}, {"title": "Large Language Models Are Implicitly Topic Models: Explaining and\n  Finding Good Demonstrations for In-Context Learning", "published_date": "2023-01-27T18:59:01Z", "link": "http://arxiv.org/pdf/2301.11916v2", "abstract": "  In recent years, pre-trained large language models have demonstrated\nremarkable efficiency in achieving an inference-time few-shot learning\ncapability known as in-context learning. However, existing literature has\nhighlighted the sensitivity of this capability to the selection of few-shot\ndemonstrations. The underlying mechanisms by which this capability arises from\nregular language model pretraining objectives remain poorly understood. In this\nstudy, we aim to examine the in-context learning phenomenon through a Bayesian\nlens, viewing large language models as topic models that implicitly infer\ntask-related information from demonstrations. On this premise, we propose an\nalgorithm for selecting optimal demonstrations from a set of annotated data and\ndemonstrate a significant 12.5% improvement relative to the random selection\nbaseline, averaged over eight GPT2 and GPT3 models on eight different\nreal-world text classification datasets. Our empirical findings support our\nhypothesis that large language models implicitly infer a latent concept\nvariable.\n", "first_author": "Xinyi Wang", "year_month": "2023-01"}, {"title": "Evaluating the Performance of Large Language Models on GAOKAO Benchmark", "published_date": "2023-05-21T14:39:28Z", "link": "http://arxiv.org/pdf/2305.12474v2", "abstract": "  Large language models have demonstrated remarkable performance across various\nnatural language processing tasks; however, their efficacy in more challenging\nand domain-specific tasks remains less explored. This paper introduces the\nGAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions\nfrom the Chinese Gaokao examination as test samples for evaluating large\nlanguage models.In order to align the evaluation results with humans as much as\npossible, we designed a method based on zero-shot prompts to analyze the\naccuracy and scoring rate of the model by dividing the questions into\nsubjective and objective types. We evaluated the ChatGPT model on\nGAOKAO-Benchmark performance.Our findings reveal that the ChatGPT model excels\nin tackling objective questions, while also shedding light on its shortcomings\nand areas for improvement. To further scrutinize the model's responses, we\nincorporate human evaluations.In conclusion, this research contributes a robust\nevaluation benchmark for future large-scale language models and offers valuable\ninsights into the limitations of such models.\n", "first_author": "Xiaotian Zhang", "year_month": "2023-05"}, {"title": "Domain Private Transformers", "published_date": "2023-05-23T16:27:12Z", "link": "http://arxiv.org/pdf/2305.14208v1", "abstract": "  Large, general purpose language models have demonstrated impressive\nperformance across many different conversational domains. While multi-domain\nlanguage models achieve low overall perplexity, their outputs are not\nguaranteed to stay within the domain of a given input prompt. This paper\nproposes domain privacy as a novel way to quantify how likely a conditional\nlanguage model will leak across domains. We also develop policy functions based\non token-level domain classification, and propose an efficient fine-tuning\nmethod to improve the trained model's domain privacy. Experiments on membership\ninference attacks show that our proposed method has comparable resiliency to\nmethods adapted from recent literature on differentially private language\nmodels.\n", "first_author": "Anmol Kabra", "year_month": "2023-05"}, {"title": "MorphPiece : Moving away from Statistical Language Representation", "published_date": "2023-07-14T10:35:04Z", "link": "http://arxiv.org/pdf/2307.07262v1", "abstract": "  Tokenization is a critical part of modern NLP pipelines. However,\ncontemporary tokenizers for Large Language Models are based on statistical\nanalysis of text corpora, without much consideration to the linguistic\nfeatures. We propose a linguistically motivated tokenization scheme,\nMorphPiece, which is based partly on morphological segmentation of the\nunderlying text. A GPT-style causal language model trained on this tokenizer\n(called MorphGPT) shows superior convergence compared to the same architecture\ntrained on a standard BPE tokenizer. Specifically we get Language Modeling\nperformance comparable to a 6 times larger model. Additionally, we evaluate\nMorphGPT on a variety of NLP tasks in supervised and unsupervised settings and\nfind superior performance across the board, compared to GPT-2 model.\n", "first_author": "Haris Jabbar", "year_month": "2023-07"}, {"title": "EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge\n  Distillation and Modal-adaptive Pruning", "published_date": "2022-10-14T13:26:41Z", "link": "http://arxiv.org/pdf/2210.07795v1", "abstract": "  Pre-trained vision-language models (VLMs) have achieved impressive results in\na range of vision-language tasks. However, popular VLMs usually consist of\nhundreds of millions of parameters which brings challenges for fine-tuning and\ndeployment in real-world applications due to space, memory, and latency\nconstraints. In this work, we introduce a distilling then pruning framework to\ncompress large vision-language models into smaller, faster, and more accurate\nones. We first shrink the size of a pre-trained large VLM and apply knowledge\ndistillation in the vision-language pre-training stage to obtain a\ntask-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm\nto automatically infer the importance of vision and language modalities for\ndifferent downstream tasks and adaptively remove redundant structures and\nneurons in different encoders with controllable target sparsity. We apply our\nframework to train EfficientVLM, a fast and accurate vision-language model\nconsisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers,\naccounting for only 93 million parameters in total, which is 44.3% of the\nteacher model. EfficientVLM retains 98.4% performance of the teacher model and\naccelerates its inference speed by 2.2x. EfficientVLM achieves a large absolute\nimprovement over previous SoTA efficient VLMs of similar sizes by a large\nmargin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2\n(+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation\n(CIDEr +6.5), demonstrating a large potential on training lightweight VLMs.\n", "first_author": "Tiannan Wang", "year_month": "2022-10"}, {"title": "Universal Phone Recognition with a Multilingual Allophone System", "published_date": "2020-02-26T21:28:57Z", "link": "http://arxiv.org/pdf/2002.11800v1", "abstract": "  Multilingual models can improve language processing, particularly for low\nresource situations, by sharing parameters across languages. Multilingual\nacoustic models, however, generally ignore the difference between phonemes\n(sounds that can support lexical contrasts in a particular language) and their\ncorresponding phones (the sounds that are actually spoken, which are language\nindependent). This can lead to performance degradation when combining a variety\nof training languages, as identically annotated phonemes can actually\ncorrespond to several different underlying phonetic realizations. In this work,\nwe propose a joint model of both language-independent phone and\nlanguage-dependent phoneme distributions. In multilingual ASR experiments over\n11 languages, we find that this model improves testing performance by 2%\nphoneme error rate absolute in low-resource conditions. Additionally, because\nwe are explicitly modeling language-independent phones, we can build a\n(nearly-)universal phone recognizer that, when combined with the PHOIBLE large,\nmanually curated database of phone inventories, can be customized into 2,000\nlanguage dependent recognizers. Experiments on two low-resourced indigenous\nlanguages, Inuktitut and Tusom, show that our recognizer achieves phone\naccuracy improvements of more than 17%, moving a step closer to speech\nrecognition for all languages in the world.\n", "first_author": "Xinjian Li", "year_month": "2020-02"}, {"title": "Comparison of Interactive Knowledge Base Spelling Correction Models for\n  Low-Resource Languages", "published_date": "2020-10-20T17:31:07Z", "link": "http://arxiv.org/pdf/2010.10472v1", "abstract": "  Spelling normalization for low resource languages is a challenging task\nbecause the patterns are hard to predict and large corpora are usually required\nto collect enough examples. This work shows a comparison of a neural model and\ncharacter language models with varying amounts on target language data. Our\nusage scenario is interactive correction with nearly zero amounts of training\nexamples, improving models as more data is collected, for example within a chat\napp. Such models are designed to be incrementally improved as feedback is given\nfrom users. In this work, we design a knowledge-base and prediction model\nembedded system for spelling correction in low-resource languages. Experimental\nresults on multiple languages show that the model could become effective with a\nsmall amount of data. We perform experiments on both natural and synthetic\ndata, as well as on data from two endangered languages (Ainu and Griko). Last,\nwe built a prototype system that was used for a small case study on Hinglish,\nwhich further demonstrated the suitability of our approach in real world\nscenarios.\n", "first_author": "Yiyuan Li", "year_month": "2020-10"}, {"title": "Few-Shot Keyword Spotting in Any Language", "published_date": "2021-04-03T17:27:37Z", "link": "http://arxiv.org/pdf/2104.01454v4", "abstract": "  We introduce a few-shot transfer learning method for keyword spotting in any\nlanguage. Leveraging open speech corpora in nine languages, we automate the\nextraction of a large multilingual keyword bank and use it to train an\nembedding model. With just five training examples, we fine-tune the embedding\nmodel for keyword spotting and achieve an average F1 score of 0.75 on keyword\nclassification for 180 new keywords unseen by the embedding model in these nine\nlanguages. This embedding model also generalizes to new languages. We achieve\nan average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13\nnew languages unseen by the embedding model. We investigate streaming accuracy\nfor our 5-shot models in two contexts: keyword spotting and keyword search.\nAcross 440 keywords in 22 languages, we achieve an average streaming keyword\nspotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe\npromising initial results on keyword search.\n", "first_author": "Mark Mazumder", "year_month": "2021-04"}, {"title": "Improving Biomedical Pretrained Language Models with Knowledge", "published_date": "2021-04-21T03:57:26Z", "link": "http://arxiv.org/pdf/2104.10344v1", "abstract": "  Pretrained language models have shown success in many natural language\nprocessing tasks. Many works explore incorporating knowledge into language\nmodels. In the biomedical domain, experts have taken decades of effort on\nbuilding large-scale knowledge bases. For example, the Unified Medical Language\nSystem (UMLS) contains millions of entities with their synonyms and defines\nhundreds of relations among entities. Leveraging this knowledge can benefit a\nvariety of downstream tasks such as named entity recognition and relation\nextraction. To this end, we propose KeBioLM, a biomedical pretrained language\nmodel that explicitly leverages knowledge from the UMLS knowledge bases.\nSpecifically, we extract entities from PubMed abstracts and link them to UMLS.\nWe then train a knowledge-aware language model that firstly applies a text-only\nencoding layer to learn entity representation and applies a text-entity fusion\nencoding to aggregate entity representation. Besides, we add two training\nobjectives as entity detection and entity linking. Experiments on the named\nentity recognition and relation extraction from the BLURB benchmark demonstrate\nthe effectiveness of our approach. Further analysis on a collected probing\ndataset shows that our model has better ability to model medical knowledge.\n", "first_author": "Zheng Yuan", "year_month": "2021-04"}, {"title": "IndicBART: A Pre-trained Model for Indic Natural Language Generation", "published_date": "2021-09-07T07:08:33Z", "link": "http://arxiv.org/pdf/2109.02903v2", "abstract": "  In this paper, we study pre-trained sequence-to-sequence models for a group\nof related languages, with a focus on Indic languages. We present IndicBART, a\nmultilingual, sequence-to-sequence pre-trained model focusing on 11 Indic\nlanguages and English. IndicBART utilizes the orthographic similarity between\nIndic scripts to improve transfer learning between similar Indic languages. We\nevaluate IndicBART on two NLG tasks: Neural Machine Translation (NMT) and\nextreme summarization. Our experiments on NMT and extreme summarization show\nthat a model specific to related languages like IndicBART is competitive with\nlarge pre-trained models like mBART50 despite being significantly smaller. It\nalso performs well on very low-resource translation scenarios where languages\nare not included in pre-training or fine-tuning. Script sharing, multilingual\ntraining, and better utilization of limited model capacity contribute to the\ngood performance of the compact IndicBART model.\n", "first_author": "Raj Dabre", "year_month": "2021-09"}, {"title": "Data-Efficient Learning of Natural Language to Linear Temporal Logic\n  Translators for Robot Task Specification", "published_date": "2023-03-09T00:09:58Z", "link": "http://arxiv.org/pdf/2303.08006v2", "abstract": "  To make robots accessible to a broad audience, it is critical to endow them\nwith the ability to take universal modes of communication, like commands given\nin natural language, and extract a concrete desired task specification, defined\nusing a formal language like linear temporal logic (LTL). In this paper, we\npresent a learning-based approach for translating from natural language\ncommands to LTL specifications with very limited human-labeled training data.\nThis is in stark contrast to existing natural-language to LTL translators,\nwhich require large human-labeled datasets, often in the form of labeled pairs\nof LTL formulas and natural language commands, to train the translator. To\nreduce reliance on human data, our approach generates a large synthetic\ntraining dataset through algorithmic generation of LTL formulas, conversion to\nstructured English, and then exploiting the paraphrasing capabilities of modern\nlarge language models (LLMs) to synthesize a diverse corpus of natural language\ncommands corresponding to the LTL formulas. We use this generated data to\nfinetune an LLM and apply a constrained decoding procedure at inference time to\nensure the returned LTL formula is syntactically correct. We evaluate our\napproach on three existing LTL/natural language datasets and show that we can\ntranslate natural language commands at 75\\% accuracy with far less human data\n($\\le$12 annotations). Moreover, when training on large human-annotated\ndatasets, our method achieves higher test accuracy (95\\% on average) than prior\nwork. Finally, we show the translated formulas can be used to plan\nlong-horizon, multi-stage tasks on a 12D quadrotor.\n", "first_author": "Jiayi Pan", "year_month": "2023-03"}, {"title": "Knowledge Distillation of Large Language Models", "published_date": "2023-06-14T14:44:03Z", "link": "http://arxiv.org/pdf/2306.08543v1", "abstract": "  Knowledge Distillation (KD) is a promising technique for reducing the high\ncomputational demand of large language models (LLMs). However, previous KD\nmethods are primarily applied to white-box classification models or training\nsmall models to imitate black-box model APIs like ChatGPT. How to effectively\ndistill the knowledge from white-box generative LLMs is still under-explored,\nwhich becomes more and more important with the prosperity of LLMs. In this\nwork, we propose MiniLLM that distills smaller language models from generative\nlarger language models. We first replace the forward Kullback-Leibler\ndivergence (KLD) objective in the standard KD approaches with reverse KLD,\nwhich is more suitable for KD on generative language models, to prevent the\nstudent model from overestimating the low-probability regions of the teacher\ndistribution. Then, we derive an effective optimization approach to learn this\nobjective. Extensive experiments in the instruction-following setting show that\nthe MiniLLM models generate more precise responses with the higher overall\nquality, lower exposure bias, better calibration, and higher long-text\ngeneration performance. Our method is also scalable for different model\nfamilies with 120M to 13B parameters. We will release our code and model\ncheckpoints at https://aka.ms/MiniLLM.\n", "first_author": "Yuxian Gu", "year_month": "2023-06"}, {"title": "Recent Advances in Natural Language Processing via Large Pre-Trained\n  Language Models: A Survey", "published_date": "2021-11-01T20:08:05Z", "link": "http://arxiv.org/pdf/2111.01243v1", "abstract": "  Large, pre-trained transformer-based language models such as BERT have\ndrastically changed the Natural Language Processing (NLP) field. We present a\nsurvey of recent work that uses these large language models to solve NLP tasks\nvia pre-training then fine-tuning, prompting, or text generation approaches. We\nalso present approaches that use pre-trained language models to generate data\nfor training augmentation or other purposes. We conclude with discussions on\nlimitations and suggested directions for future research.\n", "first_author": "Bonan Min", "year_month": "2021-11"}, {"title": "Aksharantar: Towards building open transliteration tools for the next\n  billion users", "published_date": "2022-05-06T05:13:12Z", "link": "http://arxiv.org/pdf/2205.03018v1", "abstract": "  We introduce Aksharantar, the largest publicly available transliteration\ndataset for 21 Indic languages containing 26 million transliteration pairs. We\nbuild this dataset by mining transliteration pairs from large monolingual and\nparallel corpora, as well as collecting transliterations from human annotators\nto ensure diversity of words and representation of low-resource languages. We\nintroduce a new, large, diverse testset for Indic language transliteration\ncontaining 103k words pairs spanning 19 languages that enables fine-grained\nanalysis of transliteration models.\n  We train the IndicXlit model on the Aksharantar training set. IndicXlit is a\nsingle transformer-based multilingual transliteration model for roman to Indic\nscript conversion supporting 21 Indic languages. It achieves state-of-the art\nresults on the Dakshina testset, and establishes strong baselines on the\nAksharantar testset released along with this work.\n  All the datasets and models are publicly available at\nhttps://indicnlp.ai4bharat.org/aksharantar. We hope the availability of these\nlarge-scale, open resources will spur innovation for Indic language\ntransliteration and downstream applications.\n", "first_author": "Yash Madhani", "year_month": "2022-05"}, {"title": "Alternating Recurrent Dialog Model with Large-scale Pre-trained Language\n  Models", "published_date": "2019-10-09T02:31:37Z", "link": "http://arxiv.org/pdf/1910.03756v3", "abstract": "  Existing dialog system models require extensive human annotations and are\ndifficult to generalize to different tasks. The recent success of large\npre-trained language models such as BERT and GPT-2 (Devlin et al., 2019;\nRadford et al., 2019) have suggested the effectiveness of incorporating\nlanguage priors in down-stream NLP tasks. However, how much pre-trained\nlanguage models can help dialog response generation is still under exploration.\nIn this paper, we propose a simple, general, and effective framework:\nAlternating Roles Dialog Model (ARDM). ARDM models each speaker separately and\ntakes advantage of the large pre-trained language model. It requires no\nsupervision from human annotations such as belief states or dialog acts to\nachieve effective conversations. ARDM outperforms or is on par with\nstate-of-the-art methods on two popular task-oriented dialog datasets:\nCamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging,\nnon-collaborative tasks such as persuasion. In persuasion tasks, ARDM is\ncapable of generating human-like responses to persuade people to donate to a\ncharity.\n", "first_author": "Qingyang Wu", "year_month": "2019-10"}, {"title": "Extracting Training Data from Large Language Models", "published_date": "2020-12-14T18:39:09Z", "link": "http://arxiv.org/pdf/2012.07805v2", "abstract": "  It has become common to publish large (billion parameter) language models\nthat have been trained on private datasets. This paper demonstrates that in\nsuch settings, an adversary can perform a training data extraction attack to\nrecover individual training examples by querying the language model.\n  We demonstrate our attack on GPT-2, a language model trained on scrapes of\nthe public Internet, and are able to extract hundreds of verbatim text\nsequences from the model's training data. These extracted examples include\n(public) personally identifiable information (names, phone numbers, and email\naddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible\neven though each of the above sequences are included in just one document in\nthe training data.\n  We comprehensively evaluate our extraction attack to understand the factors\nthat contribute to its success. Worryingly, we find that larger models are more\nvulnerable than smaller models. We conclude by drawing lessons and discussing\npossible safeguards for training large language models.\n", "first_author": "Nicholas Carlini", "year_month": "2020-12"}, {"title": "Data Selection for Fine-tuning Large Language Models Using Transferred\n  Shapley Values", "published_date": "2023-06-16T20:07:38Z", "link": "http://arxiv.org/pdf/2306.10165v1", "abstract": "  Although Shapley values have been shown to be highly effective for\nidentifying harmful training instances, dataset size and model complexity\nconstraints limit the ability to apply Shapley-based data valuation to\nfine-tuning large pre-trained language models. To address this, we propose\nTS-DShapley, an algorithm that reduces computational cost of Shapley-based data\nvaluation through: 1) an efficient sampling-based method that aggregates\nShapley values computed from subsets for valuation of the entire training set,\nand 2) a value transfer method that leverages value information extracted from\na simple classifier trained using representations from the target language\nmodel. Our experiments applying TS-DShapley to select data for fine-tuning\nBERT-based language models on benchmark natural language understanding (NLU)\ndatasets show that TS-DShapley outperforms existing data selection methods.\nFurther, TS-DShapley can filter fine-tuning data to increase language model\nperformance compared to training with the full fine-tuning dataset.\n", "first_author": "Stephanie Schoch", "year_month": "2023-06"}, {"title": "Large Multimodal Models: Notes on CVPR 2023 Tutorial", "published_date": "2023-06-26T17:59:31Z", "link": "http://arxiv.org/pdf/2306.14895v1", "abstract": "  This tutorial note summarizes the presentation on ``Large Multimodal Models:\nTowards Building and Surpassing Multimodal GPT-4'', a part of CVPR 2023\ntutorial on ``Recent Advances in Vision Foundation Models''. The tutorial\nconsists of three parts. We first introduce the background on recent GPT-like\nlarge models for vision-and-language modeling to motivate the research in\ninstruction-tuned large multimodal models (LMMs). As a pre-requisite, we\ndescribe the basics of instruction-tuning in large language models, which is\nfurther extended to the multimodal space. Lastly, we illustrate how to build\nthe minimum prototype of multimodal GPT-4 like models with the open-source\nresource, and review the recently emerged topics.\n", "first_author": "Chunyuan Li", "year_month": "2023-06"}, {"title": "Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems", "published_date": "2020-08-14T08:23:21Z", "link": "http://arxiv.org/pdf/2008.06239v2", "abstract": "  Task-oriented dialogue systems use four connected modules, namely, Natural\nLanguage Understanding (NLU), a Dialogue State Tracking (DST), Dialogue Policy\n(DP) and Natural Language Generation (NLG). A research challenge is to learn\neach module with the least amount of samples (i.e., few-shots) given the high\ncost related to the data collection. The most common and effective technique to\nsolve this problem is transfer learning, where large language models, either\npre-trained on text or task-specific data, are fine-tuned on the few samples.\nThese methods require fine-tuning steps and a set of parameters for each task.\nDifferently, language models, such as GPT-2 (Radford et al., 2019) and GPT-3\n(Brown et al., 2020), allow few-shot learning by priming the model with few\nexamples. In this paper, we evaluate the priming few-shot ability of language\nmodels in the NLU, DST, DP and NLG tasks. Importantly, we highlight the current\nlimitations of this approach, and we discuss the possible implication for\nfuture work.\n", "first_author": "Andrea Madotto", "year_month": "2020-08"}, {"title": "A Masked Segmental Language Model for Unsupervised Natural Language\n  Segmentation", "published_date": "2021-04-16T00:00:05Z", "link": "http://arxiv.org/pdf/2104.07829v2", "abstract": "  Segmentation remains an important preprocessing step both in languages where\n\"words\" or other important syntactic/semantic units (like morphemes) are not\nclearly delineated by white space, as well as when dealing with continuous\nspeech data, where there is often no meaningful pause between words.\nNear-perfect supervised methods have been developed for use in resource-rich\nlanguages such as Chinese, but many of the world's languages are both\nmorphologically complex, and have no large dataset of \"gold\" segmentations into\nmeaningful units. To solve this problem, we propose a new type of Segmental\nLanguage Model (Sun and Deng, 2018; Kawakami et al., 2019; Wang et al., 2021)\nfor use in both unsupervised and lightly supervised segmentation tasks. We\nintroduce a Masked Segmental Language Model (MSLM) built on a span-masking\ntransformer architecture, harnessing the power of a bi-directional masked\nmodeling context and attention. In a series of experiments, our model\nconsistently outperforms Recurrent SLMs on Chinese (PKU Corpus) in segmentation\nquality, and performs similarly to the Recurrent model on English (PTB). We\nconclude by discussing the different challenges posed in segmenting\nphonemic-type writing systems.\n", "first_author": "C. M. Downey", "year_month": "2021-04"}, {"title": "Do Multilingual Language Models Capture Differing Moral Norms?", "published_date": "2022-03-18T12:26:37Z", "link": "http://arxiv.org/pdf/2203.09904v1", "abstract": "  Massively multilingual sentence representations are trained on large corpora\nof uncurated data, with a very imbalanced proportion of languages included in\nthe training. This may cause the models to grasp cultural values including\nmoral judgments from the high-resource languages and impose them on the\nlow-resource languages. The lack of data in certain languages can also lead to\ndeveloping random and thus potentially harmful beliefs. Both these issues can\nnegatively influence zero-shot cross-lingual model transfer and potentially\nlead to harmful outcomes. Therefore, we aim to (1) detect and quantify these\nissues by comparing different models in different languages, (2) develop\nmethods for improving undesirable properties of the models. Our initial\nexperiments using the multilingual model XLM-R show that indeed multilingual\nLMs capture moral norms, even with potentially higher human-agreement than\nmonolingual ones. However, it is not yet clear to what extent these moral norms\ndiffer between languages.\n", "first_author": "Katharina H\u00e4mmerl", "year_month": "2022-03"}, {"title": "Context-Aware Language Modeling for Goal-Oriented Dialogue Systems", "published_date": "2022-04-18T17:23:11Z", "link": "http://arxiv.org/pdf/2204.10198v2", "abstract": "  Goal-oriented dialogue systems face a trade-off between fluent language\ngeneration and task-specific control. While supervised learning with large\nlanguage models is capable of producing realistic text, how to steer such\nresponses towards completing a specific task without sacrificing language\nquality remains an open question. In this work, we formulate goal-oriented\ndialogue as a partially observed Markov decision process, interpreting the\nlanguage model as a representation of both the dynamics and the policy. This\nview allows us to extend techniques from learning-based control, such as task\nrelabeling, to derive a simple and effective method to finetune language models\nin a goal-aware way, leading to significantly improved task performance. We\nadditionally introduce a number of training strategies that serve to better\nfocus the model on the task at hand. We evaluate our method, Context-Aware\nLanguage Models (CALM), on a practical flight-booking task using AirDialogue.\nEmpirically, CALM outperforms the state-of-the-art method by 7% in terms of\ntask success, matching human-level task performance.\n", "first_author": "Charlie Snell", "year_month": "2022-04"}, {"title": "Human Language Modeling", "published_date": "2022-05-10T19:11:12Z", "link": "http://arxiv.org/pdf/2205.05128v1", "abstract": "  Natural language is generated by people, yet traditional language modeling\nviews words or documents as if generated independently. Here, we propose human\nlanguage modeling (HuLM), a hierarchical extension to the language modeling\nproblem whereby a human-level exists to connect sequences of documents (e.g.\nsocial media messages) and capture the notion that human language is moderated\nby changing human states. We introduce, HaRT, a large-scale transformer model\nfor the HuLM task, pre-trained on approximately 100,000 social media users, and\ndemonstrate its effectiveness in terms of both language modeling (perplexity)\nfor social media and fine-tuning for 4 downstream tasks spanning document- and\nuser-levels: stance detection, sentiment classification, age estimation, and\npersonality assessment. Results on all tasks meet or surpass the current\nstate-of-the-art.\n", "first_author": "Nikita Soni", "year_month": "2022-05"}, {"title": "Few-shot Subgoal Planning with Language Models", "published_date": "2022-05-28T01:03:30Z", "link": "http://arxiv.org/pdf/2205.14288v1", "abstract": "  Pre-trained large language models have shown successful progress in many\nlanguage understanding benchmarks. This work explores the capability of these\nmodels to predict actionable plans in real-world environments. Given a text\ninstruction, we show that language priors encoded in pre-trained language\nmodels allow us to infer fine-grained subgoal sequences. In contrast to recent\nmethods which make strong assumptions about subgoal supervision, our\nexperiments show that language models can infer detailed subgoal sequences from\nfew training sequences without any fine-tuning. We further propose a simple\nstrategy to re-rank language model predictions based on interaction and\nfeedback from the environment. Combined with pre-trained navigation and visual\nreasoning components, our approach demonstrates competitive performance on\nsubgoal prediction and task completion in the ALFRED benchmark compared to\nprior methods that assume more subgoal supervision.\n", "first_author": "Lajanugen Logeswaran", "year_month": "2022-05"}, {"title": "On the application of Large Language Models for language teaching and\n  assessment technology", "published_date": "2023-07-17T11:12:56Z", "link": "http://arxiv.org/pdf/2307.08393v1", "abstract": "  The recent release of very large language models such as PaLM and GPT-4 has\nmade an unprecedented impact in the popular media and public consciousness,\ngiving rise to a mixture of excitement and fear as to their capabilities and\npotential uses, and shining a light on natural language processing research\nwhich had not previously received so much attention. The developments offer\ngreat promise for education technology, and in this paper we look specifically\nat the potential for incorporating large language models in AI-driven language\nteaching and assessment systems. We consider several research areas and also\ndiscuss the risks and ethical considerations surrounding generative AI in\neducation technology for language learners. Overall we find that larger\nlanguage models offer improvements over previous models in text generation,\nopening up routes toward content generation which had not previously been\nplausible. For text generation they must be prompted carefully and their\noutputs may need to be reshaped before they are ready for use. For automated\ngrading and grammatical error correction, tasks whose progress is checked on\nwell-known benchmarks, early investigations indicate that large language models\non their own do not improve on state-of-the-art results according to standard\nevaluation metrics. For grading it appears that linguistic features established\nin the literature should still be used for best performance, and for error\ncorrection it may be that the models can offer alternative feedback styles\nwhich are not measured sensitively with existing methods. In all cases, there\nis work to be done to experiment with the inclusion of large language models in\neducation technology for language learners, in order to properly understand and\nreport on their capacities and limitations, and to ensure that foreseeable\nrisks such as misinformation and harmful bias are mitigated.\n", "first_author": "Andrew Caines", "year_month": "2023-07"}, {"title": "Language Quantized AutoEncoders: Towards Unsupervised Text-Image\n  Alignment", "published_date": "2023-02-02T06:38:44Z", "link": "http://arxiv.org/pdf/2302.00902v2", "abstract": "  Recent progress in scaling up large language models has shown impressive\ncapabilities in performing few-shot learning across a wide range of text-based\ntasks. However, a key limitation is that these language models fundamentally\nlack visual perception - a crucial attribute needed to extend these models to\nbe able to interact with the real world and solve vision tasks, such as in\nvisual-question answering and robotics. Prior works have largely connected\nimage to text through pretraining and/or fine-tuning on curated image-text\ndatasets, which can be a costly and expensive process. In order to resolve this\nlimitation, we propose a simple yet effective approach called\nLanguage-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to\nalign text-image data in an unsupervised manner by leveraging pretrained\nlanguage models (e.g., BERT, RoBERTa). Our main idea is to encode image as\nsequences of text tokens by directly quantizing image embeddings using a\npretrained language codebook. We then apply random masking followed by a BERT\nmodel, and have the decoder reconstruct the original image from BERT predicted\ntext token embeddings. By doing so, LQAE learns to represent similar images\nwith similar clusters of text tokens, thereby aligning these two modalities\nwithout the use of aligned text-image pairs. This enables few-shot image\nclassification with large language models (e.g., GPT-3) as well as linear\nclassification of images based on BERT text features. To the best of our\nknowledge, our work is the first work that uses unaligned images for multimodal\ntasks by leveraging the power of pretrained language models.\n", "first_author": "Hao Liu", "year_month": "2023-02"}, {"title": "ALERT: Adapting Language Models to Reasoning Tasks", "published_date": "2022-12-16T05:15:41Z", "link": "http://arxiv.org/pdf/2212.08286v2", "abstract": "  Current large language models can perform reasonably well on complex tasks\nthat require step-by-step reasoning with few-shot learning. Are these models\napplying reasoning skills they have learnt during pre-training and reason\noutside of their training context, or are they simply memorizing their training\ncorpus at finer granularity and have learnt to better understand their context?\nTo tease apart these possibilities, we introduce ALERT, a benchmark and suite\nof analyses for assessing language models' reasoning ability comparing\npre-trained and finetuned models on complex tasks that require reasoning skills\nto solve. ALERT provides a test bed to asses any language model on fine-grained\nreasoning skills, which spans over 20 datasets and covers 10 different\nreasoning skills. We leverage ALERT to further investigate the role of\nfinetuning. With extensive empirical analysis we find that language models\nlearn more reasoning skills such as textual entailment, abductive reasoning,\nand analogical reasoning during finetuning stage compared to pretraining state.\nWe also find that when language models are finetuned they tend to overfit to\nthe prompt template, which hurts the robustness of models causing\ngeneralization problems.\n", "first_author": "Ping Yu", "year_month": "2022-12"}, {"title": "Uncovering Constraint-Based Behavior in Neural Models via Targeted\n  Fine-Tuning", "published_date": "2021-06-02T14:52:11Z", "link": "http://arxiv.org/pdf/2106.01207v1", "abstract": "  A growing body of literature has focused on detailing the linguistic\nknowledge embedded in large, pretrained language models. Existing work has\nshown that non-linguistic biases in models can drive model behavior away from\nlinguistic generalizations. We hypothesized that competing linguistic processes\nwithin a language, rather than just non-linguistic model biases, could obscure\nunderlying linguistic knowledge. We tested this claim by exploring a single\nphenomenon in four languages: English, Chinese, Spanish, and Italian. While\nhuman behavior has been found to be similar across languages, we find\ncross-linguistic variation in model behavior. We show that competing processes\nin a language act as constraints on model behavior and demonstrate that\ntargeted fine-tuning can re-weight the learned constraints, uncovering\notherwise dormant linguistic knowledge in models. Our results suggest that\nmodels need to learn both the linguistic constraints in a language and their\nrelative ranking, with mismatches in either producing non-human-like behavior.\n", "first_author": "Forrest Davis", "year_month": "2021-06"}, {"title": "MDIA: A Benchmark for Multilingual Dialogue Generation in 46 Languages", "published_date": "2022-08-27T19:35:20Z", "link": "http://arxiv.org/pdf/2208.13078v1", "abstract": "  Owing to the lack of corpora for low-resource languages, current works on\ndialogue generation have mainly focused on English. In this paper, we present\nmDIA, the first large-scale multilingual benchmark for dialogue generation\nacross low- to high-resource languages. It covers real-life conversations in 46\nlanguages across 19 language families. We present baseline results obtained by\nfine-tuning the multilingual, non-dialogue-focused pre-trained model mT5 as\nwell as English-centric, dialogue-focused pre-trained chatbot DialoGPT. The\nresults show that mT5-based models perform better on sacreBLEU and BertScore\nbut worse on diversity. Even though promising results are found in few-shot and\nzero-shot scenarios, there is a large gap between the generation quality in\nEnglish and other languages. We hope that the release of mDIA could encourage\nmore works on multilingual dialogue generation to promote language diversity.\n", "first_author": "Qingyu Zhang", "year_month": "2022-08"}, {"title": "Low-resource Bilingual Dialect Lexicon Induction with Large Language\n  Models", "published_date": "2023-04-19T20:20:41Z", "link": "http://arxiv.org/pdf/2304.09957v1", "abstract": "  Bilingual word lexicons are crucial tools for multilingual natural language\nunderstanding and machine translation tasks, as they facilitate the mapping of\nwords in one language to their synonyms in another language. To achieve this,\nnumerous papers have explored bilingual lexicon induction (BLI) in\nhigh-resource scenarios, using a typical pipeline consisting of two\nunsupervised steps: bitext mining and word alignment, both of which rely on\npre-trained large language models~(LLMs).\n  In this paper, we present an analysis of the BLI pipeline for German and two\nof its dialects, Bavarian and Alemannic. This setup poses several unique\nchallenges, including the scarcity of resources, the relatedness of the\nlanguages, and the lack of standardization in the orthography of dialects. To\nevaluate the BLI outputs, we analyze them with respect to word frequency and\npairwise edit distance. Additionally, we release two evaluation datasets\ncomprising 1,500 bilingual sentence pairs and 1,000 bilingual word pairs. They\nwere manually judged for their semantic similarity for each Bavarian-German and\nAlemannic-German language pair.\n", "first_author": "Ekaterina Artemova", "year_month": "2023-04"}, {"title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language\n  Models", "published_date": "2023-04-07T17:14:00Z", "link": "http://arxiv.org/pdf/2304.03738v2", "abstract": "  As the capabilities of generative language models continue to advance, the\nimplications of biases ingrained within these models have garnered increasing\nattention from researchers, practitioners, and the broader public. This article\ninvestigates the challenges and risks associated with biases in large-scale\nlanguage models like ChatGPT. We discuss the origins of biases, stemming from,\namong others, the nature of training data, model specifications, algorithmic\nconstraints, product design, and policy decisions. We explore the ethical\nconcerns arising from the unintended consequences of biased model outputs. We\nfurther analyze the potential opportunities to mitigate biases, the\ninevitability of some biases, and the implications of deploying these models in\nvarious applications, such as virtual assistants, content generation, and\nchatbots. Finally, we review the current approaches to identify, quantify, and\nmitigate biases in language models, emphasizing the need for a\nmulti-disciplinary, collaborative effort to develop more equitable,\ntransparent, and responsible AI systems. This article aims to stimulate a\nthoughtful dialogue within the artificial intelligence community, encouraging\nresearchers and developers to reflect on the role of biases in generative\nlanguage models and the ongoing pursuit of ethical AI.\n", "first_author": "Emilio Ferrara", "year_month": "2023-04"}, {"title": "Understanding Chinese Video and Language via Contrastive Multimodal\n  Pre-Training", "published_date": "2021-04-19T15:58:45Z", "link": "http://arxiv.org/pdf/2104.09411v1", "abstract": "  The pre-trained neural models have recently achieved impressive performances\nin understanding multimodal content. However, it is still very challenging to\npre-train neural models for video and language understanding, especially for\nChinese video-language data, due to the following reasons. Firstly, existing\nvideo-language pre-training algorithms mainly focus on the co-occurrence of\nwords and video frames, but ignore other valuable semantic and structure\ninformation of video-language content, e.g., sequential order and\nspatiotemporal relationships. Secondly, there exist conflicts between video\nsentence alignment and other proxy tasks. Thirdly, there is a lack of\nlarge-scale and high-quality Chinese video-language datasets (e.g., including\n10 million unique videos), which are the fundamental success conditions for\npre-training techniques.\n  In this work, we propose a novel video-language understanding framework named\nVICTOR, which stands for VIdeo-language understanding via Contrastive\nmulTimOdal pRe-training. Besides general proxy tasks such as masked language\nmodeling, VICTOR constructs several novel proxy tasks under the contrastive\nlearning paradigm, making the model be more robust and able to capture more\ncomplex multimodal semantic and structural relationships from different\nperspectives. VICTOR is trained on a large-scale Chinese video-language\ndataset, including over 10 million complete videos with corresponding\nhigh-quality textual descriptions. We apply the pre-trained VICTOR model to a\nseries of downstream applications and demonstrate its superior performances,\ncomparing against the state-of-the-art pre-training methods such as VideoBERT\nand UniVL. The codes and trained checkpoints will be publicly available to\nnourish further developments of the research community.\n", "first_author": "Chenyi Lei", "year_month": "2021-04"}, {"title": "Cross-Domain Deep Code Search with Few-Shot Meta Learning", "published_date": "2022-01-01T09:00:48Z", "link": "http://arxiv.org/pdf/2201.00150v5", "abstract": "  Recently, pre-trained programming language models such as CodeBERT have\ndemonstrated substantial gains in code search. Despite showing great\nperformance, they rely on the availability of large amounts of parallel data to\nfine-tune the semantic mappings between queries and code. This restricts their\npracticality in domain-specific languages with relatively scarce and expensive\ndata. In this paper, we propose CDCS, a novel approach for domain-specific code\nsearch. CDCS employs a transfer learning framework where an initial program\nrepresentation model is pre-trained on a large corpus of common programming\nlanguages (such as Java and Python), and is further adapted to domain-specific\nlanguages such as SQL and Solidity. Unlike cross-language CodeBERT, which is\ndirectly fine-tuned in the target language, CDCS adapts a few-shot\nmeta-learning algorithm called MAML to learn the good initialization of model\nparameters, which can be best reused in a domain-specific language. We evaluate\nthe proposed approach on two domain-specific languages, namely, SQL and\nSolidity, with model transferred from two widely used languages (Python and\nJava). Experimental results show that CDCS significantly outperforms\nconventional pre-trained code models that are directly fine-tuned in\ndomain-specific languages, and it is particularly effective for scarce data.\n", "first_author": "Yitian Chai", "year_month": "2022-01"}, {"title": "Sensitivity and Robustness of Large Language Models to Prompt Template\n  in Japanese Text Classification Tasks", "published_date": "2023-05-15T15:19:08Z", "link": "http://arxiv.org/pdf/2305.08714v2", "abstract": "  Prompt engineering relevance research has seen a notable surge in recent\nyears, primarily driven by advancements in pre-trained language models and\nlarge language models. However, a critical issue has been identified within\nthis domain: the inadequate of sensitivity and robustness of these models\ntowards Prompt Templates, particularly in lesser-studied languages such as\nJapanese. This paper explores this issue through a comprehensive evaluation of\nseveral representative Large Language Models (LLMs) and a widely-utilized\npre-trained model(PLM). These models are scrutinized using a benchmark dataset\nin Japanese, with the aim to assess and analyze the performance of the current\nmultilingual models in this context. Our experimental results reveal startling\ndiscrepancies. A simple modification in the sentence structure of the Prompt\nTemplate led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44.\nThis observation underscores the fact that even the highly performance GPT-4\nmodel encounters significant stability issues when dealing with diverse\nJapanese prompt templates, rendering the consistency of the model's output\nresults questionable. In light of these findings, we conclude by proposing\npotential research trajectories to further enhance the development and\nperformance of Large Language Models in their current stage.\n", "first_author": "Chengguang Gan", "year_month": "2023-05"}, {"title": "Compositional Languages Emerge in a Neural Iterated Learning Model", "published_date": "2020-02-04T15:19:09Z", "link": "http://arxiv.org/pdf/2002.01365v2", "abstract": "  The principle of compositionality, which enables natural language to\nrepresent complex concepts via a structured combination of simpler ones, allows\nus to convey an open-ended set of messages using a limited vocabulary. If\ncompositionality is indeed a natural property of language, we may expect it to\nappear in communication protocols that are created by neural agents in language\ngames. In this paper, we propose an effective neural iterated learning (NIL)\nalgorithm that, when applied to interacting neural agents, facilitates the\nemergence of a more structured type of language. Indeed, these languages\nprovide learning speed advantages to neural agents during training, which can\nbe incrementally amplified via NIL. We provide a probabilistic model of NIL and\nan explanation of why the advantage of compositional language exist. Our\nexperiments confirm our analysis, and also demonstrate that the emerged\nlanguages largely improve the generalizing power of the neural agent\ncommunication.\n", "first_author": "Yi Ren", "year_month": "2020-02"}, {"title": "From Language to Language-ish: How Brain-Like is an LSTM's\n  Representation of Nonsensical Language Stimuli?", "published_date": "2020-10-14T23:26:28Z", "link": "http://arxiv.org/pdf/2010.07435v1", "abstract": "  The representations generated by many models of language (word embeddings,\nrecurrent neural networks and transformers) correlate to brain activity\nrecorded while people read. However, these decoding results are usually based\non the brain's reaction to syntactically and semantically sound language\nstimuli. In this study, we asked: how does an LSTM (long short term memory)\nlanguage model, trained (by and large) on semantically and syntactically intact\nlanguage, represent a language sample with degraded semantic or syntactic\ninformation? Does the LSTM representation still resemble the brain's reaction?\nWe found that, even for some kinds of nonsensical language, there is a\nstatistically significant relationship between the brain's activity and the\nrepresentations of an LSTM. This indicates that, at least in some instances,\nLSTMs and the human brain handle nonsensical data similarly.\n", "first_author": "Maryam Hashemzadeh", "year_month": "2020-10"}, {"title": "Visually-Augmented Language Modeling", "published_date": "2022-05-20T13:41:12Z", "link": "http://arxiv.org/pdf/2205.10178v2", "abstract": "  Human language is grounded on multimodal knowledge including visual knowledge\nlike colors, sizes, and shapes. However, current large-scale pre-trained\nlanguage models rely on text-only self-supervised training with massive text\ndata, which precludes them from utilizing relevant visual information when\nnecessary. To address this, we propose a novel pre-training framework, named\nVaLM, to Visually-augment text tokens with retrieved relevant images for\nLanguage Modeling. Specifically, VaLM builds on a novel latent text-image\nalignment method via an image retrieval module to fetch corresponding images\ngiven a textual context. With the visually-augmented context, VaLM uses a\nvisual knowledge fusion layer to enable multimodal grounded language modeling\nby attending to both text context and visual knowledge in images. We evaluate\nVaLM on various visual knowledge-intensive commonsense reasoning tasks, which\nrequire visual information to excel. The experimental results illustrate that\nVaLM outperforms all strong language-only and vision-language baselines with\nsubstantial gains in reasoning object commonsense including color, size, and\nshape. Our code is available at https://github.com/Victorwz/VaLM.\n", "first_author": "Weizhi Wang", "year_month": "2022-05"}, {"title": "Adaptive Activation Network For Low Resource Multilingual Speech\n  Recognition", "published_date": "2022-05-28T04:02:59Z", "link": "http://arxiv.org/pdf/2205.14326v1", "abstract": "  Low resource automatic speech recognition (ASR) is a useful but thorny task,\nsince deep learning ASR models usually need huge amounts of training data. The\nexisting models mostly established a bottleneck (BN) layer by pre-training on a\nlarge source language, and transferring to the low resource target language. In\nthis work, we introduced an adaptive activation network to the upper layers of\nASR model, and applied different activation functions to different languages.\nWe also proposed two approaches to train the model: (1) cross-lingual learning,\nreplacing the activation function from source language to target language, (2)\nmultilingual learning, jointly training the Connectionist Temporal\nClassification (CTC) loss of each language and the relevance of different\nlanguages. Our experiments on IARPA Babel datasets demonstrated that our\napproaches outperform the from-scratch training and traditional bottleneck\nfeature based methods. In addition, combining the cross-lingual learning and\nmultilingual learning together could further improve the performance of\nmultilingual speech recognition.\n", "first_author": "Jian Luo", "year_month": "2022-05"}, {"title": "Offline RL for Natural Language Generation with Implicit Language Q\n  Learning", "published_date": "2022-06-05T18:38:42Z", "link": "http://arxiv.org/pdf/2206.11871v2", "abstract": "  Large language models distill broad knowledge from text corpora. However,\nthey can be inconsistent when it comes to completing user specified tasks. This\nissue can be addressed by finetuning such models via supervised learning on\ncurated datasets, or via reinforcement learning. In this work, we propose a\nnovel offline RL method, implicit language Q-learning (ILQL), designed for use\non language models, that combines both the flexible utility maximization\nframework of RL algorithms with the ability of supervised learning to leverage\npreviously collected data, as well as its simplicity and stability. Our method\nemploys a combination of value conservatism alongside an implicit dataset\nsupport constraint in learning value functions, which are then used to guide\nlanguage model generations towards maximizing user-specified utility functions.\nIn addition to empirically validating ILQL, we present a detailed empirical\nanalysis of situations where offline RL can be useful in natural language\ngeneration settings, demonstrating how it can be a more effective utility\noptimizer than prior approaches for end-to-end dialogue, and how it can\neffectively optimize high variance reward functions based on subjective\njudgement, such as whether to label a comment as toxic or not.\n", "first_author": "Charlie Snell", "year_month": "2022-06"}, {"title": "CodeKGC: Code Language Model for Generative Knowledge Graph Construction", "published_date": "2023-04-18T15:12:34Z", "link": "http://arxiv.org/pdf/2304.09048v1", "abstract": "  Current generative knowledge graph construction approaches usually fail to\ncapture structural knowledge by simply flattening natural language into\nserialized texts or a specification language. However, large generative\nlanguage model trained on structured data such as code has demonstrated\nimpressive capability in understanding natural language for structural\nprediction and reasoning tasks. Intuitively, we address the task of generative\nknowledge graph construction with code language model: given a code-format\nnatural language input, the target is to generate triples which can be\nrepresented as code completion tasks. Specifically, we develop schema-aware\nprompts that effectively utilize the semantic structure within the knowledge\ngraph. As code inherently possesses structure, such as class and function\ndefinitions, it serves as a useful model for prior semantic structural\nknowledge. Furthermore, we employ a rationale-enhanced generation method to\nboost the performance. Rationales provide intermediate steps, thereby improving\nknowledge extraction abilities. Experimental results indicate that the proposed\napproach can obtain better performance on benchmark datasets compared with\nbaselines. Code and datasets are available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.\n", "first_author": "Zhen Bi", "year_month": "2023-04"}, {"title": "Multimodal Conditionality for Natural Language Generation", "published_date": "2021-09-02T22:06:07Z", "link": "http://arxiv.org/pdf/2109.01229v1", "abstract": "  Large scale pretrained language models have demonstrated state-of-the-art\nperformance in language understanding tasks. Their application has recently\nexpanded into multimodality learning, leading to improved representations\ncombining vision and language. However, progress in adapting language models\ntowards conditional Natural Language Generation (NLG) has been limited to a\nsingle modality, generally text. We propose MAnTiS, Multimodal Adaptation for\nText Synthesis, a general approach for multimodal conditionality in\ntransformer-based NLG models. In this method, we pass inputs from each modality\nthrough modality-specific encoders, project to textual token space, and finally\njoin to form a conditionality prefix. We fine-tune the pretrained language\nmodel and encoders with the conditionality prefix guiding the generation. We\napply MAnTiS to the task of product description generation, conditioning a\nnetwork on both product images and titles to generate descriptive text. We\ndemonstrate that MAnTiS outperforms strong baseline approaches on standard NLG\nscoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS\ncan generate human quality descriptions consistent with given multimodal\ninputs.\n", "first_author": "Michael Sollami", "year_month": "2021-09"}, {"title": "Same Neurons, Different Languages: Probing Morphosyntax in Multilingual\n  Pre-trained Models", "published_date": "2022-05-04T12:22:31Z", "link": "http://arxiv.org/pdf/2205.02023v3", "abstract": "  The success of multilingual pre-trained models is underpinned by their\nability to learn representations shared by multiple languages even in absence\nof any explicit supervision. However, it remains unclear how these models learn\nto generalise across languages. In this work, we conjecture that multilingual\npre-trained models can derive language-universal abstractions about grammar. In\nparticular, we investigate whether morphosyntactic information is encoded in\nthe same subset of neurons in different languages. We conduct the first\nlarge-scale empirical study over 43 languages and 14 morphosyntactic categories\nwith a state-of-the-art neuron-level probe. Our findings show that the\ncross-lingual overlap between neurons is significant, but its extent may vary\nacross categories and depends on language proximity and pre-training data size.\n", "first_author": "Karolina Sta\u0144czak", "year_month": "2022-05"}, {"title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "published_date": "2022-09-14T17:24:07Z", "link": "http://arxiv.org/pdf/2209.06794v4", "abstract": "  Effective scaling and a flexible task interface enable large language models\nto excel at many tasks. We present PaLI (Pathways Language and Image model), a\nmodel that extends this approach to the joint modeling of language and vision.\nPaLI generates text based on visual and textual inputs, and with this interface\nperforms many vision, language, and multimodal tasks, in many languages. To\ntrain PaLI, we make use of large pre-trained encoder-decoder language models\nand Vision Transformers (ViTs). This allows us to capitalize on their existing\ncapabilities and leverage the substantial cost of training them. We find that\njoint scaling of the vision and language components is important. Since\nexisting Transformers for language are much larger than their vision\ncounterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the\nbenefits from even larger-capacity vision models. To train PaLI, we create a\nlarge multilingual mix of pretraining tasks, based on a new image-text training\nset containing 10B images and texts in over 100 languages. PaLI achieves\nstate-of-the-art in multiple vision and language tasks (such as captioning,\nvisual question-answering, scene-text understanding), while retaining a simple,\nmodular, and scalable design.\n", "first_author": "Xi Chen", "year_month": "2022-09"}, {"title": "SINC: Self-Supervised In-Context Learning for Vision-Language Tasks", "published_date": "2023-07-15T08:33:08Z", "link": "http://arxiv.org/pdf/2307.07742v1", "abstract": "  Large Pre-trained Transformers exhibit an intriguing capacity for in-context\nlearning. Without gradient updates, these models can rapidly construct new\npredictors from demonstrations presented in the inputs. Recent works promote\nthis ability in the vision-language domain by incorporating visual information\ninto large language models that can already make in-context predictions.\nHowever, these methods could inherit issues in the language domain, such as\ntemplate sensitivity and hallucination. Also, the scale of these language\nmodels raises a significant demand for computations, making learning and\noperating these models resource-intensive. To this end, we raise a question:\n``How can we enable in-context learning for general models without being\nconstrained on large language models?\". To answer it, we propose a succinct and\ngeneral framework, Self-supervised IN-Context learning (SINC), that introduces\na meta-model to learn on self-supervised prompts consisting of tailored\ndemonstrations. The learned models can be transferred to downstream tasks for\nmaking in-context predictions on-the-fly. Extensive experiments show that SINC\noutperforms gradient-based methods in various vision-language tasks under\nfew-shot settings. Furthermore, the designs of SINC help us investigate the\nbenefits of in-context learning across different tasks, and the analysis\nfurther reveals the essential components for the emergence of in-context\nlearning in the vision-language domain.\n", "first_author": "Yi-Syuan Chen", "year_month": "2023-07"}, {"title": "Reshaping Robot Trajectories Using Natural Language Commands: A Study of\n  Multi-Modal Data Alignment Using Transformers", "published_date": "2022-03-25T01:36:56Z", "link": "http://arxiv.org/pdf/2203.13411v1", "abstract": "  Natural language is the most intuitive medium for us to interact with other\npeople when expressing commands and instructions. However, using language is\nseldom an easy task when humans need to express their intent towards robots,\nsince most of the current language interfaces require rigid templates with a\nstatic set of action targets and commands. In this work, we provide a flexible\nlanguage-based interface for human-robot collaboration, which allows a user to\nreshape existing trajectories for an autonomous agent. We take advantage of\nrecent advancements in the field of large language models (BERT and CLIP) to\nencode the user command, and then combine these features with trajectory\ninformation using multi-modal attention transformers. We train the model using\nimitation learning over a dataset containing robot trajectories modified by\nlanguage commands, and treat the trajectory generation process as a sequence\nprediction problem, analogously to how language generation architectures\noperate. We evaluate the system in multiple simulated trajectory scenarios, and\nshow a significant performance increase of our model over baseline approaches.\nIn addition, our real-world experiments with a robot arm show that users\nsignificantly prefer our natural language interface over traditional methods\nsuch as kinesthetic teaching or cost-function programming. Our study shows how\nthe field of robotics can take advantage of large pre-trained language models\ntowards creating more intuitive interfaces between robots and machines. Project\nwebpage: https://arthurfenderbucker.github.io/NL_trajectory_reshaper/\n", "first_author": "Arthur Bucker", "year_month": "2022-03"}, {"title": "An Efficient Multilingual Language Model Compression through Vocabulary\n  Trimming", "published_date": "2023-05-24T11:00:33Z", "link": "http://arxiv.org/pdf/2305.15020v1", "abstract": "  Multilingual language model (LM) have become a powerful tool in NLP\nespecially for non-English languages. Nevertheless, model parameters of\nmultilingual LMs remain large due to the larger embedding matrix of the\nvocabulary covering tokens in different languages. On the contrary, monolingual\nLMs can be trained in a target language with the language-specific vocabulary\nonly, but this requires a large budget and availability of reliable corpora to\nachieve a high-quality LM from scratch. In this paper, we propose\nvocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a\ntarget language by deleting irrelevant tokens from its vocabulary. In theory,\nVT can compress any existing multilingual LM to build monolingual LMs in any\nlanguage covered by the multilingual LM. In our experiments, we show that VT\ncan retain the original performance of the multilingual LM, while being smaller\nin size (in general around 50% of the original vocabulary size is enough) than\nthe original multilingual LM. The evaluation is performed over four NLP tasks\n(two generative and two classification tasks) among four widely used\nmultilingual LMs in seven languages. Finally, we show that this methodology can\nkeep the best of both monolingual and multilingual worlds by keeping a small\nsize as monolingual models without the need for specifically retraining them,\nand even limiting potentially harmful social biases.\n", "first_author": "Asahi Ushio", "year_month": "2023-05"}, {"title": "Coupling Large Language Models with Logic Programming for Robust and\n  General Reasoning from Text", "published_date": "2023-07-15T03:29:59Z", "link": "http://arxiv.org/pdf/2307.07696v1", "abstract": "  While large language models (LLMs), such as GPT-3, appear to be robust and\ngeneral, their reasoning ability is not at a level to compete with the best\nmodels trained for specific natural language reasoning problems. In this study,\nwe observe that a large language model can serve as a highly effective few-shot\nsemantic parser. It can convert natural language sentences into a logical form\nthat serves as input for answer set programs, a logic-based declarative\nknowledge representation formalism. The combination results in a robust and\ngeneral system that can handle multiple question-answering tasks without\nrequiring retraining for each new task. It only needs a few examples to guide\nthe LLM's adaptation to a specific task, along with reusable ASP knowledge\nmodules that can be applied to multiple tasks. We demonstrate that this method\nachieves state-of-the-art performance on several NLP benchmarks, including\nbAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot\nplanning tasks that an LLM alone fails to solve.\n", "first_author": "Zhun Yang", "year_month": "2023-07"}, {"title": "nl2spec: Interactively Translating Unstructured Natural Language to\n  Temporal Logics with Large Language Models", "published_date": "2023-03-08T20:08:53Z", "link": "http://arxiv.org/pdf/2303.04864v1", "abstract": "  A rigorous formalization of desired system requirements is indispensable when\nperforming any verification task. This often limits the application of\nverification techniques, as writing formal specifications is an error-prone and\ntime-consuming manual task. To facilitate this, we present nl2spec, a framework\nfor applying Large Language Models (LLMs) to derive formal specifications (in\ntemporal logics) from unstructured natural language. In particular, we\nintroduce a new methodology to detect and resolve the inherent ambiguity of\nsystem requirements in natural language: we utilize LLMs to map subformulas of\nthe formalization back to the corresponding natural language fragments of the\ninput. Users iteratively add, delete, and edit these sub-translations to amend\nerroneous formalizations, which is easier than manually redrafting the entire\nformalization. The framework is agnostic to specific application domains and\ncan be extended to similar specification languages and new neural models. We\nperform a user study to obtain a challenging dataset, which we use to run\nexperiments on the quality of translations. We provide an open-source\nimplementation, including a web-based frontend.\n", "first_author": "Matthias Cosler", "year_month": "2023-03"}, {"title": "LLMs4OL: Large Language Models for Ontology Learning", "published_date": "2023-07-31T13:27:21Z", "link": "http://arxiv.org/pdf/2307.16648v1", "abstract": "  We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs)\nfor Ontology Learning (OL). LLMs have shown significant advancements in natural\nlanguage processing, demonstrating their ability to capture complex language\npatterns in different knowledge domains. Our LLMs4OL paradigm investigates the\nfollowing hypothesis: \\textit{Can LLMs effectively apply their language pattern\ncapturing capability to OL, which involves automatically extracting and\nstructuring knowledge from natural language text?} To test this hypothesis, we\nconduct a comprehensive evaluation using the zero-shot prompting method. We\nevaluate nine different LLM model families for three main OL tasks: term\ntyping, taxonomy discovery, and extraction of non-taxonomic relations.\nAdditionally, the evaluations encompass diverse genres of ontological\nknowledge, including lexicosemantic knowledge in WordNet, geographical\nknowledge in GeoNames, and medical knowledge in UMLS.\n", "first_author": "Hamed Babaei Giglou", "year_month": "2023-07"}, {"title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge\n  for Embodied Agents", "published_date": "2022-01-18T18:59:45Z", "link": "http://arxiv.org/pdf/2201.07207v2", "abstract": "  Can world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of\ngrounding high-level tasks, expressed in natural language (e.g. \"make\nbreakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While\nprior work focused on learning from explicit step-by-step examples of how to\nact, we surprisingly find that if pre-trained LMs are large enough and prompted\nappropriately, they can effectively decompose high-level tasks into mid-level\nplans without any further training. However, the plans produced naively by LLMs\noften cannot map precisely to admissible actions. We propose a procedure that\nconditions on existing demonstrations and semantically translates the plans to\nadmissible actions. Our evaluation in the recent VirtualHome environment shows\nthat the resulting method substantially improves executability over the LLM\nbaseline. The conducted human evaluation reveals a trade-off between\nexecutability and correctness but shows a promising sign towards extracting\nactionable knowledge from language models. Website at\nhttps://huangwl18.github.io/language-planner\n", "first_author": "Wenlong Huang", "year_month": "2022-01"}, {"title": "OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant\n  based on Instructions and Dialogue", "published_date": "2023-06-21T11:09:48Z", "link": "http://arxiv.org/pdf/2306.12174v2", "abstract": "  Large multimodal language models (LMMs) have achieved significant success in\ngeneral domains. However, due to the significant differences between medical\nimages and text and general web content, the performance of LMMs in medical\nscenarios is limited. In ophthalmology, clinical diagnosis relies on multiple\nmodalities of medical images, but unfortunately, multimodal ophthalmic large\nlanguage models have not been explored to date. In this paper, we study and\nconstruct an ophthalmic large multimodal model. Firstly, we use fundus images\nas an entry point to build a disease assessment and diagnosis pipeline to\nachieve common ophthalmic disease diagnosis and lesion segmentation. Then, we\nestablish a new ophthalmic multimodal instruction-following and dialogue\nfine-tuning dataset based on disease-related knowledge data and publicly\navailable real-world medical dialogue. We introduce visual ability into the\nlarge language model to complete the ophthalmic large language and vision\nassistant (OphGLM). Our experimental results demonstrate that the OphGLM model\nperforms exceptionally well, and it has the potential to revolutionize clinical\napplications in ophthalmology. The dataset, code, and models will be made\npublicly available at https://github.com/ML-AILab/OphGLM.\n", "first_author": "Weihao Gao", "year_month": "2023-06"}, {"title": "Extension of hidden markov model for recognizing large vocabulary of\n  sign language", "published_date": "2013-04-11T11:56:39Z", "link": "http://arxiv.org/pdf/1304.3265v1", "abstract": "  Computers still have a long way to go before they can interact with users in\na truly natural fashion. From a users perspective, the most natural way to\ninteract with a computer would be through a speech and gesture interface.\nAlthough speech recognition has made significant advances in the past ten\nyears, gesture recognition has been lagging behind. Sign Languages (SL) are the\nmost accomplished forms of gestural communication. Therefore, their automatic\nanalysis is a real challenge, which is interestingly implied to their lexical\nand syntactic organization levels. Statements dealing with sign language occupy\na significant interest in the Automatic Natural Language Processing (ANLP)\ndomain. In this work, we are dealing with sign language recognition, in\nparticular of French Sign Language (FSL). FSL has its own specificities, such\nas the simultaneity of several parameters, the important role of the facial\nexpression or movement and the use of space for the proper utterance\norganization. Unlike speech recognition, Frensh sign language (FSL) events\noccur both sequentially and simultaneously. Thus, the computational processing\nof FSL is too complex than the spoken languages. We present a novel approach\nbased on HMM to reduce the recognition complexity.\n", "first_author": "Maher Jebali", "year_month": "2013-04"}, {"title": "Language-free Training for Zero-shot Video Grounding", "published_date": "2022-10-24T06:55:29Z", "link": "http://arxiv.org/pdf/2210.12977v1", "abstract": "  Given an untrimmed video and a language query depicting a specific temporal\nmoment in the video, video grounding aims to localize the time interval by\nunderstanding the text and video simultaneously. One of the most challenging\nissues is an extremely time- and cost-consuming annotation collection,\nincluding video captions in a natural language form and their corresponding\ntemporal regions. In this paper, we present a simple yet novel training\nframework for video grounding in the zero-shot setting, which learns a network\nwith only video data without any annotation. Inspired by the recent\nlanguage-free paradigm, i.e. training without language data, we train the\nnetwork without compelling the generation of fake (pseudo) text queries into a\nnatural language form. Specifically, we propose a method for learning a video\ngrounding model by selecting a temporal interval as a hypothetical correct\nanswer and considering the visual feature selected by our method in the\ninterval as a language feature, with the help of the well-aligned\nvisual-language space of CLIP. Extensive experiments demonstrate the prominence\nof our language-free training framework, outperforming the existing zero-shot\nvideo grounding method and even several weakly-supervised approaches with large\nmargins on two standard datasets.\n", "first_author": "Dahye Kim", "year_month": "2022-10"}, {"title": "ERRA: An Embodied Representation and Reasoning Architecture for\n  Long-horizon Language-conditioned Manipulation Tasks", "published_date": "2023-04-05T06:50:22Z", "link": "http://arxiv.org/pdf/2304.02251v1", "abstract": "  This letter introduces ERRA, an embodied learning architecture that enables\nrobots to jointly obtain three fundamental capabilities (reasoning, planning,\nand interaction) for solving long-horizon language-conditioned manipulation\ntasks. ERRA is based on tightly-coupled probabilistic inferences at two\ngranularity levels. Coarse-resolution inference is formulated as sequence\ngeneration through a large language model, which infers action language from\nnatural language instruction and environment state. The robot then zooms to the\nfine-resolution inference part to perform the concrete action corresponding to\nthe action language. Fine-resolution inference is constructed as a Markov\ndecision process, which takes action language and environmental sensing as\nobservations and outputs the action. The results of action execution in\nenvironments provide feedback for subsequent coarse-resolution reasoning. Such\ncoarse-to-fine inference allows the robot to decompose and achieve long-horizon\ntasks interactively. In extensive experiments, we show that ERRA can complete\nvarious long-horizon manipulation tasks specified by abstract language\ninstructions. We also demonstrate successful generalization to the novel but\nsimilar natural language instructions.\n", "first_author": "Chao Zhao", "year_month": "2023-04"}, {"title": "Martta: A C++ Language Workbench", "published_date": "2014-03-15T04:12:39Z", "link": "http://arxiv.org/pdf/1403.3752v1", "abstract": "  Language-orientated programming promises to elevate programmer productivity\nthrough increased abstrac- tion capabilities. Structural programming\nenvironments provide apparatus to reduce the difficulties with syntax. The\nlanguage workbench, a conceptual combination of these two approaches, is a\ncomparatively novel approach to software development and has so far been\nattempted only in dynamic-dispatch, run-time-compiled languages (e.g. Java).\nHowever, it must be remembered that several fields of engineering exist, each\nhaving their own priorities. In the video games industry, where large, complex\nand diverse projects are routinely developed, efficiency is paramount and as\nsuch C++, as a development platform, is widely used. I explore the possibility\nof a language workbench capable of a gradual transition in both skills and code\nfrom the traditional C++ development environment. This article is the design\nfor a language workbench. It uses novel techniques including a\ncontext-sensitive event- driven input system and a hybrid\nsingle/multiple-inherited class model and through a prototype implementation\ndemon- strates that is both concise and practical for C++. I refute the\nhitherto implicit hypothesis that the language workbench paradigm is not\napplicable to the C++ language, showing that C++ can be used for creating an\neffective development framework usable in otherwise pure-C++ programming\nenvironments.\n", "first_author": "Gavin Wood", "year_month": "2014-03"}, {"title": "Evaluating the Effectiveness of Natural Language Inference for Hate\n  Speech Detection in Languages with Limited Labeled Data", "published_date": "2023-06-06T14:40:41Z", "link": "http://arxiv.org/pdf/2306.03722v2", "abstract": "  Most research on hate speech detection has focused on English where a\nsizeable amount of labeled training data is available. However, to expand hate\nspeech detection into more languages, approaches that require minimal training\ndata are needed. In this paper, we test whether natural language inference\n(NLI) models which perform well in zero- and few-shot settings can benefit hate\nspeech detection performance in scenarios where only a limited amount of\nlabeled data is available in the target language. Our evaluation on five\nlanguages demonstrates large performance improvements of NLI fine-tuning over\ndirect fine-tuning in the target language. However, the effectiveness of\nprevious work that proposed intermediate fine-tuning on English data is hard to\nmatch. Only in settings where the English training data does not match the test\ndomain, can our customised NLI-formulation outperform intermediate fine-tuning\non English. Based on our extensive experiments, we propose a set of\nrecommendations for hate speech detection in languages where minimal labeled\ntraining data is available.\n", "first_author": "Janis Goldzycher", "year_month": "2023-06"}, {"title": "Fine-Tuning Pre-Trained Language Models Effectively by Optimizing\n  Subnetworks Adaptively", "published_date": "2022-11-03T08:32:12Z", "link": "http://arxiv.org/pdf/2211.01642v1", "abstract": "  Large-scale pre-trained language models have achieved impressive results on a\nwide range of downstream tasks recently. However, fine-tuning an extremely\nlarge-scale pre-trained language model on limited target datasets is often\nplagued by overfitting and representation degradation. In this paper, we\npropose a Dynamic Parameter Selection (DPS) algorithm for the large-scale\npre-trained models during fine-tuning, which adaptively selects a more\npromising subnetwork to perform staging updates based on gradients of\nback-propagation. Experiments on the GLUE benchmark show that DPS outperforms\nprevious fine-tuning methods in terms of overall performance and stability, and\nconsistently achieves better results with variable pre-trained language models.\nIn addition, DPS brings a large magnitude of improvement in out-of-domain\ntransferring experiments and low-resource scenarios, which shows that it can\nmaintain stable general contextual features and reduce the representation\ncollapse. We release our code at https://github.com/ZhangHaojie077/DPS\n", "first_author": "Haojie Zhang", "year_month": "2022-11"}, {"title": "Could a Large Language Model be Conscious?", "published_date": "2023-03-04T19:14:20Z", "link": "http://arxiv.org/pdf/2303.07103v2", "abstract": "  There has recently been widespread discussion of whether large language\nmodels might be sentient or conscious. Should we take this idea seriously? I\nwill break down the strongest reasons for and against. Given mainstream\nassumptions in the science of consciousness, there are significant obstacles to\nconsciousness in current models: for example, their lack of recurrent\nprocessing, a global workspace, and unified agency. At the same time, it is\nquite possible that these obstacles will be overcome in the next decade or so.\nI conclude that while it is somewhat unlikely that current large language\nmodels are conscious, we should take seriously the possibility that successors\nto large language models may be conscious in the not-too-distant future.\n", "first_author": "David J. Chalmers", "year_month": "2023-03"}, {"title": "CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language\n  Model", "published_date": "2020-03-03T06:39:27Z", "link": "http://arxiv.org/pdf/2003.01355v2", "abstract": "  In this paper, we introduce the Chinese corpus from CLUE organization,\nCLUECorpus2020, a large-scale corpus that can be used directly for\nself-supervised learning such as pre-training of a language model, or language\ngeneration. It has 100G raw corpus with 35 billion Chinese characters, which is\nretrieved from Common Crawl. To better understand this corpus, we conduct\nlanguage understanding experiments on both small and large scale, and results\nshow that the models trained on this corpus can achieve excellent performance\non Chinese. We release a new Chinese vocabulary with a size of 8K, which is\nonly one-third of the vocabulary size used in Chinese Bert released by Google.\nIt saves computational cost and memory while works as good as original\nvocabulary. We also release both large and tiny versions of the pre-trained\nmodel on this corpus. The former achieves the state-of-the-art result, and the\nlatter retains most precision while accelerating training and prediction speed\nfor eight times compared to Bert-base. To facilitate future work on\nself-supervised learning on Chinese, we release our dataset, new vocabulary,\ncodes, and pre-trained models on Github.\n", "first_author": "Liang Xu", "year_month": "2020-03"}, {"title": "A Factorized Recurrent Neural Network based architecture for medium to\n  large vocabulary Language Modelling", "published_date": "2016-02-04T07:53:11Z", "link": "http://arxiv.org/pdf/1602.01576v1", "abstract": "  Statistical language models are central to many applications that use\nsemantics. Recurrent Neural Networks (RNN) are known to produce state of the\nart results for language modelling, outperforming their traditional n-gram\ncounterparts in many cases. To generate a probability distribution across a\nvocabulary, these models require a softmax output layer that linearly increases\nin size with the size of the vocabulary. Large vocabularies need a\ncommensurately large softmax layer and training them on typical laptops/PCs\nrequires significant time and machine resources. In this paper we present a new\ntechnique for implementing RNN based large vocabulary language models that\nsubstantially speeds up computation while optimally using the limited memory\nresources. Our technique, while building on the notion of factorizing the\noutput layer by having multiple output layers, improves on the earlier work by\nsubstantially optimizing on the individual output layer size and also\neliminating the need for a multistep prediction process.\n", "first_author": "Anantharaman Palacode Narayana Iyer", "year_month": "2016-02"}, {"title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based\n  Bias in NLP", "published_date": "2021-02-28T11:07:37Z", "link": "http://arxiv.org/pdf/2103.00453v2", "abstract": "  When trained on large, unfiltered crawls from the internet, language models\npick up and reproduce all kinds of undesirable biases that can be found in the\ndata: they often generate racist, sexist, violent or otherwise toxic language.\nAs large models require millions of training examples to achieve good\nperformance, it is difficult to completely prevent them from being exposed to\nsuch content. In this paper, we first demonstrate a surprising finding:\npretrained language models recognize, to a considerable degree, their\nundesirable biases and the toxicity of the content they produce. We refer to\nthis capability as self-diagnosis. Based on this finding, we then propose a\ndecoding algorithm that, given only a textual description of the undesired\nbehavior, reduces the probability of a language model producing problematic\ntext. We refer to this approach as self-debiasing. Self-debiasing does not rely\non manually curated word lists, nor does it require any training data or\nchanges to the model's parameters. While we by no means eliminate the issue of\nlanguage models generating biased text, we believe our approach to be an\nimportant step in this direction.\n", "first_author": "Timo Schick", "year_month": "2021-02"}, {"title": "An Inference Approach To Question Answering Over Knowledge Graphs", "published_date": "2021-12-21T10:07:55Z", "link": "http://arxiv.org/pdf/2112.11070v1", "abstract": "  Knowledge Graphs (KG) act as a great tool for holding distilled information\nfrom large natural language text corpora. The problem of natural language\nquerying over knowledge graphs is essential for the human consumption of this\ninformation. This problem is typically addressed by converting the natural\nlanguage query to a structured query and then firing the structured query on\nthe KG. Direct answering models over knowledge graphs in literature are very\nfew. The query conversion models and direct models both require specific\ntraining data pertaining to the domain of the knowledge graph. In this work, we\nconvert the problem of natural language querying over knowledge graphs to an\ninference problem over premise-hypothesis pairs. Using trained deep learning\nmodels for the converted proxy inferencing problem, we provide the solution for\nthe original natural language querying problem. Our method achieves over 90%\naccuracy on MetaQA dataset, beating the existing state-of-the-art. We also\npropose a model for inferencing called Hierarchical Recurrent Path\nEncoder(HRPE). The inferencing models can be fine-tuned to be used across\ndomains with less training data. Our approach does not require large\ndomain-specific training data for querying on new knowledge graphs from\ndifferent domains.\n", "first_author": "Aayushee Gupta", "year_month": "2021-12"}, {"title": "Categorization in the Wild: Generalizing Cognitive Models to\n  Naturalistic Data across Languages", "published_date": "2019-02-23T19:21:08Z", "link": "http://arxiv.org/pdf/1902.08830v1", "abstract": "  Categories such as animal or furniture are acquired at an early age and play\nan important role in processing, organizing, and communicating world knowledge.\nCategories exist across cultures: they allow to efficiently represent the\ncomplexity of the world, and members of a community strongly agree on their\nnature, revealing a shared mental representation. Models of category learning\nand representation, however, are typically tested on data from small-scale\nexperiments involving small sets of concepts with artificially restricted\nfeatures; and experiments predominantly involve participants of selected\ncultural and socio-economical groups (very often involving western native\nspeakers of English such as U.S. college students) . This work investigates\nwhether models of categorization generalize (a) to rich and noisy data\napproximating the environment humans live in; and (b) across languages and\ncultures. We present a Bayesian cognitive model designed to jointly learn\ncategories and their structured representation from natural language text which\nallows us to (a) evaluate performance on a large scale, and (b) apply our model\nto a diverse set of languages. We show that meaningful categories comprising\nhundreds of concepts and richly structured featural representations emerge\nacross languages. Our work illustrates the potential of recent advances in\ncomputational modeling and large scale naturalistic datasets for cognitive\nscience research.\n", "first_author": "Lea Frermann", "year_month": "2019-02"}, {"title": "CoCoLM: COmplex COmmonsense Enhanced Language Model with Discourse\n  Relations", "published_date": "2020-12-31T15:05:36Z", "link": "http://arxiv.org/pdf/2012.15643v2", "abstract": "  Large-scale pre-trained language models have demonstrated strong knowledge\nrepresentation ability. However, recent studies suggest that even though these\ngiant models contains rich simple commonsense knowledge (e.g., bird can fly and\nfish can swim.), they often struggle with the complex commonsense knowledge\nthat involves multiple eventualities (verb-centric phrases, e.g., identifying\nthe relationship between ``Jim yells at Bob'' and ``Bob is upset'').To address\nthis problem, in this paper, we propose to help pre-trained language models\nbetter incorporate complex commonsense knowledge. Different from existing\nfine-tuning approaches, we do not focus on a specific task and propose a\ngeneral language model named CoCoLM. Through the careful training over a\nlarge-scale eventuality knowledge graphs ASER, we successfully teach\npre-trained language models (i.e., BERT and RoBERTa) rich complex commonsense\nknowledge among eventualities. Experiments on multiple downstream commonsense\ntasks that requires the correct understanding of eventualities demonstrate the\neffectiveness of CoCoLM.\n", "first_author": "Changlong Yu", "year_month": "2020-12"}, {"title": "Leveraging Pre-trained Language Model for Speech Sentiment Analysis", "published_date": "2021-06-11T20:15:21Z", "link": "http://arxiv.org/pdf/2106.06598v1", "abstract": "  In this paper, we explore the use of pre-trained language models to learn\nsentiment information of written texts for speech sentiment analysis. First, we\ninvestigate how useful a pre-trained language model would be in a 2-step\npipeline approach employing Automatic Speech Recognition (ASR) and\ntranscripts-based sentiment analysis separately. Second, we propose a pseudo\nlabel-based semi-supervised training strategy using a language model on an\nend-to-end speech sentiment approach to take advantage of a large, but\nunlabeled speech dataset for training. Although spoken and written texts have\ndifferent linguistic characteristics, they can complement each other in\nunderstanding sentiment. Therefore, the proposed system can not only model\nacoustic characteristics to bear sentiment-specific information in speech\nsignals, but learn latent information to carry sentiments in the text\nrepresentation. In these experiments, we demonstrate the proposed approaches\nimprove F1 scores consistently compared to systems without a language model.\nMoreover, we also show that the proposed framework can reduce 65% of human\nsupervision by leveraging a large amount of data without human sentiment\nannotation and boost performance in a low-resource condition where the human\nsentiment annotation is not available enough.\n", "first_author": "Suwon Shon", "year_month": "2021-06"}, {"title": "Psychologically-informed chain-of-thought prompts for metaphor\n  understanding in large language models", "published_date": "2022-09-16T19:23:13Z", "link": "http://arxiv.org/pdf/2209.08141v2", "abstract": "  Probabilistic models of language understanding are valuable tools for\ninvestigating human language use. However, they need to be hand-designed for a\nparticular domain. In contrast, large language models (LLMs) are trained on\ntext that spans a wide array of domains, but they lack the structure and\ninterpretability of probabilistic models. In this paper, we use\nchain-of-thought prompts to introduce structures from probabilistic models into\nLLMs. We explore this approach in the case of metaphor understanding. Our\nchain-of-thought prompts lead language models to infer latent variables and\nreason about their relationships in order to choose appropriate paraphrases for\nmetaphors. The latent variables and relationships chosen are informed by\ntheories of metaphor understanding from cognitive psychology. We apply these\nprompts to the two largest versions of GPT-3 and show that they can improve\nperformance in a paraphrase selection task.\n", "first_author": "Ben Prystawski", "year_month": "2022-09"}, {"title": "On the homology language of HDA models of transition systems", "published_date": "2022-08-03T18:08:45Z", "link": "http://arxiv.org/pdf/2208.02286v1", "abstract": "  Given a transition system with an independence relation on the alphabet of\nlabels, one can associate with it a usually very large symmetric\nhigher-dimensional automaton. The purpose of this paper is to show that by\nchoosing an acyclic relation whose symmetric closure is the given independence\nrelation, it is possible to construct a much smaller nonsymmetric HDA with the\nsame homology language.\n", "first_author": "Thomas Kahl", "year_month": "2022-08"}, {"title": "Causal-Discovery Performance of ChatGPT in the context of Neuropathic\n  Pain Diagnosis", "published_date": "2023-01-24T19:23:38Z", "link": "http://arxiv.org/pdf/2301.13819v2", "abstract": "  ChatGPT has demonstrated exceptional proficiency in natural language\nconversation, e.g., it can answer a wide range of questions while no previous\nlarge language models can. Thus, we would like to push its limit and explore\nits ability to answer causal discovery questions by using a medical benchmark\n(Tu et al. 2019) in causal discovery.\n", "first_author": "Ruibo Tu", "year_month": "2023-01"}, {"title": "Improving the Diproche CNL through autoformalization via GPT-3", "published_date": "2023-03-12T20:11:25Z", "link": "http://arxiv.org/pdf/2303.17513v1", "abstract": "  The Diproche system is an automated proof checker for texts written in a\ncontrolled fragment of German, designed for didactical applications in classes\nintroducing students to proofs for the first time. The first version of the\nsystem used a controlled natural language for which a Prolog formalization\nroutine was written. In this paper, we explore the possibility of prompting\nlarge language models for autoformalization in the context of Diproche, with\nencouraging first results.\n", "first_author": "Merlin Carl", "year_month": "2023-03"}, {"title": "A Deep Generative Model for Code-Switched Text", "published_date": "2019-06-21T06:27:17Z", "link": "http://arxiv.org/pdf/1906.08972v1", "abstract": "  Code-switching, the interleaving of two or more languages within a sentence\nor discourse is pervasive in multilingual societies. Accurate language models\nfor code-switched text are critical for NLP tasks. State-of-the-art\ndata-intensive neural language models are difficult to train well from scarce\nlanguage-labeled code-switched text. A potential solution is to use deep\ngenerative models to synthesize large volumes of realistic code-switched text.\nAlthough generative adversarial networks and variational autoencoders can\nsynthesize plausible monolingual text from continuous latent space, they cannot\nadequately address code-switched text, owing to their informal style and\ncomplex interplay between the constituent languages. We introduce VACS, a novel\nvariational autoencoder architecture specifically tailored to code-switching\nphenomena. VACS encodes to and decodes from a two-level hierarchical\nrepresentation, which models syntactic contextual signals in the lower level,\nand language switching signals in the upper layer. Sampling representations\nfrom the prior and decoding them produced well-formed, diverse code-switched\nsentences. Extensive experiments show that using synthetic code-switched text\nwith natural monolingual data results in significant (33.06%) drop in\nperplexity.\n", "first_author": "Bidisha Samanta", "year_month": "2019-06"}, {"title": "Rare Words: A Major Problem for Contextualized Embeddings And How to Fix\n  it by Attentive Mimicking", "published_date": "2019-04-14T15:26:52Z", "link": "http://arxiv.org/pdf/1904.06707v4", "abstract": "  Pretraining deep neural network architectures with a language modeling\nobjective has brought large improvements for many natural language processing\ntasks. Exemplified by BERT, a recently proposed such architecture, we\ndemonstrate that despite being trained on huge amounts of data, deep language\nmodels still struggle to understand rare words. To fix this problem, we adapt\nAttentive Mimicking, a method that was designed to explicitly learn embeddings\nfor rare words, to deep language models. In order to make this possible, we\nintroduce one-token approximation, a procedure that enables us to use Attentive\nMimicking even when the underlying language model uses subword-based\ntokenization, i.e., it does not assign embeddings to all words. To evaluate our\nmethod, we create a novel dataset that tests the ability of language models to\ncapture semantic properties of words without any task-specific fine-tuning.\nUsing this dataset, we show that adding our adapted version of Attentive\nMimicking to BERT does indeed substantially improve its understanding of rare\nwords.\n", "first_author": "Timo Schick", "year_month": "2019-04"}, {"title": "Detecting ESG topics using domain-specific language models and data\n  augmentation approaches", "published_date": "2020-10-16T11:20:07Z", "link": "http://arxiv.org/pdf/2010.08319v1", "abstract": "  Despite recent advances in deep learning-based language modelling, many\nnatural language processing (NLP) tasks in the financial domain remain\nchallenging due to the paucity of appropriately labelled data. Other issues\nthat can limit task performance are differences in word distribution between\nthe general corpora - typically used to pre-train language models - and\nfinancial corpora, which often exhibit specialized language and symbology.\nHere, we investigate two approaches that may help to mitigate these issues.\nFirstly, we experiment with further language model pre-training using large\namounts of in-domain data from business and financial news. We then apply\naugmentation approaches to increase the size of our dataset for model\nfine-tuning. We report our findings on an Environmental, Social and Governance\n(ESG) controversies dataset and demonstrate that both approaches are beneficial\nto accuracy in classification tasks.\n", "first_author": "Tim Nugent", "year_month": "2020-10"}, {"title": "HateBERT: Retraining BERT for Abusive Language Detection in English", "published_date": "2020-10-23T15:14:14Z", "link": "http://arxiv.org/pdf/2010.12472v2", "abstract": "  In this paper, we introduce HateBERT, a re-trained BERT model for abusive\nlanguage detection in English. The model was trained on RAL-E, a large-scale\ndataset of Reddit comments in English from communities banned for being\noffensive, abusive, or hateful that we have collected and made available to the\npublic. We present the results of a detailed comparison between a general\npre-trained language model and the abuse-inclined version obtained by\nretraining with posts from the banned communities on three English datasets for\noffensive, abusive language and hate speech detection tasks. In all datasets,\nHateBERT outperforms the corresponding general BERT model. We also discuss a\nbattery of experiments comparing the portability of the generic pre-trained\nlanguage model and its corresponding abusive language-inclined counterpart\nacross the datasets, indicating that portability is affected by compatibility\nof the annotated phenomena.\n", "first_author": "Tommaso Caselli", "year_month": "2020-10"}, {"title": "SLM: Learning a Discourse Language Representation with Sentence\n  Unshuffling", "published_date": "2020-10-30T13:33:41Z", "link": "http://arxiv.org/pdf/2010.16249v1", "abstract": "  We introduce Sentence-level Language Modeling, a new pre-training objective\nfor learning a discourse language representation in a fully self-supervised\nmanner. Recent pre-training methods in NLP focus on learning either bottom or\ntop-level language representations: contextualized word representations derived\nfrom language model objectives at one extreme and a whole sequence\nrepresentation learned by order classification of two given textual segments at\nthe other. However, these models are not directly encouraged to capture\nrepresentations of intermediate-size structures that exist in natural languages\nsuch as sentences and the relationships among them. To that end, we propose a\nnew approach to encourage learning of a contextualized sentence-level\nrepresentation by shuffling the sequence of input sentences and training a\nhierarchical transformer model to reconstruct the original ordering. Through\nexperiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show\nthat this feature of our model improves the performance of the original BERT by\nlarge margins.\n", "first_author": "Haejun Lee", "year_month": "2020-10"}, {"title": "Allocating Large Vocabulary Capacity for Cross-lingual Language Model\n  Pre-training", "published_date": "2021-09-15T14:04:16Z", "link": "http://arxiv.org/pdf/2109.07306v1", "abstract": "  Compared to monolingual models, cross-lingual models usually require a more\nexpressive vocabulary to represent all languages adequately. We find that many\nlanguages are under-represented in recent cross-lingual language models due to\nthe limited vocabulary capacity. To this end, we propose an algorithm VoCap to\ndetermine the desired vocabulary capacity of each language. However, increasing\nthe vocabulary size significantly slows down the pre-training speed. In order\nto address the issues, we propose k-NN-based target sampling to accelerate the\nexpensive softmax. Our experiments show that the multilingual vocabulary\nlearned with VoCap benefits cross-lingual language model pre-training.\nMoreover, k-NN-based target sampling mitigates the side-effects of increasing\nthe vocabulary size while achieving comparable performance and faster\npre-training speed. The code and the pretrained multilingual vocabularies are\navailable at https://github.com/bozheng-hit/VoCapXLM.\n", "first_author": "Bo Zheng", "year_month": "2021-09"}, {"title": "Cross-Lingual Language Model Meta-Pretraining", "published_date": "2021-09-23T03:47:44Z", "link": "http://arxiv.org/pdf/2109.11129v1", "abstract": "  The success of pretrained cross-lingual language models relies on two\nessential abilities, i.e., generalization ability for learning downstream tasks\nin a source language, and cross-lingual transferability for transferring the\ntask knowledge to other languages. However, current methods jointly learn the\ntwo abilities in a single-phase cross-lingual pretraining process, resulting in\na trade-off between generalization and cross-lingual transfer. In this paper,\nwe propose cross-lingual language model meta-pretraining, which learns the two\nabilities in different training phases. Our method introduces an additional\nmeta-pretraining phase before cross-lingual pretraining, where the model learns\ngeneralization ability on a large-scale monolingual corpus. Then, the model\nfocuses on learning cross-lingual transfer on a multilingual corpus.\nExperimental results show that our method improves both generalization and\ncross-lingual transfer, and produces better-aligned representations across\ndifferent languages.\n", "first_author": "Zewen Chi", "year_month": "2021-09"}, {"title": "Perturbation Sensitivity Analysis to Detect Unintended Model Biases", "published_date": "2019-10-09T19:25:21Z", "link": "http://arxiv.org/pdf/1910.04210v1", "abstract": "  Data-driven statistical Natural Language Processing (NLP) techniques leverage\nlarge amounts of language data to build models that can understand language.\nHowever, most language data reflect the public discourse at the time the data\nwas produced, and hence NLP models are susceptible to learning incidental\nassociations around named referents at a particular point in time, in addition\nto general linguistic meaning. An NLP system designed to model notions such as\nsentiment and toxicity should ideally produce scores that are independent of\nthe identity of such entities mentioned in text and their social associations.\nFor example, in a general purpose sentiment analysis system, a phrase such as I\nhate Katy Perry should be interpreted as having the same sentiment as I hate\nTaylor Swift. Based on this idea, we propose a generic evaluation framework,\nPerturbation Sensitivity Analysis, which detects unintended model biases\nrelated to named entities, and requires no new annotations or corpora. We\ndemonstrate the utility of this analysis by employing it on two different NLP\nmodels --- a sentiment model and a toxicity model --- applied on online\ncomments in English language from four different genres.\n", "first_author": "Vinodkumar Prabhakaran", "year_month": "2019-10"}, {"title": "Segatron: Segment-Aware Transformer for Language Modeling and\n  Understanding", "published_date": "2020-04-30T17:38:27Z", "link": "http://arxiv.org/pdf/2004.14996v2", "abstract": "  Transformers are powerful for sequence modeling. Nearly all state-of-the-art\nlanguage models and pre-trained language models are based on the Transformer\narchitecture. However, it distinguishes sequential tokens only with the token\nposition index. We hypothesize that better contextual representations can be\ngenerated from the Transformer with richer positional information. To verify\nthis, we propose a segment-aware Transformer (Segatron), by replacing the\noriginal token position encoding with a combined position encoding of\nparagraph, sentence, and token. We first introduce the segment-aware mechanism\nto Transformer-XL, which is a popular Transformer-based language model with\nmemory extension and relative position encoding. We find that our method can\nfurther improve the Transformer-XL base model and large model, achieving 17.1\nperplexity on the WikiText-103 dataset. We further investigate the pre-training\nmasked language modeling task with Segatron. Experimental results show that\nBERT pre-trained with Segatron (SegaBERT) can outperform BERT with vanilla\nTransformer on various NLP tasks, and outperforms RoBERTa on zero-shot sentence\nrepresentation learning.\n", "first_author": "He Bai", "year_month": "2020-04"}, {"title": "EstBERT: A Pretrained Language-Specific BERT for Estonian", "published_date": "2020-11-09T21:33:53Z", "link": "http://arxiv.org/pdf/2011.04784v3", "abstract": "  This paper presents EstBERT, a large pretrained transformer-based\nlanguage-specific BERT model for Estonian. Recent work has evaluated\nmultilingual BERT models on Estonian tasks and found them to outperform the\nbaselines. Still, based on existing studies on other languages, a\nlanguage-specific BERT model is expected to improve over the multilingual ones.\nWe first describe the EstBERT pretraining process and then present the results\nof the models based on finetuned EstBERT for multiple NLP tasks, including POS\nand morphological tagging, named entity recognition and text classification.\nThe evaluation results show that the models based on EstBERT outperform\nmultilingual BERT models on five tasks out of six, providing further evidence\ntowards a view that training language-specific BERT models are still useful,\neven when multilingual models are available.\n", "first_author": "Hasan Tanvir", "year_month": "2020-11"}, {"title": "Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models", "published_date": "2023-05-23T18:27:51Z", "link": "http://arxiv.org/pdf/2305.14456v1", "abstract": "  Are language models culturally biased? It is important that language models\nconform to the cultural aspects of the communities they serve. However, we show\nin this paper that language models suffer from a significant bias towards\nWestern culture when handling and generating text in Arabic, often preferring,\nand producing Western-fitting content as opposed to the relevant Arab content.\nWe quantify this bias through a likelihood scoring-based metric using naturally\noccurring contexts that we collect from online social media. Our experiments\nreveal that both Arabic monolingual and multilingual models exhibit bias\ntowards Western culture in eight different cultural aspects: person names,\nfood, clothing, location, literature, beverage, religion, and sports. Models\nalso tend to exhibit more bias when prompted with Arabic sentences that are\nmore linguistically aligned with English. These findings raise concerns about\nthe cultural relevance of current language models. Our analyses show that\nproviding culture-indicating tokens or culturally-relevant demonstrations to\nthe model can help in debiasing.\n", "first_author": "Tarek Naous", "year_month": "2023-05"}, {"title": "Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale", "published_date": "2023-05-26T21:22:10Z", "link": "http://arxiv.org/pdf/2305.17266v2", "abstract": "  In recent years, language models have drastically grown in size, and the\nabilities of these models have been shown to improve with scale. The majority\nof recent scaling laws studies focused on high-compute high-parameter count\nsettings, leaving the question of when these abilities begin to emerge largely\nunanswered. In this paper, we investigate whether the effects of pre-training\ncan be observed when the problem size is reduced, modeling a smaller,\nreduced-vocabulary language. We show the benefits of pre-training with masked\nlanguage modeling (MLM) objective in models as small as 1.25M parameters, and\nestablish a strong correlation between pre-training perplexity and downstream\nperformance (GLUE benchmark). We examine downscaling effects, extending scaling\nlaws to models as small as ~1M parameters. At this scale, we observe a break of\nthe power law for compute-optimal models and show that the MLM loss does not\nscale smoothly with compute-cost (FLOPs) below $2.2 \\times 10^{15}$ FLOPs. We\nalso find that adding layers does not always benefit downstream performance.\n", "first_author": "Vijeta Deshpande", "year_month": "2023-05"}, {"title": "Analyzing the Limits of Self-Supervision in Handling Bias in Language", "published_date": "2021-12-16T05:36:08Z", "link": "http://arxiv.org/pdf/2112.08637v2", "abstract": "  Prompting inputs with natural language task descriptions has emerged as a\npopular mechanism to elicit reasonably accurate outputs from large-scale\ngenerative language models with little to no in-context supervision. This also\nhelps gain insight into how well language models capture the semantics of a\nwide range of downstream tasks purely from self-supervised pre-training on\nmassive corpora of unlabeled text. Such models have naturally also been exposed\nto a lot of undesirable content like racist and sexist language and there is\nlimited work on awareness of models along these dimensions. In this paper, we\ndefine and comprehensively evaluate how well such language models capture the\nsemantics of four tasks for bias: diagnosis, identification, extraction and\nrephrasing. We define three broad classes of task descriptions for these tasks:\nstatement, question, and completion, with numerous lexical variants within each\nclass. We study the efficacy of prompting for each task using these classes and\nthe null task description across several decoding methods and few-shot\nexamples. Our analyses indicate that language models are capable of performing\nthese tasks to widely varying degrees across different bias dimensions, such as\ngender and political affiliation. We believe our work is an important step\ntowards unbiased language models by quantifying the limits of current\nself-supervision objectives at accomplishing such sociologically challenging\ntasks.\n", "first_author": "Lisa Bauer", "year_month": "2021-12"}, {"title": "Revisiting the \"Video\" in Video-Language Understanding", "published_date": "2022-06-03T17:57:33Z", "link": "http://arxiv.org/pdf/2206.01720v1", "abstract": "  What makes a video task uniquely suited for videos, beyond what can be\nunderstood from a single image? Building on recent progress in self-supervised\nimage-language models, we revisit this question in the context of video and\nlanguage tasks. We propose the atemporal probe (ATP), a new model for\nvideo-language analysis which provides a stronger bound on the baseline\naccuracy of multimodal models constrained by image-level understanding. By\napplying this model to standard discriminative video and language tasks, such\nas video question answering and text-to-video retrieval, we characterize the\nlimitations and potential of current video-language benchmarks. We find that\nunderstanding of event temporality is often not necessary to achieve strong or\nstate-of-the-art performance, even compared with recent large-scale\nvideo-language models and in contexts intended to benchmark deeper video-level\nunderstanding. We also demonstrate how ATP can improve both video-language\ndataset and model design. We describe a technique for leveraging ATP to better\ndisentangle dataset subsets with a higher concentration of temporally\nchallenging data, improving benchmarking efficacy for causal and temporal\nunderstanding. Further, we show that effectively integrating ATP into full\nvideo-level temporal models can improve efficiency and state-of-the-art\naccuracy.\n", "first_author": "Shyamal Buch", "year_month": "2022-06"}, {"title": "Evaluating Biased Attitude Associations of Language Models in an\n  Intersectional Context", "published_date": "2023-07-07T03:01:56Z", "link": "http://arxiv.org/pdf/2307.03360v1", "abstract": "  Language models are trained on large-scale corpora that embed implicit biases\ndocumented in psychology. Valence associations (pleasantness/unpleasantness) of\nsocial groups determine the biased attitudes towards groups and concepts in\nsocial cognition. Building on this established literature, we quantify how\nsocial groups are valenced in English language models using a sentence template\nthat provides an intersectional context. We study biases related to age,\neducation, gender, height, intelligence, literacy, race, religion, sex, sexual\norientation, social class, and weight. We present a concept projection approach\nto capture the valence subspace through contextualized word embeddings of\nlanguage models. Adapting the projection-based approach to embedding\nassociation tests that quantify bias, we find that language models exhibit the\nmost biased attitudes against gender identity, social class, and sexual\norientation signals in language. We find that the largest and better-performing\nmodel that we study is also more biased as it effectively captures bias\nembedded in sociocultural data. We validate the bias evaluation method by\noverperforming on an intrinsic valence evaluation task. The approach enables us\nto measure complex intersectional biases as they are known to manifest in the\noutputs and applications of language models that perpetuate historical biases.\nMoreover, our approach contributes to design justice as it studies the\nassociations of groups underrepresented in language such as transgender and\nhomosexual individuals.\n", "first_author": "Shiva Omrani Sabbaghi", "year_month": "2023-07"}, {"title": "Cross-Lingual NER for Financial Transaction Data in Low-Resource\n  Languages", "published_date": "2023-07-16T00:45:42Z", "link": "http://arxiv.org/pdf/2307.08714v1", "abstract": "  We propose an efficient modeling framework for cross-lingual named entity\nrecognition in semi-structured text data. Our approach relies on both knowledge\ndistillation and consistency training. The modeling framework leverages\nknowledge from a large language model (XLMRoBERTa) pre-trained on the source\nlanguage, with a student-teacher relationship (knowledge distillation). The\nstudent model incorporates unsupervised consistency training (with KL\ndivergence loss) on the low-resource target language.\n  We employ two independent datasets of SMSs in English and Arabic, each\ncarrying semi-structured banking transaction information, and focus on\nexhibiting the transfer of knowledge from English to Arabic. With access to\nonly 30 labeled samples, our model can generalize the recognition of merchants,\namounts, and other fields from English to Arabic. We show that our modeling\napproach, while efficient, performs best overall when compared to\nstate-of-the-art approaches like DistilBERT pre-trained on the target language\nor a supervised model directly trained on labeled data in the target language.\n  Our experiments show that it is enough to learn to recognize entities in\nEnglish to reach reasonable performance in a low-resource language in the\npresence of a few labeled samples of semi-structured data. The proposed\nframework has implications for developing multi-lingual applications,\nespecially in geographies where digital endeavors rely on both English and one\nor more low-resource language(s), sometimes mixed with English or employed\nsingly.\n", "first_author": "Sunisth Kumar", "year_month": "2023-07"}, {"title": "Visualization in the Era of Artificial Intelligence: Experiments for\n  Creating Structural Visualizations by Prompting Large Language Models", "published_date": "2023-05-05T09:16:59Z", "link": "http://arxiv.org/pdf/2305.03380v2", "abstract": "  Large Language Models (LLMs) have revolutionized natural language processing\nby generating human-like text and images from textual input. However, their\npotential to generate complex 2D/3D visualizations has been largely unexplored.\nWe report initial experiments showing that LLMs can generate 2D/3D\nvisualizations that may be used for legal visualization. Further research is\nneeded for complex 2D visualizations and 3D scenes. LLMs can become a powerful\ntool for many industries and applications, generating complex visualizations\nwith minimal training.\n", "first_author": "Hans-Georg Fill", "year_month": "2023-05"}, {"title": "LegaLMFiT: Efficient Short Legal Text Classification with LSTM Language\n  Model Pre-Training", "published_date": "2021-09-02T14:45:04Z", "link": "http://arxiv.org/pdf/2109.00993v3", "abstract": "  Large Transformer-based language models such as BERT have led to broad\nperformance improvements on many NLP tasks. Domain-specific variants of these\nmodels have demonstrated excellent performance on a variety of specialised\ntasks. In legal NLP, BERT-based models have led to new state-of-the-art results\non multiple tasks. The exploration of these models has demonstrated the\nimportance of capturing the specificity of the legal language and its\nvocabulary. However, such approaches suffer from high computational costs,\nleading to a higher ecological impact and lower accessibility. Our findings,\nfocusing on English language legal text, show that lightweight LSTM-based\nLanguage Models are able to capture enough information from a small legal text\npretraining corpus and achieve excellent performance on short legal text\nclassification tasks. This is achieved with a significantly reduced\ncomputational overhead compared to BERT-based models. However, our method also\nshows degraded performance on a more complex task, multi-label classification\nof longer documents, highlighting the limitations of this lightweight approach.\n", "first_author": "Benjamin Clavi\u00e9", "year_month": "2021-09"}, {"title": "Transfer Learning Approaches for Building Cross-Language Dense Retrieval\n  Models", "published_date": "2022-01-20T22:11:38Z", "link": "http://arxiv.org/pdf/2201.08471v1", "abstract": "  The advent of transformer-based models such as BERT has led to the rise of\nneural ranking models. These models have improved the effectiveness of\nretrieval systems well beyond that of lexical term matching models such as\nBM25. While monolingual retrieval tasks have benefited from large-scale\ntraining collections such as MS MARCO and advances in neural architectures,\ncross-language retrieval tasks have fallen behind these advancements. This\npaper introduces ColBERT-X, a generalization of the ColBERT\nmulti-representation dense retrieval model that uses the XLM-RoBERTa (XLM-R)\nencoder to support cross-language information retrieval (CLIR). ColBERT-X can\nbe trained in two ways. In zero-shot training, the system is trained on the\nEnglish MS MARCO collection, relying on the XLM-R encoder for cross-language\nmappings. In translate-train, the system is trained on the MS MARCO English\nqueries coupled with machine translations of the associated MS MARCO passages.\nResults on ad hoc document ranking tasks in several languages demonstrate\nsubstantial and statistically significant improvements of these trained dense\nretrieval models over traditional lexical CLIR baselines.\n", "first_author": "Suraj Nair", "year_month": "2022-01"}, {"title": "Solving Quantitative Reasoning Problems with Language Models", "published_date": "2022-06-29T18:54:49Z", "link": "http://arxiv.org/pdf/2206.14858v2", "abstract": "  Language models have achieved remarkable performance on a wide range of tasks\nthat require natural language understanding. Nevertheless, state-of-the-art\nmodels have generally struggled with tasks that require quantitative reasoning,\nsuch as solving mathematics, science, and engineering problems at the college\nlevel. To help close this gap, we introduce Minerva, a large language model\npretrained on general natural language data and further trained on technical\ncontent. The model achieves state-of-the-art performance on technical\nbenchmarks without the use of external tools. We also evaluate our model on\nover two hundred undergraduate-level problems in physics, biology, chemistry,\neconomics, and other sciences that require quantitative reasoning, and find\nthat the model can correctly answer nearly a third of them.\n", "first_author": "Aitor Lewkowycz", "year_month": "2022-06"}, {"title": "The Wisdom of Hindsight Makes Language Models Better Instruction\n  Followers", "published_date": "2023-02-10T12:16:38Z", "link": "http://arxiv.org/pdf/2302.05206v1", "abstract": "  Reinforcement learning has seen wide success in finetuning large language\nmodels to better align with instructions via human feedback. The so-called\nalgorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates\nimpressive performance on the GPT series models. However, the underlying\nReinforcement Learning (RL) algorithm is complex and requires an additional\ntraining pipeline for reward and value networks. In this paper, we consider an\nalternative approach: converting feedback to instruction by relabeling the\noriginal one and training the model for better alignment in a supervised\nmanner. Such an algorithm doesn't require any additional parameters except for\nthe original language model and maximally reuses the pretraining pipeline. To\nachieve this, we formulate instruction alignment problem for language models as\na goal-reaching problem in decision making. We propose Hindsight Instruction\nRelabeling (HIR), a novel algorithm for aligning language models with\ninstructions. The resulting two-stage algorithm shed light to a family of\nreward-free approaches that utilize the hindsightly relabeled instructions\nbased on feedback. We evaluate the performance of HIR extensively on 12\nchallenging BigBench reasoning tasks and show that HIR outperforms the baseline\nalgorithms and is comparable to or even surpasses supervised finetuning.\n", "first_author": "Tianjun Zhang", "year_month": "2023-02"}, {"title": "Scalable Syntax-Aware Language Models Using Knowledge Distillation", "published_date": "2019-06-14T23:42:08Z", "link": "http://arxiv.org/pdf/1906.06438v1", "abstract": "  Prior work has shown that, on small amounts of training data, syntactic\nneural language models learn structurally sensitive generalisations more\nsuccessfully than sequential language models. However, their computational\ncomplexity renders scaling difficult, and it remains an open question whether\nstructural biases are still necessary when sequential models have access to\never larger amounts of training data. To answer this question, we introduce an\nefficient knowledge distillation (KD) technique that transfers knowledge from a\nsyntactic language model trained on a small corpus to an LSTM language model,\nhence enabling the LSTM to develop a more structurally sensitive representation\nof the larger training data it learns from. On targeted syntactic evaluations,\nwe find that, while sequential LSTMs perform much better than previously\nreported, our proposed technique substantially improves on this baseline,\nyielding a new state of the art. Our findings and analysis affirm the\nimportance of structural biases, even in models that learn from large amounts\nof data.\n", "first_author": "Adhiguna Kuncoro", "year_month": "2019-06"}, {"title": "REALM: Retrieval-Augmented Language Model Pre-Training", "published_date": "2020-02-10T18:40:59Z", "link": "http://arxiv.org/pdf/2002.08909v1", "abstract": "  Language model pre-training has been shown to capture a surprising amount of\nworld knowledge, crucial for NLP tasks such as question answering. However,\nthis knowledge is stored implicitly in the parameters of a neural network,\nrequiring ever-larger networks to cover more facts.\n  To capture knowledge in a more modular and interpretable way, we augment\nlanguage model pre-training with a latent knowledge retriever, which allows the\nmodel to retrieve and attend over documents from a large corpus such as\nWikipedia, used during pre-training, fine-tuning and inference. For the first\ntime, we show how to pre-train such a knowledge retriever in an unsupervised\nmanner, using masked language modeling as the learning signal and\nbackpropagating through a retrieval step that considers millions of documents.\n  We demonstrate the effectiveness of Retrieval-Augmented Language Model\npre-training (REALM) by fine-tuning on the challenging task of Open-domain\nQuestion Answering (Open-QA). We compare against state-of-the-art models for\nboth explicit and implicit knowledge storage on three popular Open-QA\nbenchmarks, and find that we outperform all previous methods by a significant\nmargin (4-16% absolute accuracy), while also providing qualitative benefits\nsuch as interpretability and modularity.\n", "first_author": "Kelvin Guu", "year_month": "2020-02"}, {"title": "Topical Language Generation using Transformers", "published_date": "2021-03-11T03:45:24Z", "link": "http://arxiv.org/pdf/2103.06434v1", "abstract": "  Large-scale transformer-based language models (LMs) demonstrate impressive\ncapabilities in open text generation. However, controlling the generated text's\nproperties such as the topic, style, and sentiment is challenging and often\nrequires significant changes to the model architecture or retraining and\nfine-tuning the model on new supervised data. This paper presents a novel\napproach for Topical Language Generation (TLG) by combining a pre-trained LM\nwith topic modeling information. We cast the problem using Bayesian probability\nformulation with topic probabilities as a prior, LM probabilities as the\nlikelihood, and topical language generation probability as the posterior. In\nlearning the model, we derive the topic probability distribution from the\nuser-provided document's natural structure. Furthermore, we extend our model by\nintroducing new parameters and functions to influence the quantity of the\ntopical features presented in the generated text. This feature would allow us\nto easily control the topical properties of the generated text. Our\nexperimental results demonstrate that our model outperforms the\nstate-of-the-art results on coherency, diversity, and fluency while being\nfaster in decoding.\n", "first_author": "Rohola Zandie", "year_month": "2021-03"}, {"title": "Multilingual Language Models Predict Human Reading Behavior", "published_date": "2021-04-12T13:03:49Z", "link": "http://arxiv.org/pdf/2104.05433v1", "abstract": "  We analyze if large language models are able to predict patterns of human\nreading behavior. We compare the performance of language-specific and\nmultilingual pretrained transformer models to predict reading time measures\nreflecting natural human sentence processing on Dutch, English, German, and\nRussian texts. This results in accurate models of human reading behavior, which\nindicates that transformer models implicitly encode relative importance in\nlanguage in a way that is comparable to human processing mechanisms. We find\nthat BERT and XLM models successfully predict a range of eye tracking features.\nIn a series of experiments, we analyze the cross-domain and cross-language\nabilities of these models and show how they reflect human sentence processing.\n", "first_author": "Nora Hollenstein", "year_month": "2021-04"}, {"title": "Structural Persistence in Language Models: Priming as a Window into\n  Abstract Language Representations", "published_date": "2021-09-30T10:38:38Z", "link": "http://arxiv.org/pdf/2109.14989v2", "abstract": "  We investigate the extent to which modern, neural language models are\nsusceptible to structural priming, the phenomenon whereby the structure of a\nsentence makes the same structure more probable in a follow-up sentence. We\nexplore how priming can be used to study the potential of these models to learn\nabstract structural information, which is a prerequisite for good performance\non tasks that require natural language understanding skills. We introduce a\nnovel metric and release Prime-LM, a large corpus where we control for various\nlinguistic factors which interact with priming strength. We find that\nTransformer models indeed show evidence of structural priming, but also that\nthe generalisations they learned are to some extent modulated by semantic\ninformation. Our experiments also show that the representations acquired by the\nmodels may not only encode abstract sequential structure but involve certain\nlevel of hierarchical syntactic information. More generally, our study shows\nthat the priming paradigm is a useful, additional tool for gaining insights\ninto the capacities of language models and opens the door to future\npriming-based investigations that probe the model's internal states.\n", "first_author": "Arabella Sinclair", "year_month": "2021-09"}, {"title": "IT5: Large-scale Text-to-text Pretraining for Italian Language\n  Understanding and Generation", "published_date": "2022-03-07T22:39:01Z", "link": "http://arxiv.org/pdf/2203.03759v1", "abstract": "  The T5 model and its unified text-to-text paradigm contributed in advancing\nthe state-of-the-art for many natural language processing tasks. While some\nmultilingual variants of the T5 model have recently been introduced, their\nperformances were found to provide suboptimal performances for languages other\nthan English if compared to monolingual variants. We are motivated by these\nfindings to introduce IT5, the first family of encoder-decoder transformer\nmodels pretrained specifically on Italian. We perform a thorough cleaning of a\nweb-crawled Italian corpus including more than 40 billion words and use it to\npretrain three IT5 models of different sizes. The performance of IT5 models and\ntheir multilingual counterparts is then evaluated on a broad range of natural\nlanguage understanding and generation benchmarks for Italian. We find the\nmonolingual IT5 models to provide the best scale-to-performance ratio across\ntested models, consistently outperforming their multilingual counterparts and\nsetting a new state-of-the-art for most Italian conditional language generation\ntasks.\n", "first_author": "Gabriele Sarti", "year_month": "2022-03"}, {"title": "Capturing the diversity of multilingual societies", "published_date": "2021-05-06T10:27:43Z", "link": "http://arxiv.org/pdf/2105.02570v4", "abstract": "  Cultural diversity encoded within languages of the world is at risk, as many\nlanguages have become endangered in the last decades in a context of growing\nglobalization. To preserve this diversity, it is first necessary to understand\nwhat drives language extinction, and which mechanisms might enable coexistence.\nHere, we study language shift mechanisms using theoretical and data-driven\nperspectives. A large-scale empirical analysis of multilingual societies using\nTwitter and census data yields a wide diversity of spatial patterns of language\ncoexistence. It ranges from a mixing of language speakers to segregation with\nmultilinguals on the boundaries of disjoint linguistic domains. To understand\nhow these different states can emerge and, especially, become stable, we\npropose a model in which language coexistence is reached when learning the\nother language is facilitated and when bilinguals favor the use of the\nendangered language. Simulations carried out in a metapopulation framework\nhighlight the importance of spatial interactions arising from people mobility\nto explain the stability of a mixed state or the presence of a boundary between\ntwo linguistic regions. Further, we find that the history of languages is\ncritical to understand their present state.\n", "first_author": "Thomas Louf", "year_month": "2021-05"}, {"title": "An Empirical Study of Developers' Discussions about Security Challenges\n  of Different Programming Languages", "published_date": "2021-07-29T03:19:52Z", "link": "http://arxiv.org/pdf/2107.13723v2", "abstract": "  Given programming languages can provide different types and levels of\nsecurity support, it is critically important to consider security aspects while\nselecting programming languages for developing software systems. Inadequate\nconsideration of security in the choice of a programming language may lead to\npotential ramifications for secure development. Whilst theoretical analysis of\nthe supposed security properties of different programming languages has been\nconducted, there has been relatively little effort to empirically explore the\nactual security challenges experienced by developers. We have performed a\nlarge-scale study of the security challenges of 15 programming languages by\nquantitatively and qualitatively analysing the developers' discussions from\nStack Overflow and GitHub. By leveraging topic modelling, we have derived a\ntaxonomy of 18 major security challenges for 6 topic categories. We have also\nconducted comparative analysis to understand how the identified challenges vary\nregarding the different programming languages and data sources. Our findings\nsuggest that the challenges and their characteristics differ substantially for\ndifferent programming languages and data sources, i.e., Stack Overflow and\nGitHub. The findings provide evidence-based insights and understanding of\nsecurity challenges related to different programming languages to software\nprofessionals (i.e., practitioners or researchers). The reported taxonomy of\nsecurity challenges can assist both practitioners and researchers in better\nunderstanding and traversing the secure development landscape. This study\nhighlights the importance of the choice of technology, e.g., programming\nlanguage, in secure software engineering. Hence, the findings are expected to\nmotivate practitioners to consider the potential impact of the choice of\nprogramming languages on software security.\n", "first_author": "Roland Croft", "year_month": "2021-07"}, {"title": "VAuLT: Augmenting the Vision-and-Language Transformer for Sentiment\n  Classification on Social Media", "published_date": "2022-08-18T18:51:13Z", "link": "http://arxiv.org/pdf/2208.09021v3", "abstract": "  We propose the Vision-and-Augmented-Language Transformer (VAuLT). VAuLT is an\nextension of the popular Vision-and-Language Transformer (ViLT), and improves\nperformance on vision-and-language (VL) tasks that involve more complex text\ninputs than image captions while having minimal impact on training and\ninference efficiency. ViLT, importantly, enables efficient training and\ninference in VL tasks, achieved by encoding images using a linear projection of\npatches instead of an object detector. However, it is pretrained on captioning\ndatasets, where the language input is simple, literal, and descriptive,\ntherefore lacking linguistic diversity. So, when working with multimedia data\nin the wild, such as multimodal social media data, there is a notable shift\nfrom captioning language data, as well as diversity of tasks. We indeed find\nevidence that the language capacity of ViLT is lacking. The key insight and\nnovelty of VAuLT is to propagate the output representations of a large language\nmodel (LM) like BERT to the language input of ViLT. We show that joint training\nof the LM and ViLT can yield relative improvements up to 20% over ViLT and\nachieve state-of-the-art or comparable performance on VL tasks involving richer\nlanguage inputs and affective constructs, such as for Target-Oriented Sentiment\nClassification in TWITTER-2015 and TWITTER-2017, and Sentiment Classification\nin MVSA-Single and MVSA-Multiple. Our code is available at\nhttps://github.com/gchochla/VAuLT.\n", "first_author": "Georgios Chochlakis", "year_month": "2022-08"}, {"title": "Diversity and Language Technology: How Techno-Linguistic Bias Can Cause\n  Epistemic Injustice", "published_date": "2023-07-25T16:08:27Z", "link": "http://arxiv.org/pdf/2307.13714v1", "abstract": "  It is well known that AI-based language technology -- large language models,\nmachine translation systems, multilingual dictionaries, and corpora -- is\ncurrently limited to 2 to 3 percent of the world's most widely spoken and/or\nfinancially and politically best supported languages. In response, recent\nresearch efforts have sought to extend the reach of AI technology to\n``underserved languages.'' In this paper, we show that many of these attempts\nproduce flawed solutions that adhere to a hard-wired representational\npreference for certain languages, which we call techno-linguistic bias.\nTechno-linguistic bias is distinct from the well-established phenomenon of\nlinguistic bias as it does not concern the languages represented but rather the\ndesign of the technologies. As we show through the paper, techno-linguistic\nbias can result in systems that can only express concepts that are part of the\nlanguage and culture of dominant powers, unable to correctly represent concepts\nfrom other communities. We argue that at the root of this problem lies a\nsystematic tendency of technology developer communities to apply a simplistic\nunderstanding of diversity which does not do justice to the more profound\ndifferences that languages, and ultimately the communities that speak them,\nembody. Drawing on the concept of epistemic injustice, we point to the broader\nsociopolitical consequences of the bias we identify and show how it can lead\nnot only to a disregard for valuable aspects of diversity but also to an\nunder-representation of the needs and diverse worldviews of marginalized\nlanguage communities.\n", "first_author": "Paula Helm", "year_month": "2023-07"}, {"title": "ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language\n  Models via Efficient Large-Batch Adversarial Noise", "published_date": "2022-01-29T01:47:01Z", "link": "http://arxiv.org/pdf/2201.12469v1", "abstract": "  In recent years, large pre-trained Transformer-based language models have led\nto dramatic improvements in many natural language understanding tasks. To train\nthese models with increasing sizes, many neural network practitioners attempt\nto increase the batch sizes in order to leverage multiple GPUs to improve\ntraining speed. However, increasing the batch size often makes the optimization\nmore difficult, leading to slow convergence or poor generalization that can\nrequire orders of magnitude more training time to achieve the same model\nquality. In this paper, we explore the steepness of the loss landscape of\nlarge-batch optimization for adapting pre-trained Transformer-based language\nmodels to domain-specific tasks and find that it tends to be highly complex and\nirregular, posing challenges to generalization on downstream tasks.\n  To tackle this challenge, we propose ScaLA, a novel and efficient method to\naccelerate the adaptation speed of pre-trained transformer networks. Different\nfrom prior methods, we take a sequential game-theoretic approach by adding\nlightweight adversarial noise into large-batch optimization, which\nsignificantly improves adaptation speed while preserving model generalization.\nExperiment results show that ScaLA attains 2.7--9.8$\\times$ adaptation speedups\nover the baseline for GLUE on BERT-base and RoBERTa-large, while achieving\ncomparable and sometimes higher accuracy than the state-of-the-art large-batch\noptimization methods. Finally, we also address the theoretical aspect of\nlarge-batch optimization with adversarial noise and provide a theoretical\nconvergence rate analysis for ScaLA using techniques for analyzing non-convex\nsaddle-point problems.\n", "first_author": "Minjia Zhang", "year_month": "2022-01"}, {"title": "RSGPT: A Remote Sensing Vision Language Model and Benchmark", "published_date": "2023-07-28T02:23:35Z", "link": "http://arxiv.org/pdf/2307.15266v1", "abstract": "  The emergence of large-scale large language models, with GPT-4 as a prominent\nexample, has significantly propelled the rapid advancement of artificial\ngeneral intelligence and sparked the revolution of Artificial Intelligence 2.0.\nIn the realm of remote sensing (RS), there is a growing interest in developing\nlarge vision language models (VLMs) specifically tailored for data analysis in\nthis domain. However, current research predominantly revolves around visual\nrecognition tasks, lacking comprehensive, large-scale image-text datasets that\nare aligned and suitable for training large VLMs, which poses significant\nchallenges to effectively training such models for RS applications. In computer\nvision, recent research has demonstrated that fine-tuning large vision language\nmodels on small-scale, high-quality datasets can yield impressive performance\nin visual and language understanding. These results are comparable to\nstate-of-the-art VLMs trained from scratch on massive amounts of data, such as\nGPT-4. Inspired by this captivating idea, in this work, we build a high-quality\nRemote Sensing Image Captioning dataset (RSICap) that facilitates the\ndevelopment of large VLMs in the RS field. Unlike previous RS datasets that\neither employ model-generated captions or short descriptions, RSICap comprises\n2,585 human-annotated captions with rich and high-quality information. This\ndataset offers detailed descriptions for each image, encompassing scene\ndescriptions (e.g., residential area, airport, or farmland) as well as object\ninformation (e.g., color, shape, quantity, absolute position, etc). To\nfacilitate the evaluation of VLMs in the field of RS, we also provide a\nbenchmark evaluation dataset called RSIEval. This dataset consists of\nhuman-annotated captions and visual question-answer pairs, allowing for a\ncomprehensive assessment of VLMs in the context of RS.\n", "first_author": "Yuan Hu", "year_month": "2023-07"}, {"title": "Evaluating Contextualized Language Models for Hungarian", "published_date": "2021-02-22T09:29:01Z", "link": "http://arxiv.org/pdf/2102.10848v1", "abstract": "  We present an extended comparison of contextualized language models for\nHungarian. We compare huBERT, a Hungarian model against 4 multilingual models\nincluding the multilingual BERT model. We evaluate these models through three\ntasks, morphological probing, POS tagging and NER. We find that huBERT works\nbetter than the other models, often by a large margin, particularly near the\nglobal optimum (typically at the middle layers). We also find that huBERT tends\nto generate fewer subwords for one word and that using the last subword for\ntoken-level tasks is generally a better choice than using the first one.\n", "first_author": "Judit \u00c1cs", "year_month": "2021-02"}, {"title": "Hybrid Autoregressive Transducer (hat)", "published_date": "2020-03-12T20:47:06Z", "link": "http://arxiv.org/pdf/2003.07705v1", "abstract": "  This paper proposes and evaluates the hybrid autoregressive transducer (HAT)\nmodel, a time-synchronous encoderdecoder model that preserves the modularity of\nconventional automatic speech recognition systems. The HAT model provides a way\nto measure the quality of the internal language model that can be used to\ndecide whether inference with an external language model is beneficial or not.\nThis article also presents a finite context version of the HAT model that\naddresses the exposure bias problem and significantly simplifies the overall\ntraining and inference. We evaluate our proposed model on a large-scale voice\nsearch task. Our experiments show significant improvements in WER compared to\nthe state-of-the-art approaches.\n", "first_author": "Ehsan Variani", "year_month": "2020-03"}, {"title": "Large-scale Cloze Test Dataset Created by Teachers", "published_date": "2017-11-09T01:41:12Z", "link": "http://arxiv.org/pdf/1711.03225v3", "abstract": "  Cloze tests are widely adopted in language exams to evaluate students'\nlanguage proficiency. In this paper, we propose the first large-scale\nhuman-created cloze test dataset CLOTH, containing questions used in\nmiddle-school and high-school language exams. With missing blanks carefully\ncreated by teachers and candidate choices purposely designed to be nuanced,\nCLOTH requires a deeper language understanding and a wider attention span than\npreviously automatically-generated cloze datasets. We test the performance of\ndedicatedly designed baseline models including a language model trained on the\nOne Billion Word Corpus and show humans outperform them by a significant\nmargin. We investigate the source of the performance gap, trace model\ndeficiencies to some distinct properties of CLOTH, and identify the limited\nability of comprehending the long-term context to be the key bottleneck.\n", "first_author": "Qizhe Xie", "year_month": "2017-11"}, {"title": "A Bibliometric Review of Large Language Models Research from 2017 to\n  2023", "published_date": "2023-04-03T21:46:41Z", "link": "http://arxiv.org/pdf/2304.02020v1", "abstract": "  Large language models (LLMs) are a class of language models that have\ndemonstrated outstanding performance across a range of natural language\nprocessing (NLP) tasks and have become a highly sought-after research area,\nbecause of their ability to generate human-like language and their potential to\nrevolutionize science and technology. In this study, we conduct bibliometric\nand discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000\npublications, this paper serves as a roadmap for researchers, practitioners,\nand policymakers to navigate the current landscape of LLMs research. We present\nthe research trends from 2017 to early 2023, identifying patterns in research\nparadigms and collaborations. We start with analyzing the core algorithm\ndevelopments and NLP tasks that are fundamental in LLMs research. We then\ninvestigate the applications of LLMs in various fields and domains including\nmedicine, engineering, social science, and humanities. Our review also reveals\nthe dynamic, fast-paced evolution of LLMs research. Overall, this paper offers\nvaluable insights into the current state, impact, and potential of LLMs\nresearch and its applications.\n", "first_author": "Lizhou Fan", "year_month": "2023-04"}, {"title": "Using Language Models For Knowledge Acquisition in Natural Language\n  Reasoning Problems", "published_date": "2023-04-04T13:01:48Z", "link": "http://arxiv.org/pdf/2304.01771v1", "abstract": "  For a natural language problem that requires some non-trivial reasoning to\nsolve, there are at least two ways to do it using a large language model (LLM).\nOne is to ask it to solve it directly. The other is to use it to extract the\nfacts from the problem text and then use a theorem prover to solve it. In this\nnote, we compare the two methods using ChatGPT and GPT4 on a series of logic\nword puzzles, and conclude that the latter is the right approach.\n", "first_author": "Fangzhen Lin", "year_month": "2023-04"}, {"title": "AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural\n  Language Processing", "published_date": "2023-06-11T22:55:18Z", "link": "http://arxiv.org/pdf/2306.06800v1", "abstract": "  Developing monolingual large Pre-trained Language Models (PLMs) is shown to\nbe very successful in handling different tasks in Natural Language Processing\n(NLP). In this work, we present AraMUS, the largest Arabic PLM with 11B\nparameters trained on 529GB of high-quality Arabic textual data. AraMUS\nachieves state-of-the-art performances on a diverse set of Arabic\nclassification and generative tasks. Moreover, AraMUS shows impressive few-shot\nlearning abilities compared with the best existing Arabic PLMs.\n", "first_author": "Asaad Alghamdi", "year_month": "2023-06"}, {"title": "OneAligner: Zero-shot Cross-lingual Transfer with One Rich-Resource\n  Language Pair for Low-Resource Sentence Retrieval", "published_date": "2022-05-17T19:52:42Z", "link": "http://arxiv.org/pdf/2205.08605v1", "abstract": "  Aligning parallel sentences in multilingual corpora is essential to curating\ndata for downstream applications such as Machine Translation. In this work, we\npresent OneAligner, an alignment model specially designed for sentence\nretrieval tasks. This model is able to train on only one language pair and\ntransfers, in a cross-lingual fashion, to low-resource language pairs with\nnegligible degradation in performance. When trained with all language pairs of\na large-scale parallel multilingual corpus (OPUS-100), this model achieves the\nstate-of-the-art result on the Tateoba dataset, outperforming an equally-sized\nprevious model by 8.0 points in accuracy while using less than 0.6% of their\nparallel data. When finetuned on a single rich-resource language pair, be it\nEnglish-centered or not, our model is able to match the performance of the ones\nfinetuned on all language pairs under the same data budget with less than 2.0\npoints decrease in accuracy. Furthermore, with the same setup, scaling up the\nnumber of rich-resource language pairs monotonically improves the performance,\nreaching a minimum of 0.4 points discrepancy in accuracy, making it less\nmandatory to collect any low-resource parallel data. Finally, we conclude\nthrough empirical results and analyses that the performance of the sentence\nalignment task depends mostly on the monolingual and parallel data size, up to\na certain size threshold, rather than on what language pairs are used for\ntraining or evaluation.\n", "first_author": "Tong Niu", "year_month": "2022-05"}, {"title": "Benchmarking Arabic AI with Large Language Models", "published_date": "2023-05-24T10:16:16Z", "link": "http://arxiv.org/pdf/2305.14982v1", "abstract": "  With large Foundation Models (FMs), language technologies (AI in general) are\nentering a new paradigm: eliminating the need for developing large-scale\ntask-specific datasets and supporting a variety of tasks through set-ups\nranging from zero-shot to few-shot learning. However, understanding FMs\ncapabilities requires a systematic benchmarking effort by comparing FMs\nperformance with the state-of-the-art (SOTA) task-specific models. With that\ngoal, past work focused on the English language and included a few efforts with\nmultiple languages. Our study contributes to ongoing research by evaluating FMs\nperformance for standard Arabic NLP and Speech processing, including a range of\ntasks from sequence tagging to content classification across diverse domains.\nWe start with zero-shot learning using GPT-3.5-turbo, Whisper, and USM,\naddressing 33 unique tasks using 59 publicly available datasets resulting in 96\ntest setups. For a few tasks, FMs performs on par or exceeds the performance of\nthe SOTA models but for the majority it under-performs. Given the importance of\nprompt for the FMs performance, we discuss our prompt strategies in detail and\nelaborate on our findings. Our future work on Arabic AI will explore few-shot\nprompting, expand the range of tasks, and investigate additional open-source\nmodels.\n", "first_author": "Ahmed Abdelali", "year_month": "2023-05"}, {"title": "Estimating the Personality of White-Box Language Models", "published_date": "2022-04-25T23:53:53Z", "link": "http://arxiv.org/pdf/2204.12000v2", "abstract": "  Technology for open-ended language generation, a key application of\nartificial intelligence, has advanced to a great extent in recent years.\nLarge-scale language models, which are trained on large corpora of text, are\nbeing used in a wide range of applications everywhere, from virtual assistants\nto conversational bots. While these language models output fluent text,\nexisting research shows that these models can and do capture human biases. Many\nof these biases, especially those that could potentially cause harm, are being\nwell-investigated. On the other hand, studies that infer and change human\npersonality traits inherited by these models have been scarce or non-existent.\nOur work seeks to address this gap by exploring the personality traits of\nseveral large-scale language models designed for open-ended text generation and\nthe datasets used for training them. We build on the popular Big Five factors\nand develop robust methods that quantify the personality traits of these models\nand their underlying datasets. In particular, we trigger the models with a\nquestionnaire designed for personality assessment and subsequently classify the\ntext responses into quantifiable traits using a Zero-shot classifier. Our\nestimation scheme sheds light on an important anthropomorphic element found in\nsuch AI models and can help stakeholders decide how they should be applied as\nwell as how society could perceive them. Additionally, we examined approaches\nto alter these personalities, adding to our understanding of how AI models can\nbe adapted to specific contexts.\n", "first_author": "Saketh Reddy Karra", "year_month": "2022-04"}, {"title": "Phrase-Level Class based Language Model for Mandarin Smart Speaker Query\n  Recognition", "published_date": "2019-09-02T05:55:36Z", "link": "http://arxiv.org/pdf/1909.00556v1", "abstract": "  The success of speech assistants requires precise recognition of a number of\nentities on particular contexts. A common solution is to train a class-based\nn-gram language model and then expand the classes into specific words or\nphrases. However, when the class has a huge list, e.g., more than 20 million\nsongs, a fully expansion will cause memory explosion. Worse still, the list\nitems in the class need to be updated frequently, which requires a dynamic\nmodel updating technique. In this work, we propose to train pruned language\nmodels for the word classes to replace the slots in the root n-gram. We further\npropose to use a novel technique, named Difference Language Model (DLM), to\ncorrect the bias from the pruned language models. Once the decoding graph is\nbuilt, we only need to recalculate the DLM when the entities in word classes\nare updated. Results show that the proposed method consistently and\nsignificantly outperforms the conventional approaches on all datasets, esp. for\nlarge lists, which the conventional approaches cannot handle.\n", "first_author": "Yiheng Huang", "year_month": "2019-09"}, {"title": "The Turking Test: Can Language Models Understand Instructions?", "published_date": "2020-10-22T18:44:16Z", "link": "http://arxiv.org/pdf/2010.11982v1", "abstract": "  Supervised machine learning provides the learner with a set of input-output\nexamples of the target task. Humans, however, can also learn to perform new\ntasks from instructions in natural language. Can machines learn to understand\ninstructions as well? We present the Turking Test, which examines a model's\nability to follow natural language instructions of varying complexity. These\nrange from simple tasks, like retrieving the nth word of a sentence, to ones\nthat require creativity, such as generating examples for SNLI and SQuAD in\nplace of human intelligence workers (\"turkers\"). Despite our lenient evaluation\nmethodology, we observe that a large pretrained language model performs poorly\nacross all tasks. Analyzing the model's error patterns reveals that the model\ntends to ignore explicit instructions and often generates outputs that cannot\nbe construed as an attempt to solve the task. While it is not yet clear whether\ninstruction understanding can be captured by traditional language models, the\nsheer expressivity of instruction understanding makes it an appealing\nalternative to the rising few-shot inference paradigm.\n", "first_author": "Avia Efrat", "year_month": "2020-10"}, {"title": "Variable Name Recovery in Decompiled Binary Code using Constrained\n  Masked Language Modeling", "published_date": "2021-03-23T19:09:22Z", "link": "http://arxiv.org/pdf/2103.12801v1", "abstract": "  Decompilation is the procedure of transforming binary programs into a\nhigh-level representation, such as source code, for human analysts to examine.\nWhile modern decompilers can reconstruct and recover much information that is\ndiscarded during compilation, inferring variable names is still extremely\ndifficult. Inspired by recent advances in natural language processing, we\npropose a novel solution to infer variable names in decompiled code based on\nMasked Language Modeling, Byte-Pair Encoding, and neural architectures such as\nTransformers and BERT. Our solution takes \\textit{raw} decompiler output, the\nless semantically meaningful code, as input, and enriches it using our proposed\n\\textit{finetuning} technique, Constrained Masked Language Modeling. Using\nConstrained Masked Language Modeling introduces the challenge of predicting the\nnumber of masked tokens for the original variable name. We address this\n\\textit{count of token prediction} challenge with our post-processing\nalgorithm. Compared to the state-of-the-art approaches, our trained VarBERT\nmodel is simpler and of much better performance. We evaluated our model on an\nexisting large-scale data set with 164,632 binaries and showed that it can\npredict variable names identical to the ones present in the original source\ncode up to 84.15\\% of the time.\n", "first_author": "Pratyay Banerjee", "year_month": "2021-03"}, {"title": "LAWDR: Language-Agnostic Weighted Document Representations from\n  Pre-trained Models", "published_date": "2021-06-07T07:14:00Z", "link": "http://arxiv.org/pdf/2106.03379v1", "abstract": "  Cross-lingual document representations enable language understanding in\nmultilingual contexts and allow transfer learning from high-resource to\nlow-resource languages at the document level. Recently large pre-trained\nlanguage models such as BERT, XLM and XLM-RoBERTa have achieved great success\nwhen fine-tuned on sentence-level downstream tasks. It is tempting to apply\nthese cross-lingual models to document representation learning. However, there\nare two challenges: (1) these models impose high costs on long document\nprocessing and thus many of them have strict length limit; (2) model\nfine-tuning requires extra data and computational resources, which is not\npractical in resource-limited settings. In this work, we address these\nchallenges by proposing unsupervised Language-Agnostic Weighted Document\nRepresentations (LAWDR). We study the geometry of pre-trained sentence\nembeddings and leverage it to derive document representations without\nfine-tuning. Evaluated on cross-lingual document alignment, LAWDR demonstrates\ncomparable performance to state-of-the-art models on benchmark datasets.\n", "first_author": "Hongyu Gong", "year_month": "2021-06"}, {"title": "PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding\n  from Language Models", "published_date": "2021-09-10T20:14:08Z", "link": "http://arxiv.org/pdf/2109.05093v1", "abstract": "  Large pre-trained language models for textual data have an unconstrained\noutput space; at each decoding step, they can produce any of 10,000s of\nsub-word tokens. When fine-tuned to target constrained formal languages like\nSQL, these models often generate invalid code, rendering it unusable. We\npropose PICARD (code and trained models available at\nhttps://github.com/ElementAI/picard), a method for constraining auto-regressive\ndecoders of language models through incremental parsing. PICARD helps to find\nvalid output sequences by rejecting inadmissible tokens at each decoding step.\nOn the challenging Spider and CoSQL text-to-SQL translation tasks, we show that\nPICARD transforms fine-tuned T5 models with passable performance into\nstate-of-the-art solutions.\n", "first_author": "Torsten Scholak", "year_month": "2021-09"}, {"title": "Exploring Versatile Generative Language Model Via Parameter-Efficient\n  Transfer Learning", "published_date": "2020-04-08T06:18:44Z", "link": "http://arxiv.org/pdf/2004.03829v2", "abstract": "  Fine-tuning pre-trained generative language models to down-stream language\ngeneration tasks has shown promising results. However, this comes with the cost\nof having a single, large model for each task, which is not ideal in\nlow-memory/power scenarios (e.g., mobile). In this paper, we propose an\neffective way to fine-tune multiple down-stream generation tasks simultaneously\nusing a single, large pre-trained model. The experiments on five diverse\nlanguage generation tasks show that by just using an additional 2-3% parameters\nfor each task, our model can maintain or even improve the performance of\nfine-tuning the whole model.\n", "first_author": "Zhaojiang Lin", "year_month": "2020-04"}, {"title": "Discovering Useful Sentence Representations from Large Pretrained\n  Language Models", "published_date": "2020-08-20T16:03:51Z", "link": "http://arxiv.org/pdf/2008.09049v1", "abstract": "  Despite the extensive success of pretrained language models as encoders for\nbuilding NLP systems, they haven't seen prominence as decoders for sequence\ngeneration tasks. We explore the question of whether these models can be\nadapted to be used as universal decoders. To be considered \"universal,\" a\ndecoder must have an implicit representation for any target sentence $s$, such\nthat it can recover that sentence exactly when conditioned on its\nrepresentation. For large transformer-based language models trained on vast\namounts of English text, we investigate whether such representations can be\neasily discovered using standard optimization methods. We present and compare\nthree representation injection techniques for transformer-based models and\nthree accompanying methods which map sentences to and from this representation\nspace. Experiments show that not only do representations exist for sentences\nfrom a variety of genres. More importantly, without needing complex\noptimization algorithms, our methods recover these sentences almost perfectly\nwithout fine-tuning the underlying language model at all.\n", "first_author": "Nishant Subramani", "year_month": "2020-08"}, {"title": "Unsupervised Paraphrasing with Pretrained Language Models", "published_date": "2020-10-24T11:55:28Z", "link": "http://arxiv.org/pdf/2010.12885v2", "abstract": "  Paraphrase generation has benefited extensively from recent progress in the\ndesigning of training objectives and model architectures. However, previous\nexplorations have largely focused on supervised methods, which require a large\namount of labeled data that is costly to collect. To address this drawback, we\nadopt a transfer learning approach and propose a training pipeline that enables\npre-trained language models to generate high-quality paraphrases in an\nunsupervised setting. Our recipe consists of task-adaptation, self-supervision,\nand a novel decoding algorithm named Dynamic Blocking (DB). To enforce a\nsurface form dissimilar from the input, whenever the language model emits a\ntoken contained in the source sequence, DB prevents the model from outputting\nthe subsequent source token for the next generation step. We show with\nautomatic and human evaluations that our approach achieves state-of-the-art\nperformance on both the Quora Question Pair (QQP) and the ParaNMT datasets and\nis robust to domain shift between the two datasets of distinct distributions.\nWe also demonstrate that our model transfers to paraphrasing in other languages\nwithout any additional finetuning.\n", "first_author": "Tong Niu", "year_month": "2020-10"}, {"title": "DaCy: A Unified Framework for Danish NLP", "published_date": "2021-07-12T10:14:31Z", "link": "http://arxiv.org/pdf/2107.05295v1", "abstract": "  Danish natural language processing (NLP) has in recent years obtained\nconsiderable improvements with the addition of multiple new datasets and\nmodels. However, at present, there is no coherent framework for applying\nstate-of-the-art models for Danish. We present DaCy: a unified framework for\nDanish NLP built on SpaCy. DaCy uses efficient multitask models which obtain\nstate-of-the-art performance on named entity recognition, part-of-speech\ntagging, and dependency parsing. DaCy contains tools for easy integration of\nexisting models such as for polarity, emotion, or subjectivity detection. In\naddition, we conduct a series of tests for biases and robustness of Danish NLP\npipelines through augmentation of the test set of DaNE. DaCy large compares\nfavorably and is especially robust to long input lengths and spelling\nvariations and errors. All models except DaCy large display significant biases\nrelated to ethnicity while only Polyglot shows a significant gender bias. We\nargue that for languages with limited benchmark sets, data augmentation can be\nparticularly useful for obtaining more realistic and fine-grained performance\nestimates. We provide a series of augmenters as a first step towards a more\nthorough evaluation of language models for low and medium resource languages\nand encourage further development.\n", "first_author": "Kenneth Enevoldsen", "year_month": "2021-07"}, {"title": "Sentence Bottleneck Autoencoders from Transformer Language Models", "published_date": "2021-08-31T19:39:55Z", "link": "http://arxiv.org/pdf/2109.00055v2", "abstract": "  Representation learning for text via pretraining a language model on a large\ncorpus has become a standard starting point for building NLP systems. This\napproach stands in contrast to autoencoders, also trained on raw text, but with\nthe objective of learning to encode each input as a vector that allows full\nreconstruction. Autoencoders are attractive because of their latent space\nstructure and generative properties. We therefore explore the construction of a\nsentence-level autoencoder from a pretrained, frozen transformer language\nmodel. We adapt the masked language modeling objective as a generative,\ndenoising one, while only training a sentence bottleneck and a single-layer\nmodified transformer decoder. We demonstrate that the sentence representations\ndiscovered by our model achieve better quality than previous methods that\nextract representations from pretrained transformers on text similarity tasks,\nstyle transfer (an example of controlled generation), and single-sentence\nclassification tasks in the GLUE benchmark, while using fewer parameters than\nlarge pretrained models.\n", "first_author": "Ivan Montero", "year_month": "2021-08"}, {"title": "Submix: Practical Private Prediction for Large-Scale Language Models", "published_date": "2022-01-04T04:23:38Z", "link": "http://arxiv.org/pdf/2201.00971v1", "abstract": "  Recent data-extraction attacks have exposed that language models can memorize\nsome training samples verbatim. This is a vulnerability that can compromise the\nprivacy of the model's training data. In this work, we introduce SubMix: a\npractical protocol for private next-token prediction designed to prevent\nprivacy violations by language models that were fine-tuned on a private corpus\nafter pre-training on a public corpus. We show that SubMix limits the leakage\nof information that is unique to any individual user in the private corpus via\na relaxation of group differentially private prediction. Importantly, SubMix\nadmits a tight, data-dependent privacy accounting mechanism, which allows it to\nthwart existing data-extraction attacks while maintaining the utility of the\nlanguage model. SubMix is the first protocol that maintains privacy even when\npublicly releasing tens of thousands of next-token predictions made by large\ntransformer-based models such as GPT-2.\n", "first_author": "Antonio Ginart", "year_month": "2022-01"}, {"title": "Exploring Length Generalization in Large Language Models", "published_date": "2022-07-11T14:24:38Z", "link": "http://arxiv.org/pdf/2207.04901v2", "abstract": "  The ability to extrapolate from short problem instances to longer ones is an\nimportant form of out-of-distribution generalization in reasoning tasks, and is\ncrucial when learning from datasets where longer problem instances are rare.\nThese include theorem proving, solving quantitative mathematics problems, and\nreading/summarizing novels. In this paper, we run careful empirical studies\nexploring the length generalization capabilities of transformer-based language\nmodels. We first establish that naively finetuning transformers on length\ngeneralization tasks shows significant generalization deficiencies independent\nof model scale. We then show that combining pretrained large language models'\nin-context learning abilities with scratchpad prompting (asking the model to\noutput solution steps before producing an answer) results in a dramatic\nimprovement in length generalization. We run careful failure analyses on each\nof the learning modalities and identify common sources of mistakes that\nhighlight opportunities in equipping language models with the ability to\ngeneralize to longer problems.\n", "first_author": "Cem Anil", "year_month": "2022-07"}, {"title": "A Watermark for Large Language Models", "published_date": "2023-01-24T18:52:59Z", "link": "http://arxiv.org/pdf/2301.10226v3", "abstract": "  Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\nbefore a word is generated, and then softly promoting use of green tokens\nduring sampling. We propose a statistical test for detecting the watermark with\ninterpretable p-values, and derive an information-theoretic framework for\nanalyzing the sensitivity of the watermark. We test the watermark using a\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\nfamily, and discuss robustness and security.\n", "first_author": "John Kirchenbauer", "year_month": "2023-01"}, {"title": "Leveraging Label Variation in Large Language Models for Zero-Shot Text\n  Classification", "published_date": "2023-07-24T17:49:31Z", "link": "http://arxiv.org/pdf/2307.12973v1", "abstract": "  The zero-shot learning capabilities of large language models (LLMs) make them\nideal for text classification without annotation or supervised training. Many\nstudies have shown impressive results across multiple tasks. While tasks, data,\nand results differ widely, their similarities to human annotation can aid us in\ntackling new tasks with minimal expenses. We evaluate using 5 state-of-the-art\nLLMs as \"annotators\" on 5 different tasks (age, gender, topic, sentiment\nprediction, and hate speech detection), across 4 languages: English, French,\nGerman, and Spanish. No single model excels at all tasks, across languages, or\nacross all labels within a task. However, aggregation techniques designed for\nhuman annotators perform substantially better than any one individual model.\nOverall, though, LLMs do not rival even simple supervised models, so they do\nnot (yet) replace the need for human annotation. We also discuss the tradeoffs\nbetween speed, accuracy, cost, and bias when it comes to aggregated model\nlabeling versus human annotation.\n", "first_author": "Flor Miriam Plaza-del-Arco", "year_month": "2023-07"}, {"title": "Post-Training Dialogue Summarization using Pseudo-Paraphrasing", "published_date": "2022-04-28T13:42:19Z", "link": "http://arxiv.org/pdf/2204.13498v1", "abstract": "  Previous dialogue summarization techniques adapt large language models\npretrained on the narrative text by injecting dialogue-specific features into\nthe models. These features either require additional knowledge to recognize or\nmake the resulting models harder to tune. To bridge the format gap between\ndialogues and narrative summaries in dialogue summarization tasks, we propose\nto post-train pretrained language models (PLMs) to rephrase from dialogue to\nnarratives. After that, the model is fine-tuned for dialogue summarization as\nusual. Comprehensive experiments show that our approach significantly improves\nvanilla PLMs on dialogue summarization and outperforms other SOTA models by the\nsummary quality and implementation costs.\n", "first_author": "Qi Jia", "year_month": "2022-04"}, {"title": "Short-answer scoring with ensembles of pretrained language models", "published_date": "2022-02-23T15:12:20Z", "link": "http://arxiv.org/pdf/2202.11558v1", "abstract": "  We investigate the effectiveness of ensembles of pretrained transformer-based\nlanguage models on short answer questions using the Kaggle Automated Short\nAnswer Scoring dataset. We fine-tune a collection of popular small, base, and\nlarge pretrained transformer-based language models, and train one feature-base\nmodel on the dataset with the aim of testing ensembles of these models. We used\nan early stopping mechanism and hyperparameter optimization in training. We\nobserve that generally that the larger models perform slightly better, however,\nthey still fall short of state-of-the-art results one their own. Once we\nconsider ensembles of models, there are ensembles of a number of large networks\nthat do produce state-of-the-art results, however, these ensembles are too\nlarge to realistically be put in a production environment.\n", "first_author": "Christopher Ormerod", "year_month": "2022-02"}, {"title": "Enhancing Knowledge Graph Construction Using Large Language Models", "published_date": "2023-05-08T12:53:06Z", "link": "http://arxiv.org/pdf/2305.04676v1", "abstract": "  The growing trend of Large Language Models (LLM) development has attracted\nsignificant attention, with models for various applications emerging\nconsistently. However, the combined application of Large Language Models with\nsemantic technologies for reasoning and inference is still a challenging task.\nThis paper analyzes how the current advances in foundational LLM, like ChatGPT,\ncan be compared with the specialized pretrained models, like REBEL, for joint\nentity and relation extraction. To evaluate this approach, we conducted several\nexperiments using sustainability-related text as our use case. We created\npipelines for the automatic creation of Knowledge Graphs from raw texts, and\nour findings indicate that using advanced LLM models can improve the accuracy\nof the process of creating these graphs from unstructured text. Furthermore, we\nexplored the potential of automatic ontology creation using foundation LLM\nmodels, which resulted in even more relevant and accurate knowledge graphs.\n", "first_author": "Milena Trajanoska", "year_month": "2023-05"}, {"title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "published_date": "2023-06-14T15:18:48Z", "link": "http://arxiv.org/pdf/2306.08568v1", "abstract": "  Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM\n", "first_author": "Ziyang Luo", "year_month": "2023-06"}, {"title": "MiQA: A Benchmark for Inference on Metaphorical Questions", "published_date": "2022-10-14T17:46:05Z", "link": "http://arxiv.org/pdf/2210.07993v1", "abstract": "  We propose a benchmark to assess the capability of large language models to\nreason with conventional metaphors. Our benchmark combines the previously\nisolated topics of metaphor detection and commonsense reasoning into a single\ntask that requires a model to make inferences by accurately selecting between\nthe literal and metaphorical register. We examine the performance of\nstate-of-the-art pre-trained models on binary-choice tasks and find a large\ndiscrepancy between the performance of small and very large models, going from\nchance to near-human level. We also analyse the largest model in a generative\nsetting and find that although human performance is approached, careful\nmultiple-shot prompting is required.\n", "first_author": "Iulia-Maria Comsa", "year_month": "2022-10"}, {"title": "Compressing Neural Language Models by Sparse Word Representations", "published_date": "2016-10-13T06:55:54Z", "link": "http://arxiv.org/pdf/1610.03950v1", "abstract": "  Neural networks are among the state-of-the-art techniques for language\nmodeling. Existing neural language models typically map discrete words to\ndistributed, dense vector representations. After information processing of the\npreceding context words by hidden layers, an output layer estimates the\nprobability of the next word. Such approaches are time- and memory-intensive\nbecause of the large numbers of parameters for word embeddings and the output\nlayer. In this paper, we propose to compress neural language models by sparse\nword representations. In the experiments, the number of parameters in our model\nincreases very slowly with the growth of the vocabulary size, which is almost\nimperceptible. Moreover, our approach not only reduces the parameter space to a\nlarge extent, but also improves the performance in terms of the perplexity\nmeasure.\n", "first_author": "Yunchuan Chen", "year_month": "2016-10"}, {"title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models", "published_date": "2022-02-14T08:20:15Z", "link": "http://arxiv.org/pdf/2202.06539v3", "abstract": "  Past work has shown that large language models are susceptible to privacy\nattacks, where adversaries generate sequences from a trained model and detect\nwhich sequences are memorized from the training set. In this work, we show that\nthe success of these attacks is largely due to duplication in commonly used\nweb-scraped training sets. We first show that the rate at which language models\nregenerate training sequences is superlinearly related to a sequence's count in\nthe training set. For instance, a sequence that is present 10 times in the\ntraining data is on average generated ~1000 times more often than a sequence\nthat is present only once. We next show that existing methods for detecting\nmemorized sequences have near-chance accuracy on non-duplicated training\nsequences. Finally, we find that after applying methods to deduplicate training\ndata, language models are considerably more secure against these types of\nprivacy attacks. Taken together, our results motivate an increased focus on\ndeduplication in privacy-sensitive applications and a reevaluation of the\npracticality of existing privacy attacks.\n", "first_author": "Nikhil Kandpal", "year_month": "2022-02"}, {"title": "You Don't Know My Favorite Color: Preventing Dialogue Representations\n  from Revealing Speakers' Private Personas", "published_date": "2022-04-26T09:36:18Z", "link": "http://arxiv.org/pdf/2205.10228v1", "abstract": "  Social chatbots, also known as chit-chat chatbots, evolve rapidly with large\npretrained language models. Despite the huge progress, privacy concerns have\narisen recently: training data of large language models can be extracted via\nmodel inversion attacks. On the other hand, the datasets used for training\nchatbots contain many private conversations between two individuals. In this\nwork, we further investigate the privacy leakage of the hidden states of\nchatbots trained by language modeling which has not been well studied yet. We\nshow that speakers' personas can be inferred through a simple neural network\nwith high accuracy. To this end, we propose effective defense objectives to\nprotect persona leakage from hidden states. We conduct extensive experiments to\ndemonstrate that our proposed defense objectives can greatly reduce the attack\naccuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve\nlanguage models' powerful generation ability.\n", "first_author": "Haoran Li", "year_month": "2022-04"}, {"title": "Creating a Large Language Model of a Philosopher", "published_date": "2023-02-02T01:10:26Z", "link": "http://arxiv.org/pdf/2302.01339v2", "abstract": "  Can large language models be trained to produce philosophical texts that are\ndifficult to distinguish from texts produced by human philosophers? To address\nthis question, we fine-tuned OpenAI's GPT-3 with the works of philosopher\nDaniel C. Dennett as additional training data. To explore the Dennett model, we\nasked the real Dennett ten philosophical questions and then posed the same\nquestions to the language model, collecting four responses for each question\nwithout cherry-picking. We recruited 425 participants to distinguish Dennett's\nanswer from the four machine-generated answers. Experts on Dennett's work (N =\n25) succeeded 51% of the time, above the chance rate of 20% but short of our\nhypothesized rate of 80% correct. For two of the ten questions, the language\nmodel produced at least one answer that experts selected more frequently than\nDennett's own answer. Philosophy blog readers (N = 302) performed similarly to\nthe experts, while ordinary research participants (N = 98) were near chance\ndistinguishing GPT-3's responses from those of an \"actual human philosopher\".\n", "first_author": "Eric Schwitzgebel", "year_month": "2023-02"}, {"title": "POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained\n  models", "published_date": "2023-04-29T22:05:22Z", "link": "http://arxiv.org/pdf/2305.00350v1", "abstract": "  Through prompting, large-scale pre-trained models have become more expressive\nand powerful, gaining significant attention in recent years. Though these big\nmodels have zero-shot capabilities, in general, labeled data are still required\nto adapt them to downstream tasks. To overcome this critical limitation, we\npropose an unsupervised fine-tuning framework to directly fine-tune the model\nor prompt on the unlabeled target data. We demonstrate how to apply our method\nto both language-augmented vision and masked-language models by aligning the\ndiscrete distributions extracted from the prompts and target data. To verify\nour approach's applicability, we conduct extensive experiments on image\nclassification, sentiment analysis, and natural language inference tasks.\nAcross 13 image-related tasks and 15 language-related ones, the proposed\napproach achieves consistent improvements over the baselines.\n", "first_author": "Korawat Tanwisuth", "year_month": "2023-04"}, {"title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models\n  and Large Language Models", "published_date": "2023-06-08T07:10:39Z", "link": "http://arxiv.org/pdf/2306.04980v1", "abstract": "  This work introduces approaches to assessing phrase breaks in ESL learners'\nspeech using pre-trained language models (PLMs) and large language models\n(LLMs). There are two tasks: overall assessment of phrase break for a speech\nclip and fine-grained assessment of every possible phrase break position. To\nleverage NLP models, speech input is first force-aligned with texts, and then\npre-processed into a token sequence, including words and phrase break\ninformation. To utilize PLMs, we propose a pre-training and fine-tuning\npipeline with the processed tokens. This process includes pre-training with a\nreplaced break token detection module and fine-tuning with text classification\nand sequence labeling. To employ LLMs, we design prompts for ChatGPT. The\nexperiments show that with the PLMs, the dependence on labeled training data\nhas been greatly reduced, and the performance has improved. Meanwhile, we\nverify that ChatGPT, a renowned LLM, has potential for further advancement in\nthis area.\n", "first_author": "Zhiyi Wang", "year_month": "2023-06"}, {"title": "Pretrained Language Models for Document-Level Neural Machine Translation", "published_date": "2019-11-08T08:00:45Z", "link": "http://arxiv.org/pdf/1911.03110v1", "abstract": "  Previous work on document-level NMT usually focuses on limited contexts\nbecause of degraded performance on larger contexts. In this paper, we\ninvestigate on using large contexts with three main contributions: (1)\nDifferent from previous work which pertrained models on large-scale\nsentence-level parallel corpora, we use pretrained language models,\nspecifically BERT, which are trained on monolingual documents; (2) We propose\ncontext manipulation methods to control the influence of large contexts, which\nlead to comparable results on systems using small and large contexts; (3) We\nintroduce a multi-task training for regularization to avoid models overfitting\nour training corpora, which further improves our systems together with a deeper\nencoder. Experiments are conducted on the widely used IWSLT data sets with\nthree language pairs, i.e., Chinese--English, French--English and\nSpanish--English. Results show that our systems are significantly better than\nthree previously reported document-level systems.\n", "first_author": "Liangyou Li", "year_month": "2019-11"}, {"title": "Neural Language Modeling for Contextualized Temporal Graph Generation", "published_date": "2020-10-20T07:08:00Z", "link": "http://arxiv.org/pdf/2010.10077v2", "abstract": "  This paper presents the first study on using large-scale pre-trained language\nmodels for automated generation of an event-level temporal graph for a\ndocument. Despite the huge success of neural pre-training methods in NLP tasks,\nits potential for temporal reasoning over event graphs has not been\nsufficiently explored. Part of the reason is the difficulty in obtaining large\ntraining corpora with human-annotated events and temporal links. We address\nthis challenge by using existing IE/NLP tools to automatically generate a large\nquantity (89,000) of system-produced document-graph pairs, and propose a novel\nformulation of the contextualized graph generation problem as a\nsequence-to-sequence mapping task. These strategies enable us to leverage and\nfine-tune pre-trained language models on the system-induced training data for\nthe graph generation task. Our experiments show that our approach is highly\neffective in generating structurally and semantically valid graphs. Further,\nevaluation on a challenging hand-labeled, out-domain corpus shows that our\nmethod outperforms the closest existing method by a large margin on several\nmetrics. Code and pre-trained models are available at\nhttps://github.com/madaan/temporal-graph-gen.\n", "first_author": "Aman Madaan", "year_month": "2020-10"}, {"title": "Robosourcing Educational Resources -- Leveraging Large Language Models\n  for Learnersourcing", "published_date": "2022-11-09T07:13:03Z", "link": "http://arxiv.org/pdf/2211.04715v1", "abstract": "  In this article, we introduce and evaluate the concept of robosourcing for\ncreating educational content. Robosourcing lies in the intersection of\ncrowdsourcing and large language models, where instead of a crowd of humans,\nrequests to large language models replace some of the work traditionally\nperformed by the crowd. Robosourcing includes a human-in-the-loop to provide\npriming (input) as well as to evaluate and potentially adjust the generated\nartefacts; these evaluations could also be used to improve the large language\nmodels. We propose a system to outline the robosourcing process. We further\nstudy the feasibility of robosourcing in the context of education by conducting\nan evaluation of robosourced and programming exercises, generated using OpenAI\nCodex. Our results suggest that robosourcing could significantly reduce human\neffort in creating diverse educational content while maintaining quality\nsimilar to human-created content.\n", "first_author": "Paul Denny", "year_month": "2022-11"}, {"title": "Large Language Models in Sport Science & Medicine: Opportunities, Risks\n  and Considerations", "published_date": "2023-05-05T21:20:02Z", "link": "http://arxiv.org/pdf/2305.03851v1", "abstract": "  This paper explores the potential opportunities, risks, and challenges\nassociated with the use of large language models (LLMs) in sports science and\nmedicine. LLMs are large neural networks with transformer style architectures\ntrained on vast amounts of textual data, and typically refined with human\nfeedback. LLMs can perform a large range of natural language processing tasks.\nIn sports science and medicine, LLMs have the potential to support and augment\nthe knowledge of sports medicine practitioners, make recommendations for\npersonalised training programs, and potentially distribute high-quality\ninformation to practitioners in developing countries. However, there are also\npotential risks associated with the use and development of LLMs, including\nbiases in the dataset used to create the model, the risk of exposing\nconfidential data, the risk of generating harmful output, and the need to align\nthese models with human preferences through feedback. Further research is\nneeded to fully understand the potential applications of LLMs in sports science\nand medicine and to ensure that their use is ethical and beneficial to\nathletes, clients, patients, practitioners, and the general public.\n", "first_author": "Mark Connor", "year_month": "2023-05"}, {"title": "Transfer Learning for Low-Resource Neural Machine Translation", "published_date": "2016-04-08T00:16:35Z", "link": "http://arxiv.org/pdf/1604.02201v1", "abstract": "  The encoder-decoder framework for neural machine translation (NMT) has been\nshown effective in large data scenarios, but is much less effective for\nlow-resource languages. We present a transfer learning method that\nsignificantly improves Bleu scores across a range of low-resource languages.\nOur key idea is to first train a high-resource language pair (the parent\nmodel), then transfer some of the learned parameters to the low-resource pair\n(the child model) to initialize and constrain training. Using our transfer\nlearning method we improve baseline NMT models by an average of 5.6 Bleu on\nfour low-resource language pairs. Ensembling and unknown word replacement add\nanother 2 Bleu which brings the NMT performance on low-resource machine\ntranslation close to a strong syntax based machine translation (SBMT) system,\nexceeding its performance on one language pair. Additionally, using the\ntransfer learning model for re-scoring, we can improve the SBMT system by an\naverage of 1.3 Bleu, improving the state-of-the-art on low-resource machine\ntranslation.\n", "first_author": "Barret Zoph", "year_month": "2016-04"}, {"title": "A Generative Model for Joint Natural Language Understanding and\n  Generation", "published_date": "2020-06-12T22:38:55Z", "link": "http://arxiv.org/pdf/2006.07499v1", "abstract": "  Natural language understanding (NLU) and natural language generation (NLG)\nare two fundamental and related tasks in building task-oriented dialogue\nsystems with opposite objectives: NLU tackles the transformation from natural\nlanguage to formal representations, whereas NLG does the reverse. A key to\nsuccess in either task is parallel training data which is expensive to obtain\nat a large scale. In this work, we propose a generative model which couples NLU\nand NLG through a shared latent variable. This approach allows us to explore\nboth spaces of natural language and formal representations, and facilitates\ninformation sharing through the latent space to eventually benefit NLU and NLG.\nOur model achieves state-of-the-art performance on two dialogue datasets with\nboth flat and tree-structured formal representations. We also show that the\nmodel can be trained in a semi-supervised fashion by utilising unlabelled data\nto boost its performance.\n", "first_author": "Bo-Hsiang Tseng", "year_month": "2020-06"}, {"title": "GREEK-BERT: The Greeks visiting Sesame Street", "published_date": "2020-08-27T09:36:14Z", "link": "http://arxiv.org/pdf/2008.12014v2", "abstract": "  Transformer-based language models, such as BERT and its variants, have\nachieved state-of-the-art performance in several downstream natural language\nprocessing (NLP) tasks on generic benchmark datasets (e.g., GLUE, SQUAD, RACE).\nHowever, these models have mostly been applied to the resource-rich English\nlanguage. In this paper, we present GREEK-BERT, a monolingual BERT-based\nlanguage model for modern Greek. We evaluate its performance in three NLP\ntasks, i.e., part-of-speech tagging, named entity recognition, and natural\nlanguage inference, obtaining state-of-the-art performance. Interestingly, in\ntwo of the benchmarks GREEK-BERT outperforms two multilingual Transformer-based\nmodels (M-BERT, XLM-R), as well as shallower neural baselines operating on\npre-trained word embeddings, by a large margin (5%-10%). Most importantly, we\nmake both GREEK-BERT and our training code publicly available, along with code\nillustrating how GREEK-BERT can be fine-tuned for downstream NLP tasks. We\nexpect these resources to boost NLP research and applications for modern Greek.\n", "first_author": "John Koutsikakis", "year_month": "2020-08"}, {"title": "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced\n  Language Model Pre-training", "published_date": "2020-10-23T22:14:50Z", "link": "http://arxiv.org/pdf/2010.12688v2", "abstract": "  Prior work on Data-To-Text Generation, the task of converting knowledge graph\n(KG) triples into natural text, focused on domain-specific benchmark datasets.\nIn this paper, however, we verbalize the entire English Wikidata KG, and\ndiscuss the unique challenges associated with a broad, open-domain, large-scale\nverbalization. We further show that verbalizing a comprehensive, encyclopedic\nKG like Wikidata can be used to integrate structured KGs and natural language\ncorpora. In contrast to the many architectures that have been developed to\nintegrate these two sources, our approach converts the KG into natural text,\nallowing it to be seamlessly integrated into existing language models. It\ncarries the further advantages of improved factual accuracy and reduced\ntoxicity in the resulting language model. We evaluate this approach by\naugmenting the retrieval corpus in a retrieval language model and showing\nsignificant improvements on the knowledge intensive tasks of open domain QA and\nthe LAMA knowledge probe.\n", "first_author": "Oshin Agarwal", "year_month": "2020-10"}, {"title": "Federated Learning Meets Natural Language Processing: A Survey", "published_date": "2021-07-27T05:07:48Z", "link": "http://arxiv.org/pdf/2107.12603v1", "abstract": "  Federated Learning aims to learn machine learning models from multiple\ndecentralized edge devices (e.g. mobiles) or servers without sacrificing local\ndata privacy. Recent Natural Language Processing techniques rely on deep\nlearning and large pre-trained language models. However, both big deep neural\nand language models are trained with huge amounts of data which often lies on\nthe server side. Since text data is widely originated from end users, in this\nwork, we look into recent NLP models and techniques which use federated\nlearning as the learning framework. Our survey discusses major challenges in\nfederated natural language processing, including the algorithm challenges,\nsystem challenges as well as the privacy issues. We also provide a critical\nreview of the existing Federated NLP evaluation methods and tools. Finally, we\nhighlight the current research gaps and future directions.\n", "first_author": "Ming Liu", "year_month": "2021-07"}, {"title": "From Good to Best: Two-Stage Training for Cross-lingual Machine Reading\n  Comprehension", "published_date": "2021-12-09T07:31:15Z", "link": "http://arxiv.org/pdf/2112.04735v1", "abstract": "  Cross-lingual Machine Reading Comprehension (xMRC) is challenging due to the\nlack of training data in low-resource languages. The recent approaches use\ntraining data only in a resource-rich language like English to fine-tune\nlarge-scale cross-lingual pre-trained language models. Due to the big\ndifference between languages, a model fine-tuned only by a source language may\nnot perform well for target languages. Interestingly, we observe that while the\ntop-1 results predicted by the previous approaches may often fail to hit the\nground-truth answers, the correct answers are often contained in the top-k\npredicted results. Based on this observation, we develop a two-stage approach\nto enhance the model performance. The first stage targets at recall: we design\na hard-learning (HL) algorithm to maximize the likelihood that the top-k\npredictions contain the accurate answer. The second stage focuses on precision:\nan answer-aware contrastive learning (AA-CL) mechanism is developed to learn\nthe fine difference between the accurate answer and other candidates. Our\nextensive experiments show that our model significantly outperforms a series of\nstrong baselines on two cross-lingual MRC benchmark datasets.\n", "first_author": "Nuo Chen", "year_month": "2021-12"}, {"title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as\n  Translators and Checkers", "published_date": "2023-06-10T21:58:29Z", "link": "http://arxiv.org/pdf/2306.06531v1", "abstract": "  For effective human-robot interaction, robots need to understand, plan, and\nexecute complex, long-horizon tasks described by natural language. The recent\nand remarkable advances in large language models (LLMs) have shown promise for\ntranslating natural language into robot action sequences for complex tasks.\nHowever, many existing approaches either translate the natural language\ndirectly into robot trajectories, or factor the inference process by\ndecomposing language into task sub-goals, then relying on a motion planner to\nexecute each sub-goal. When complex environmental and temporal constraints are\ninvolved, inference over planning tasks must be performed jointly with motion\nplans using traditional task-and-motion planning (TAMP) algorithms, making such\nfactorization untenable. Rather than using LLMs to directly plan task\nsub-goals, we instead perform few-shot translation from natural language task\ndescriptions to an intermediary task representation that can then be consumed\nby a TAMP algorithm to jointly solve the task and motion plan. To improve\ntranslation, we automatically detect and correct both syntactic and semantic\nerrors via autoregressive re-prompting, resulting in significant improvements\nin task completion. We show that our approach outperforms several methods using\nLLMs as planners in complex task domains.\n", "first_author": "Yongchao Chen", "year_month": "2023-06"}, {"title": "Deep Language Networks: Joint Prompt Training of Stacked LLMs using\n  Variational Inference", "published_date": "2023-06-21T18:45:56Z", "link": "http://arxiv.org/pdf/2306.12509v1", "abstract": "  We view large language models (LLMs) as stochastic \\emph{language layers} in\na network, where the learnable parameters are the natural language\n\\emph{prompts} at each layer. We stack two such layers, feeding the output of\none layer to the next. We call the stacked architecture a \\emph{Deep Language\nNetwork} (DLN). We first show how to effectively perform prompt optimization\nfor a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs\n(DLN-2), where two prompts must be learnt. We consider the output of the first\nlayer as a latent variable to marginalize, and devise a variational inference\nalgorithm for joint prompt training. A DLN-2 reaches higher performance than a\nsingle layer, sometimes comparable to few-shot GPT-4 even when each LLM in the\nnetwork is smaller and less powerful. The DLN code is open source:\nhttps://github.com/microsoft/deep-language-networks .\n", "first_author": "Alessandro Sordoni", "year_month": "2023-06"}, {"title": "Large Language Models as Fiduciaries: A Case Study Toward Robustly\n  Communicating With Artificial Intelligence Through Legal Standards", "published_date": "2023-01-24T16:03:20Z", "link": "http://arxiv.org/pdf/2301.10095v2", "abstract": "  Artificial Intelligence (AI) is taking on increasingly autonomous roles,\ne.g., browsing the web as a research assistant and managing money. But\nspecifying goals and restrictions for AI behavior is difficult. Similar to how\nparties to a legal contract cannot foresee every potential \"if-then\"\ncontingency of their future relationship, we cannot specify desired AI behavior\nfor all circumstances. Legal standards facilitate robust communication of\ninherently vague and underspecified goals. Instructions (in the case of\nlanguage models, \"prompts\") that employ legal standards will allow AI agents to\ndevelop shared understandings of the spirit of a directive that generalize\nexpectations regarding acceptable actions to take in unspecified states of the\nworld. Standards have built-in context that is lacking from other goal\nspecification languages, such as plain language and programming languages.\nThrough an empirical study on thousands of evaluation labels we constructed\nfrom U.S. court opinions, we demonstrate that large language models (LLMs) are\nbeginning to exhibit an \"understanding\" of one of the most relevant legal\nstandards for AI agents: fiduciary obligations. Performance comparisons across\nmodels suggest that, as LLMs continue to exhibit improved core capabilities,\ntheir legal standards understanding will also continue to improve. OpenAI's\nlatest LLM has 78% accuracy on our data, their previous release has 73%\naccuracy, and a model from their 2020 GPT-3 paper has 27% accuracy (worse than\nrandom). Our research is an initial step toward a framework for evaluating AI\nunderstanding of legal standards more broadly, and for conducting reinforcement\nlearning with legal feedback (RLLF).\n", "first_author": "John J. Nay", "year_month": "2023-01"}, {"title": "MISGENDERED: Limits of Large Language Models in Understanding Pronouns", "published_date": "2023-06-06T18:27:52Z", "link": "http://arxiv.org/pdf/2306.03950v2", "abstract": "  Content Warning: This paper contains examples of misgendering and erasure\nthat could be offensive and potentially triggering.\n  Gender bias in language technologies has been widely studied, but research\nhas mostly been restricted to a binary paradigm of gender. It is essential also\nto consider non-binary gender identities, as excluding them can cause further\nharm to an already marginalized group. In this paper, we comprehensively\nevaluate popular language models for their ability to correctly use English\ngender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze,\nxe, thon) that are used by individuals whose gender identity is not represented\nby binary pronouns. We introduce MISGENDERED, a framework for evaluating large\nlanguage models' ability to correctly use preferred pronouns, consisting of (i)\ninstances declaring an individual's pronoun, followed by a sentence with a\nmissing pronoun, and (ii) an experimental setup for evaluating masked and\nauto-regressive language models using a unified method. When prompted\nout-of-the-box, language models perform poorly at correctly predicting\nneo-pronouns (averaging 7.7% accuracy) and gender-neutral pronouns (averaging\n34.2% accuracy). This inability to generalize results from a lack of\nrepresentation of non-binary pronouns in training data and memorized\nassociations. Few-shot adaptation with explicit examples in the prompt improves\nperformance for neo-pronouns, but only to 64.7% even with 20 shots. We release\nthe full dataset, code, and demo at\nhttps://tamannahossainkay.github.io/misgendered/\n", "first_author": "Tamanna Hossain", "year_month": "2023-06"}, {"title": "Unleashing Infinite-Length Input Capacity for Large-scale Language\n  Models with Self-Controlled Memory System", "published_date": "2023-04-26T07:25:31Z", "link": "http://arxiv.org/pdf/2304.13343v1", "abstract": "  Large-scale Language Models (LLMs) are constrained by their inability to\nprocess lengthy inputs. To address this limitation, we propose the\nSelf-Controlled Memory (SCM) system to unleash infinite-length input capacity\nfor large-scale language models. Our SCM system is composed of three key\nmodules: the language model agent, the memory stream, and the memory\ncontroller. The language model agent iteratively processes ultra-long inputs\nand stores all historical information in the memory stream. The memory\ncontroller provides the agent with both long-term memory (archived memory) and\nshort-term memory (flash memory) to generate precise and coherent responses.\nThe controller determines which memories from archived memory should be\nactivated and how to incorporate them into the model input. Our SCM system can\nbe integrated with any LLMs to enable them to process ultra-long texts without\nany modification or fine-tuning. Experimental results show that our SCM system\nenables LLMs, which are not optimized for multi-turn dialogue, to achieve\nmulti-turn dialogue capabilities that are comparable to ChatGPT, and to\noutperform ChatGPT in scenarios involving ultra-long document summarization or\nlong-term conversations. Additionally, we will supply a test set, which covers\ncommon long-text input scenarios, for evaluating the abilities of LLMs in\nprocessing long documents.~\\footnote{Working in\nprogress.}\\footnote{\\url{https://github.com/wbbeyourself/SCM4LLMs}}\n", "first_author": "Xinnian Liang", "year_month": "2023-04"}, {"title": "hmBERT: Historical Multilingual Language Models for Named Entity\n  Recognition", "published_date": "2022-05-31T07:30:33Z", "link": "http://arxiv.org/pdf/2205.15575v2", "abstract": "  Compared to standard Named Entity Recognition (NER), identifying persons,\nlocations, and organizations in historical texts constitutes a big challenge.\nTo obtain machine-readable corpora, the historical text is usually scanned and\nOptical Character Recognition (OCR) needs to be performed. As a result, the\nhistorical corpora contain errors. Also, entities like location or organization\ncan change over time, which poses another challenge. Overall, historical texts\ncome with several peculiarities that differ greatly from modern texts and large\nlabeled corpora for training a neural tagger are hardly available for this\ndomain. In this work, we tackle NER for historical German, English, French,\nSwedish, and Finnish by training large historical language models. We\ncircumvent the need for large amounts of labeled data by using unlabeled data\nfor pretraining a language model. We propose hmBERT, a historical multilingual\nBERT-based language model, and release the model in several versions of\ndifferent sizes. Furthermore, we evaluate the capability of hmBERT by solving\ndownstream NER as part of this year's HIPE-2022 shared task and provide\ndetailed analysis and insights. For the Multilingual Classical Commentary\ncoarse-grained NER challenge, our tagger HISTeria outperforms the other teams'\nmodels for two out of three languages.\n", "first_author": "Stefan Schweter", "year_month": "2022-05"}, {"title": "An Analysis of Social Biases Present in BERT Variants Across Multiple\n  Languages", "published_date": "2022-11-25T23:38:08Z", "link": "http://arxiv.org/pdf/2211.14402v1", "abstract": "  Although large pre-trained language models have achieved great success in\nmany NLP tasks, it has been shown that they reflect human biases from their\npre-training corpora. This bias may lead to undesirable outcomes when these\nmodels are applied in real-world settings. In this paper, we investigate the\nbias present in monolingual BERT models across a diverse set of languages\n(English, Greek, and Persian). While recent research has mostly focused on\ngender-related biases, we analyze religious and ethnic biases as well and\npropose a template-based method to measure any kind of bias, based on sentence\npseudo-likelihood, that can handle morphologically complex languages with\ngender-based adjective declensions. We analyze each monolingual model via this\nmethod and visualize cultural similarities and differences across different\ndimensions of bias. Ultimately, we conclude that current methods of probing for\nbias are highly language-dependent, necessitating cultural insights regarding\nthe unique ways bias is expressed in each language and culture (e.g. through\ncoded language, synecdoche, and other similar linguistic concepts). We also\nhypothesize that higher measured social biases in the non-English BERT models\ncorrelate with user-generated content in their training.\n", "first_author": "Aristides Milios", "year_month": "2022-11"}, {"title": "Learning Python Code Suggestion with a Sparse Pointer Network", "published_date": "2016-11-24T21:01:46Z", "link": "http://arxiv.org/pdf/1611.08307v1", "abstract": "  To enhance developer productivity, all modern integrated development\nenvironments (IDEs) include code suggestion functionality that proposes likely\nnext tokens at the cursor. While current IDEs work well for statically-typed\nlanguages, their reliance on type annotations means that they do not provide\nthe same level of support for dynamic programming languages as for\nstatically-typed languages. Moreover, suggestion engines in modern IDEs do not\npropose expressions or multi-statement idiomatic code. Recent work has shown\nthat language models can improve code suggestion systems by learning from\nsoftware repositories. This paper introduces a neural language model with a\nsparse pointer network aimed at capturing very long-range dependencies. We\nrelease a large-scale code suggestion corpus of 41M lines of Python code\ncrawled from GitHub. On this corpus, we found standard neural language models\nto perform well at suggesting local phenomena, but struggle to refer to\nidentifiers that are introduced many tokens in the past. By augmenting a neural\nlanguage model with a pointer network specialized in referring to predefined\nclasses of identifiers, we obtain a much lower perplexity and a 5 percentage\npoints increase in accuracy for code suggestion compared to an LSTM baseline.\nIn fact, this increase in code suggestion accuracy is due to a 13 times more\naccurate prediction of identifiers. Furthermore, a qualitative analysis shows\nthis model indeed captures interesting long-range dependencies, like referring\nto a class member defined over 60 tokens in the past.\n", "first_author": "Avishkar Bhoopchand", "year_month": "2016-11"}, {"title": "Towards Multi-Language Recipe Personalisation and Recommendation", "published_date": "2020-07-27T11:26:49Z", "link": "http://arxiv.org/pdf/2007.13440v2", "abstract": "  Multi-language recipe personalisation and recommendation is an under-explored\nfield of information retrieval in academic and production systems. The existing\ngaps in our current understanding are numerous, even on fundamental questions\nsuch as whether consistent and high-quality recipe recommendation can be\ndelivered across languages. In this paper, we introduce the multi-language\nrecipe recommendation setting and present grounding results that will help to\nestablish the potential and absolute value of future work in this area. Our\nwork draws on several billion events from millions of recipes and users from\nArabic, English, Indonesian, Russian, and Spanish. We represent recipes using a\ncombination of normalised ingredients, standardised skills and image embeddings\nobtained without human intervention. In modelling, we take a classical approach\nbased on optimising an embedded bi-linear user-item metric space towards the\ninteractions that most strongly elicit cooking intent. For users without\ninteraction histories, a bespoke content-based cold-start model that predicts\ncontext and recipe affinity is introduced. We show that our approach to\npersonalisation is stable and easily scales to new languages. A robust\ncross-validation campaign is employed and consistently rejects baseline models\nand representations, strongly favouring those we propose. Our results are\npresented in a language-oriented (as opposed to model-oriented) fashion to\nemphasise the language-based goals of this work. We believe that this is the\nfirst large-scale work that comprehensively considers the value and potential\nof multi-language recipe recommendation and personalisation as well as\ndelivering scalable and reliable models.\n", "first_author": "Niall Twomey", "year_month": "2020-07"}, {"title": "LGDN: Language-Guided Denoising Network for Video-Language Modeling", "published_date": "2022-09-23T03:35:59Z", "link": "http://arxiv.org/pdf/2209.11388v3", "abstract": "  Video-language modeling has attracted much attention with the rapid growth of\nweb videos. Most existing methods assume that the video frames and text\ndescription are semantically correlated, and focus on video-language modeling\nat video level. However, this hypothesis often fails for two reasons: (1) With\nthe rich semantics of video contents, it is difficult to cover all frames with\na single video-level description; (2) A raw video typically has\nnoisy/meaningless information (e.g., scenery shot, transition or teaser).\nAlthough a number of recent works deploy attention mechanism to alleviate this\nproblem, the irrelevant/noisy information still makes it very difficult to\naddress. To overcome such challenge, we thus propose an efficient and effective\nmodel, termed Language-Guided Denoising Network (LGDN), for video-language\nmodeling. Different from most existing methods that utilize all extracted video\nframes, LGDN dynamically filters out the misaligned or redundant frames under\nthe language supervision and obtains only 2--4 salient frames per video for\ncross-modal token-level alignment. Extensive experiments on five public\ndatasets show that our LGDN outperforms the state-of-the-arts by large margins.\nWe also provide detailed ablation study to reveal the critical importance of\nsolving the noise issue, in hope of inspiring future video-language work.\n", "first_author": "Haoyu Lu", "year_month": "2022-09"}]